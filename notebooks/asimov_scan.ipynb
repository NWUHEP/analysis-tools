{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bias studies with full systematics\n",
    "\n",
    "To assess the impact of various sources of systematic, we will rely on an Asimov dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T18:00:49.810789Z",
     "start_time": "2020-04-10T18:00:49.663225Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/naodell/work/wbr/analysis\n",
      "{\n",
      "  \"shell_port\": 52613,\n",
      "  \"iopub_port\": 41493,\n",
      "  \"stdin_port\": 39057,\n",
      "  \"control_port\": 56551,\n",
      "  \"hb_port\": 37123,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"9bef3229-ea63d6381607a0da23d81915\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-3ee21c65-f2b0-43ce-b280-aa3b136eb703.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "## imports and configuration\n",
    "%cd '/home/naodell/work/wbr/analysis'\n",
    "#%load_ext autoreload\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.optimize import minimize, basinhopping\n",
    "from scipy.stats import norm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scripts.plot_tools as pt\n",
    "import scripts.fit_helpers as fh\n",
    "from nllfit.nllfitter import ScanParameters\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "rc_params = {\n",
    "             'figure.figsize': (10, 10),\n",
    "             'axes.labelsize': 20,\n",
    "             'axes.facecolor': 'white',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'legend.fontsize': 20,\n",
    "             'xtick.labelsize':20,\n",
    "             'ytick.labelsize':20,\n",
    "             'font.size':18,\n",
    "             'font.sans-serif':['Arial', 'sans-serif'],\n",
    "             'mathtext.sf':'Arial',\n",
    "             'lines.markersize':8.,\n",
    "             'lines.linewidth':2.5,\n",
    "            }\n",
    "matplotlib.rcParams.update(rc_params)\n",
    "\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T18:00:54.761417Z",
     "start_time": "2020-04-10T18:00:49.811903Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# configure, get the input data, and do any additional processing that is needed\n",
    "input_dir  = f'local_data/templates/updated_e_sf/'\n",
    "processes = ['ttbar', 't', 'ww', 'wjets', 'zjets_alt', 'diboson', 'fakes'] \n",
    "selections = [\n",
    "              'ee', 'mumu',  \n",
    "              'emu', \n",
    "              'mutau', 'etau', \n",
    "              'mu4j', 'e4j'\n",
    "             ]\n",
    "plot_labels = fh.fancy_labels\n",
    "\n",
    "# initialize fit data\n",
    "fit_data = fh.FitData(input_dir, selections, processes, process_cut=0.05)\n",
    "params = fit_data._parameters\n",
    "params_pre = fit_data.get_params_init().values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T18:00:54.766506Z",
     "start_time": "2020-04-10T18:00:54.762672Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize category veto list\n",
    "fit_data.veto_list = [\n",
    "    # baseline\n",
    "    #'ee_cat_gt2_eq1_b', 'ee_cat_gt2_gt2_b', \n",
    "    #'mumu_cat_gt2_eq1_b', 'mumu_cat_gt2_gt2_b', \n",
    "    #'emu_cat_gt2_eq1_a', 'emu_cat_gt2_gt2_a', \n",
    "    #'etau_cat_eq2_eq1', 'etau_cat_gt3_eq1', 'etau_cat_eq2_gt2', 'etau_cat_gt3_gt2', \n",
    "    #'mutau_cat_eq2_eq1', 'mutau_cat_gt3_eq1', 'mutau_cat_eq2_gt2', 'mutau_cat_gt3_gt2', \n",
    "    #'e4j_cat_gt4_eq1', 'e4j_cat_gt4_gt2',\n",
    "    #'mu4j_cat_gt4_eq1', 'mu4j_cat_gt4_gt2', \n",
    "    'e4j_cat_eq3_gt2', 'mu4j_cat_eq3_gt2',\n",
    "    \n",
    "    # e/mu DY CR\n",
    "    'ee_cat_gt2_eq0',  'mumu_cat_gt2_eq0', \n",
    "    \n",
    "    # e+mu additional ttbar\n",
    "    'emu_cat_gt2_eq0', 'emu_cat_eq1_eq0_a', 'emu_cat_eq1_eq1_a', \n",
    "    \n",
    "    # e+mu WW\n",
    "    'emu_cat_eq0_eq0_a', \n",
    "    \n",
    "    # e/mu+tau additional CR\n",
    "    'mutau_cat_eq0_eq0', 'mutau_cat_eq1_eq0', \n",
    "    'mutau_cat_gt2_eq0', 'mutau_cat_eq1_eq1', \n",
    "    'etau_cat_eq0_eq0', 'etau_cat_eq1_eq0', \n",
    "    'etau_cat_gt2_eq0', 'etau_cat_eq1_eq1', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T18:00:54.782125Z",
     "start_time": "2020-04-10T18:00:54.767507Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# generate scan points\n",
    "beta_scan_vals = ScanParameters(['beta_e', 'beta_mu', 'beta_tau'], \n",
    "                                [(0.104, 0.118), (0.104, 0.118), (0.104, 0.118)], \n",
    "                                [5, 5, 5]\n",
    "                                )\n",
    "scan_vals = np.array(beta_scan_vals.get_scan_vals()[0])\n",
    "beta_h = np.transpose([1 - np.sum(scan_vals, axis=1)])\n",
    "scan_vals = np.hstack((scan_vals, beta_h, np.outer(np.ones(scan_vals.shape[0]), params_pre[4:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T18:00:54.796976Z",
     "start_time": "2020-04-10T18:00:54.784754Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize fit data\n",
    "fobj = partial(fit_data.objective,\n",
    "               data = None,\n",
    "               do_bb_lite = True,\n",
    "               no_shape = True,\n",
    "               lu_test = 2\n",
    "              )\n",
    "\n",
    "fobj_jac = partial(fit_data.objective_jacobian,\n",
    "                   data = None,\n",
    "                   do_bb_lite = True,\n",
    "                   no_shape = True,\n",
    "                   lu_test = 2\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.674Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ca8f28517d437fb0b89530237a96c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 9.491202\n",
      "         Iterations: 67\n",
      "         Function evaluations: 217\n",
      "         Gradient evaluations: 203\n",
      "  False 999.8120854212011 1334.7503861404114 9.4912020440062\n",
      " scan vals:  [0.104 0.104 0.104 0.688]\n",
      " fit vals: [0.1039 0.1032 0.1017 0.6911]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.549802\n",
      "         Iterations: 66\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 100\n",
      "  False 980.0042898012446 1240.0364434057376 8.549801620286974\n",
      " scan vals:  [0.104  0.104  0.1075 0.6845]\n",
      " fit vals: [0.106  0.1069 0.1083 0.6788]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.008128\n",
      "         Iterations: 63\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 99\n",
      "  False 992.9606279311238 1200.1027805339484 5.008127875418122\n",
      " scan vals:  [0.104 0.104 0.111 0.681]\n",
      " fit vals: [0.1041 0.1053 0.1097 0.6808]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 10.699173\n",
      "         Iterations: 61\n",
      "         Function evaluations: 178\n",
      "         Gradient evaluations: 167\n",
      "  False 1001.2004191503158 1177.0867663974284 10.699172610509956\n",
      " scan vals:  [0.104  0.104  0.1145 0.6775]\n",
      " fit vals: [0.1015 0.1024 0.1129 0.6832]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.610150\n",
      "         Iterations: 63\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 92\n",
      "  False 991.1383148361599 1157.0592615648322 3.610150332310418\n",
      " scan vals:  [0.104 0.104 0.118 0.674]\n",
      " fit vals: [0.1039 0.1047 0.1202 0.6712]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 6.576323\n",
      "         Iterations: 53\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 72\n",
      "  True 985.6953855818064 1245.144530727565 6.576323285527921\n",
      " scan vals:  [0.104  0.1075 0.104  0.6845]\n",
      " fit vals: [0.103  0.1081 0.1017 0.6872]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.477764\n",
      "         Iterations: 49\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 116\n",
      "  False 992.2453910081865 1202.5848404738226 3.4777637875940424\n",
      " scan vals:  [0.104  0.1075 0.1075 0.681 ]\n",
      " fit vals: [0.1031 0.1068 0.1001 0.69  ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.591804\n",
      "         Iterations: 48\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 89\n",
      "  False 999.4235510065572 1182.6054582376016 7.591803894726607\n",
      " scan vals:  [0.104  0.1075 0.111  0.6775]\n",
      " fit vals: [0.1046 0.1093 0.1101 0.6761]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.255216\n",
      "         Iterations: 48\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 92\n",
      "  False 987.3637380984285 1164.9723538450908 3.2552155664557425\n",
      " scan vals:  [0.104  0.1075 0.1145 0.674 ]\n",
      " fit vals: [0.1045 0.1089 0.109  0.6776]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 9.741836\n",
      "         Iterations: 46\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 78\n",
      "  False 983.6173964228606 1176.905483689585 9.741836476319884\n",
      " scan vals:  [0.104  0.1075 0.118  0.6705]\n",
      " fit vals: [0.1044 0.1069 0.1254 0.6632]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 10.808191\n",
      "         Iterations: 40\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 98\n",
      "  False 987.2684805862434 1985.8680606146102 10.808191412912036\n",
      " scan vals:  [0.104 0.111 0.104 0.681]\n",
      " fit vals: [0.1056 0.1119 0.1047 0.6778]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.313279\n",
      "         Iterations: 45\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 67\n",
      "  False 998.5383047688372 1970.485215654608 4.313278661044074\n",
      " scan vals:  [0.104  0.111  0.1075 0.6775]\n",
      " fit vals: [0.1046 0.1114 0.1063 0.6778]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 8.867601\n",
      "         Iterations: 45\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 107\n",
      "  True 997.3887959178657 1964.6263557014795 8.867601452024438\n",
      " scan vals:  [0.104 0.111 0.111 0.674]\n",
      " fit vals: [0.103  0.11   0.1106 0.6763]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.087202\n",
      "         Iterations: 49\n",
      "         Function evaluations: 152\n",
      "         Gradient evaluations: 141\n",
      "  False 1001.0230976184173 1985.1381800226784 7.087201751130653\n",
      " scan vals:  [0.104  0.111  0.1145 0.6705]\n",
      " fit vals: [0.1052 0.111  0.1134 0.6704]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 9.567242\n",
      "         Iterations: 54\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 100\n",
      "  False 1001.1110403498378 2023.368937808927 9.567241508074666\n",
      " scan vals:  [0.104 0.111 0.118 0.667]\n",
      " fit vals: [0.103  0.112  0.1187 0.6663]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 8.924980\n",
      "         Iterations: 66\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 100\n",
      "  True 1002.2955403387542 3515.257015043718 8.92497966893542\n",
      " scan vals:  [0.104  0.1145 0.104  0.6775]\n",
      " fit vals: [0.1034 0.1125 0.1106 0.6735]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.524537\n",
      "         Iterations: 59\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 110\n",
      "  False 996.8060979304458 3502.5836622634874 6.524537119857988\n",
      " scan vals:  [0.104  0.1145 0.1075 0.674 ]\n",
      " fit vals: [0.1049 0.115  0.1051 0.675 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.994651\n",
      "         Iterations: 65\n",
      "         Function evaluations: 158\n",
      "         Gradient evaluations: 147\n",
      "  False 997.8159251710417 3518.3915912224375 8.994651337695755\n",
      " scan vals:  [0.104  0.1145 0.111  0.6705]\n",
      " fit vals: [0.1034 0.1152 0.1132 0.6681]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.426943\n",
      "         Iterations: 64\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 128\n",
      "  False 987.8616405968779 3544.870138710225 6.42694292127539\n",
      " scan vals:  [0.104  0.1145 0.1145 0.667 ]\n",
      " fit vals: [0.1042 0.1143 0.1157 0.6659]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 9.467327\n",
      "         Iterations: 56\n",
      "         Function evaluations: 178\n",
      "         Gradient evaluations: 168\n",
      "  False 992.0923558623048 3606.8546208329485 9.467327042771723\n",
      " scan vals:  [0.104  0.1145 0.118  0.6635]\n",
      " fit vals: [0.103  0.1133 0.118  0.6657]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.833722\n",
      "         Iterations: 66\n",
      "         Function evaluations: 173\n",
      "         Gradient evaluations: 163\n",
      "  False 985.9688603660853 5753.5838431124275 8.833722422120111\n",
      " scan vals:  [0.104 0.118 0.104 0.674]\n",
      " fit vals: [0.1036 0.1186 0.1021 0.6757]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.219226\n",
      "         Iterations: 61\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 89\n",
      "  False 983.8720942657138 5761.085140258325 7.219226312917889\n",
      " scan vals:  [0.104  0.118  0.1075 0.6705]\n",
      " fit vals: [0.1041 0.1182 0.1132 0.6645]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.325125\n",
      "         Iterations: 66\n",
      "         Function evaluations: 180\n",
      "         Gradient evaluations: 168\n",
      "  False 990.2950560574119 5799.167897873682 4.325124843365328\n",
      " scan vals:  [0.104 0.118 0.111 0.667]\n",
      " fit vals: [0.1037 0.118  0.1152 0.6631]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.297811\n",
      "         Iterations: 60\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 83\n",
      "  False 990.1372193178822 5852.391427905396 6.297811427069161\n",
      " scan vals:  [0.104  0.118  0.1145 0.6635]\n",
      " fit vals: [0.1038 0.1176 0.1155 0.6632]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5.669226\n",
      "         Iterations: 68\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 82\n",
      "  True 990.1415048061936 5927.190758763436 5.6692260848602825\n",
      " scan vals:  [0.104 0.118 0.118 0.66 ]\n",
      " fit vals: [0.1052 0.1172 0.1247 0.6529]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.544398\n",
      "         Iterations: 52\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 95\n",
      "  False 992.4206428685081 1034.6933178964096 6.5443976037618405\n",
      " scan vals:  [0.1075 0.104  0.104  0.6845]\n",
      " fit vals: [0.1081 0.103  0.1084 0.6805]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.625136\n",
      "         Iterations: 50\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 69\n",
      "  False 990.0540614213426 975.1708032583613 5.625136085767616\n",
      " scan vals:  [0.1075 0.104  0.1075 0.681 ]\n",
      " fit vals: [0.1078 0.1032 0.1115 0.6774]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.210601\n",
      "         Iterations: 50\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 89\n",
      "  False 987.2916749016317 937.3337359071392 6.210601376384902\n",
      " scan vals:  [0.1075 0.104  0.111  0.6775]\n",
      " fit vals: [0.1091 0.1051 0.1024 0.6833]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.441451\n",
      "         Iterations: 56\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "  False 988.7983682234649 925.4699175243527 7.441450530447381\n",
      " scan vals:  [0.1075 0.104  0.1145 0.674 ]\n",
      " fit vals: [0.1087 0.1056 0.1162 0.6695]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.675302\n",
      "         Iterations: 54\n",
      "         Function evaluations: 164\n",
      "         Gradient evaluations: 152\n",
      "  False 979.4580515175036 924.1235054033558 4.675302347063482\n",
      " scan vals:  [0.1075 0.104  0.118  0.6705]\n",
      " fit vals: [0.1074 0.1037 0.115  0.6738]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 10.326220\n",
      "         Iterations: 42\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 103\n",
      "  False 982.7579681325547 947.4616147335769 10.32622042130659\n",
      " scan vals:  [0.1075 0.1075 0.104  0.681 ]\n",
      " fit vals: [0.1088 0.1094 0.1082 0.6735]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.648603\n",
      "         Iterations: 42\n",
      "         Function evaluations: 197\n",
      "         Gradient evaluations: 186\n",
      "  False 988.1744375852791 921.8564737812509 4.648603051183802\n",
      " scan vals:  [0.1075 0.1075 0.1075 0.6775]\n",
      " fit vals: [0.1068 0.1076 0.1175 0.6681]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.681304\n",
      "         Iterations: 41\n",
      "         Function evaluations: 159\n",
      "         Gradient evaluations: 148\n",
      "  False 990.4843715289073 915.1617812322924 6.681304022277145\n",
      " scan vals:  [0.1075 0.1075 0.111  0.674 ]\n",
      " fit vals: [0.1073 0.1072 0.1105 0.675 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.398899\n",
      "         Iterations: 38\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 128\n",
      "  False 989.8699266040462 927.1966885281417 5.398898979149618\n",
      " scan vals:  [0.1075 0.1075 0.1145 0.6705]\n",
      " fit vals: [0.1084 0.1074 0.1162 0.6681]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.178615\n",
      "         Iterations: 44\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 75\n",
      "  False 984.8780214328348 956.1806874326863 5.1786148222604504\n",
      " scan vals:  [0.1075 0.1075 0.118  0.667 ]\n",
      " fit vals: [0.1068 0.1072 0.1152 0.6708]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.682290\n",
      "         Iterations: 48\n",
      "         Function evaluations: 141\n",
      "         Gradient evaluations: 130\n",
      "  False 988.6723560866193 1690.8644757203044 7.6822895733664245\n",
      " scan vals:  [0.1075 0.111  0.104  0.6775]\n",
      " fit vals: [0.1066 0.1106 0.1027 0.68  ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1.353885\n",
      "         Iterations: 47\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 184\n",
      "  False 978.0416902288787 1672.0058913398025 1.3538853169932905\n",
      " scan vals:  [0.1075 0.111  0.1075 0.674 ]\n",
      " fit vals: [0.1082 0.1115 0.1122 0.6681]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.647465\n",
      "         Iterations: 53\n",
      "         Function evaluations: 183\n",
      "         Gradient evaluations: 171\n",
      "  False 990.4235999540448 1698.167620567164 7.647464845986458\n",
      " scan vals:  [0.1075 0.111  0.111  0.6705]\n",
      " fit vals: [0.106  0.109  0.1058 0.6792]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.770654\n",
      "         Iterations: 49\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 96\n",
      "  False 989.6853667468075 1732.8654388946322 8.770653981135183\n",
      " scan vals:  [0.1075 0.111  0.1145 0.667 ]\n",
      " fit vals: [0.1071 0.1117 0.1119 0.6693]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.932501\n",
      "         Iterations: 49\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 124\n",
      "  False 979.057078397431 1779.0119469067135 7.932500838015876\n",
      " scan vals:  [0.1075 0.111  0.118  0.6635]\n",
      " fit vals: [0.1065 0.1096 0.1158 0.668 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.060086\n",
      "         Iterations: 55\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 106\n",
      "  False 986.9523379085284 3202.1870129077925 7.060085646404267\n",
      " scan vals:  [0.1075 0.1145 0.104  0.674 ]\n",
      " fit vals: [0.1097 0.1159 0.1168 0.6575]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 7.593287\n",
      "         Iterations: 60\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 81\n",
      "  True 994.1801544029972 3220.9901683473513 7.593287142009083\n",
      " scan vals:  [0.1075 0.1145 0.1075 0.6705]\n",
      " fit vals: [0.1071 0.1136 0.1121 0.6672]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.680346\n",
      "         Iterations: 58\n",
      "         Function evaluations: 237\n",
      "         Gradient evaluations: 223\n",
      "  False 980.4382538408643 3240.8695427021316 5.680345623724636\n",
      " scan vals:  [0.1075 0.1145 0.111  0.667 ]\n",
      " fit vals: [0.109  0.1164 0.1111 0.6636]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.706478\n",
      "         Iterations: 59\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 83\n",
      "  False 1001.3699221356169 3317.1261278207617 5.7064781527360715\n",
      " scan vals:  [0.1075 0.1145 0.1145 0.6635]\n",
      " fit vals: [0.1083 0.1139 0.1131 0.6646]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.027585\n",
      "         Iterations: 59\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 114\n",
      "  False 984.0364563995831 3376.5115608635397 5.027585355656701\n",
      " scan vals:  [0.1075 0.1145 0.118  0.66  ]\n",
      " fit vals: [0.1086 0.1165 0.1121 0.6628]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.382823\n",
      "         Iterations: 63\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 96\n",
      "  False 997.1098298316808 5465.955408687905 6.382823225863282\n",
      " scan vals:  [0.1075 0.118  0.104  0.6705]\n",
      " fit vals: [0.1077 0.1167 0.0994 0.6762]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.041736\n",
      "         Iterations: 68\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 124\n",
      "  False 992.9728447569394 5490.507367107787 7.041735721990543\n",
      " scan vals:  [0.1075 0.118  0.1075 0.667 ]\n",
      " fit vals: [0.1082 0.1193 0.1065 0.6661]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 10.227691\n",
      "         Iterations: 66\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 138\n",
      "  False 985.4057402776425 5533.754320027524 10.22769094904512\n",
      " scan vals:  [0.1075 0.118  0.111  0.6635]\n",
      " fit vals: [0.1063 0.1174 0.1144 0.662 ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.640096\n",
      "         Iterations: 62\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 94\n",
      "  False 989.3307398390263 5610.283373134786 8.640095552252754\n",
      " scan vals:  [0.1075 0.118  0.1145 0.66  ]\n",
      " fit vals: [0.1083 0.1201 0.1136 0.6581]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.868671\n",
      "         Iterations: 66\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 130\n",
      "  False 996.6790980620164 5711.722072483114 3.8686706057815337\n",
      " scan vals:  [0.1075 0.118  0.118  0.6565]\n",
      " fit vals: [0.1058 0.1186 0.1168 0.6587]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 12.134421\n",
      "         Iterations: 53\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 125\n",
      "  False 997.492957567365 1325.0609033685978 12.134420541337235\n",
      " scan vals:  [0.111 0.104 0.104 0.681]\n",
      " fit vals: [0.1124 0.1046 0.1009 0.6821]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.074623\n",
      "         Iterations: 50\n",
      "         Function evaluations: 85\n",
      "         Gradient evaluations: 74\n",
      "  False 975.8655345637735 1262.080774886189 7.074623285308854\n",
      " scan vals:  [0.111  0.104  0.1075 0.6775]\n",
      " fit vals: [0.1124 0.105  0.1144 0.6682]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.363268\n",
      "         Iterations: 51\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 80\n",
      "  False 977.5932925034647 1244.6269742319164 6.363267768503419\n",
      " scan vals:  [0.111 0.104 0.111 0.674]\n",
      " fit vals: [0.1114 0.1041 0.1096 0.6749]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.396691\n",
      "         Iterations: 46\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 101\n",
      "  False 976.652024426383 1246.3017608730527 8.396690958773059\n",
      " scan vals:  [0.111  0.104  0.1145 0.6705]\n",
      " fit vals: [0.1112 0.1037 0.1135 0.6715]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.718927\n",
      "         Iterations: 50\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 102\n",
      "  False 1013.9021020648767 1307.6285882565478 7.718927059388762\n",
      " scan vals:  [0.111 0.104 0.118 0.667]\n",
      " fit vals: [0.1104 0.1039 0.1245 0.6612]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.417796\n",
      "         Iterations: 39\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 91\n",
      "  False 1000.443584943743 1249.0976629226511 6.417796456232845\n",
      " scan vals:  [0.111  0.1075 0.104  0.6775]\n",
      " fit vals: [0.1127 0.108  0.1073 0.672 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.270418\n",
      "         Iterations: 44\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 129\n",
      "  False 990.4338029774468 1224.2066089775392 3.270417848488191\n",
      " scan vals:  [0.111  0.1075 0.1075 0.674 ]\n",
      " fit vals: [0.1128 0.1094 0.1059 0.6719]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.454899\n",
      "         Iterations: 41\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 126\n",
      "  False 995.595568917253 1236.5934916286808 7.454898564568542\n",
      " scan vals:  [0.111  0.1075 0.111  0.6705]\n",
      " fit vals: [0.1084 0.1058 0.1113 0.6746]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.345774\n",
      "         Iterations: 46\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "  False 990.7587503829709 1260.728765920193 5.34577370540108\n",
      " scan vals:  [0.111  0.1075 0.1145 0.667 ]\n",
      " fit vals: [0.1145 0.1096 0.1143 0.6616]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 8.042468\n",
      "         Iterations: 44\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 139\n",
      "  True 991.7750357715363 1312.1395632454469 8.042467514318107\n",
      " scan vals:  [0.111  0.1075 0.118  0.6635]\n",
      " fit vals: [0.1094 0.1073 0.1191 0.6643]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.924272\n",
      "         Iterations: 53\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 140\n",
      "  False 994.4313438447624 1979.6326915556513 8.924271553114028\n",
      " scan vals:  [0.111 0.111 0.104 0.674]\n",
      " fit vals: [0.112  0.1102 0.107  0.6709]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.262245\n",
      "         Iterations: 58\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "  False 992.1222476921221 1985.5708157881852 8.262244570846695\n",
      " scan vals:  [0.111  0.111  0.1075 0.6705]\n",
      " fit vals: [0.112  0.1114 0.1076 0.6691]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.218030\n",
      "         Iterations: 56\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 89\n",
      "  False 980.5034844813102 2004.295852967618 5.21803016232355\n",
      " scan vals:  [0.111 0.111 0.111 0.667]\n",
      " fit vals: [0.1136 0.1126 0.1141 0.6597]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.983848\n",
      "         Iterations: 58\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 137\n",
      "  False 1006.0541300205358 2081.9390169386925 6.983847613770281\n",
      " scan vals:  [0.111  0.111  0.1145 0.6635]\n",
      " fit vals: [0.1143 0.112  0.1128 0.6608]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.821657\n",
      "         Iterations: 56\n",
      "         Function evaluations: 215\n",
      "         Gradient evaluations: 201\n",
      "  False 978.6006086497297 2128.012158231797 6.821656540594296\n",
      " scan vals:  [0.111 0.111 0.118 0.66 ]\n",
      " fit vals: [0.1126 0.1125 0.1226 0.6523]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.075541\n",
      "         Iterations: 59\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 100\n",
      "  False 984.0211620324329 3481.645328212188 4.075541212657569\n",
      " scan vals:  [0.111  0.1145 0.104  0.6705]\n",
      " fit vals: [0.1126 0.1135 0.102  0.6719]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.527910\n",
      "         Iterations: 64\n",
      "         Function evaluations: 264\n",
      "         Gradient evaluations: 250\n",
      "  False 998.8358396524244 3524.842302094513 8.527910315257294\n",
      " scan vals:  [0.111  0.1145 0.1075 0.667 ]\n",
      " fit vals: [0.1096 0.1139 0.1099 0.6667]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.459916\n",
      "         Iterations: 58\n",
      "         Function evaluations: 186\n",
      "         Gradient evaluations: 173\n",
      "  False 976.9303243277245 3553.4532651741183 6.459916402792341\n",
      " scan vals:  [0.111  0.1145 0.111  0.6635]\n",
      " fit vals: [0.1105 0.1144 0.1164 0.6587]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 6.040388\n",
      "         Iterations: 62\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 120\n",
      "  True 995.2392305911053 3644.074240020424 6.040388240648064\n",
      " scan vals:  [0.111  0.1145 0.1145 0.66  ]\n",
      " fit vals: [0.1118 0.1151 0.1207 0.6524]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.393564\n",
      "         Iterations: 63\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 97\n",
      "  False 990.3113628381456 3732.9473345399206 4.393564253063603\n",
      " scan vals:  [0.111  0.1145 0.118  0.6565]\n",
      " fit vals: [0.1099 0.1136 0.1155 0.661 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 9.179622\n",
      "         Iterations: 68\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 108\n",
      "  False 1016.4839702341725 5767.349938147129 9.179621830415666\n",
      " scan vals:  [0.111 0.118 0.104 0.667]\n",
      " fit vals: [0.1107 0.1169 0.1203 0.6521]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.196226\n",
      "         Iterations: 62\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 102\n",
      "  False 987.3632293417105 5784.05347055017 6.196226453451985\n",
      " scan vals:  [0.111  0.118  0.1075 0.6635]\n",
      " fit vals: [0.1116 0.1185 0.1109 0.659 ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.617871\n",
      "         Iterations: 63\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "  False 987.5595368021859 5852.286814003676 3.6178712318913884\n",
      " scan vals:  [0.111 0.118 0.111 0.66 ]\n",
      " fit vals: [0.1094 0.1155 0.104  0.6711]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.433172\n",
      "         Iterations: 68\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 97\n",
      "  False 983.4118017678437 5938.057525286768 5.433172478980991\n",
      " scan vals:  [0.111  0.118  0.1145 0.6565]\n",
      " fit vals: [0.112  0.1195 0.118  0.6505]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.526346\n",
      "         Iterations: 68\n",
      "         Function evaluations: 158\n",
      "         Gradient evaluations: 146\n",
      "  False 985.2343895486913 6051.379279779061 4.526345853763171\n",
      " scan vals:  [0.111 0.118 0.118 0.653]\n",
      " fit vals: [0.1112 0.1175 0.1283 0.6429]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 11.058072\n",
      "         Iterations: 50\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "  False 998.3313700575369 2161.230143873391 11.058072289285406\n",
      " scan vals:  [0.1145 0.104  0.104  0.6775]\n",
      " fit vals: [0.116  0.1036 0.1063 0.6741]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 9.523244\n",
      "         Iterations: 57\n",
      "         Function evaluations: 85\n",
      "         Gradient evaluations: 74\n",
      "  False 999.4259227934398 2135.0009639771224 9.523244181127332\n",
      " scan vals:  [0.1145 0.104  0.1075 0.674 ]\n",
      " fit vals: [0.1151 0.1059 0.1133 0.6657]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.815163\n",
      "         Iterations: 48\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 94\n",
      "  False 983.1617121427461 2113.6951655079256 5.815163075691788\n",
      " scan vals:  [0.1145 0.104  0.111  0.6705]\n",
      " fit vals: [0.1134 0.1026 0.1148 0.6692]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.786527\n",
      "         Iterations: 50\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 104\n",
      "  False 977.2735062357675 2124.676371496571 5.786526880996595\n",
      " scan vals:  [0.1145 0.104  0.1145 0.667 ]\n",
      " fit vals: [0.1174 0.1054 0.1171 0.6601]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 6.266759\n",
      "         Iterations: 50\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 107\n",
      "  False 987.090948983892 2172.9390850664504 6.266759335770134\n",
      " scan vals:  [0.1145 0.104  0.118  0.6635]\n",
      " fit vals: [0.1145 0.1055 0.1201 0.6599]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 4.973140\n",
      "         Iterations: 54\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 71\n",
      "  True 985.9220744009438 2069.2061425669954 4.973139782100735\n",
      " scan vals:  [0.1145 0.1075 0.104  0.674 ]\n",
      " fit vals: [0.1153 0.1073 0.1078 0.6697]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.799684\n",
      "         Iterations: 52\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 109\n",
      "  False 996.3327729653909 2079.0996853310744 7.799684281790312\n",
      " scan vals:  [0.1145 0.1075 0.1075 0.6705]\n",
      " fit vals: [0.1138 0.107  0.1045 0.6747]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.551047\n",
      "         Iterations: 53\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 79\n",
      "  False 988.2841684813202 2092.750665230889 7.551047052784772\n",
      " scan vals:  [0.1145 0.1075 0.111  0.667 ]\n",
      " fit vals: [0.1154 0.1064 0.1092 0.6691]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 7.214455\n",
      "         Iterations: 59\n",
      "         Function evaluations: 201\n",
      "         Gradient evaluations: 189\n",
      "  False 996.4182054810561 2144.4440563856997 7.214455002020807\n",
      " scan vals:  [0.1145 0.1075 0.1145 0.6635]\n",
      " fit vals: [0.1165 0.109  0.115  0.6595]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.971722\n",
      "         Iterations: 52\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 82\n",
      "  False 979.1643006230232 2192.2865416952573 5.971722345701231\n",
      " scan vals:  [0.1145 0.1075 0.118  0.66  ]\n",
      " fit vals: [0.1148 0.1081 0.115  0.6621]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.169269\n",
      "         Iterations: 65\n",
      "         Function evaluations: 167\n",
      "         Gradient evaluations: 156\n",
      "  False 979.5195625825579 2799.032119646694 4.169268763747307\n",
      " scan vals:  [0.1145 0.111  0.104  0.6705]\n",
      " fit vals: [0.1169 0.1125 0.1094 0.6612]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 10.539316\n",
      "         Iterations: 69\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 143\n",
      "  False 988.7841550332313 2831.2400741878596 10.539316355291994\n",
      " scan vals:  [0.1145 0.111  0.1075 0.667 ]\n",
      " fit vals: [0.1155 0.1122 0.1073 0.6649]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.770754\n",
      "         Iterations: 63\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 93\n",
      "  False 1000.5078598477759 2888.113405735782 8.770754287908888\n",
      " scan vals:  [0.1145 0.111  0.111  0.6635]\n",
      " fit vals: [0.117  0.1119 0.116  0.6551]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.975430\n",
      "         Iterations: 63\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 107\n",
      "  False 995.8722665365287 2950.4883442418445 5.975429728765821\n",
      " scan vals:  [0.1145 0.111  0.1145 0.66  ]\n",
      " fit vals: [0.1135 0.1109 0.1182 0.6573]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# carry out the scan #\n",
    "\n",
    "# minimizer options\n",
    "min_options = dict(\n",
    "                   #eps=1e-10,\n",
    "                   gtol=1e-3, \n",
    "                   disp=True\n",
    "                  )\n",
    "\n",
    "results = []\n",
    "cost = []\n",
    "sv_accept = []\n",
    "mask = fit_data._pmask\n",
    "pinit = params_pre[mask]\n",
    "for sv in tqdm(scan_vals):\n",
    "    \n",
    "    # randomize n.p.\n",
    "    mask[:4] = False\n",
    "    np_random = params_pre[mask] + fit_data._perr_init[mask]*np.random.randn(mask.sum())\n",
    "    fit_data._pval_init[mask] = np_random\n",
    "    #sv[mask] = np_random\n",
    "    mask[:4] = True\n",
    "    \n",
    "    # generate data from scan values w/ statistical variation\n",
    "    sample = dict()\n",
    "    for category in fit_data._categories:\n",
    "        val, var = fit_data.mixture_model(sv, category)\n",
    "        sample[category] = [np.random.poisson(val), val]\n",
    "        #sample[category] = [val, val]\n",
    "    \n",
    "    fobj.keywords['data'] = sample\n",
    "    fobj_jac.keywords['data'] = sample\n",
    "\n",
    "    # carry out minimization\n",
    "    result = minimize(fobj, pinit,\n",
    "                      method  = 'BFGS', \n",
    "                      options = min_options,\n",
    "                      jac     = fobj_jac,\n",
    "                      #args    = (sample)\n",
    "                     )\n",
    "    print(' ', result.success, fit_data.objective(pinit), fit_data.objective(sv[mask]), result.fun)\n",
    "    #print(' jacobian: ', result.jac)\n",
    "    #print(' init vals: ', pinit)\n",
    "    #print(' nps: ', np_random)\n",
    "    print(' scan vals: ', sv[:4])\n",
    "    print(' fit vals:', result.x[:4], end='\\n\\n')\n",
    "\n",
    "    results.append(result.x)\n",
    "    cost.append(result.fun)\n",
    "    sv_accept.append(sv[mask])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.678Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#calculate biases\n",
    "results = np.array(results)\n",
    "sv_accept = np.array(sv_accept)\n",
    "cost = np.array(cost)\n",
    "\n",
    "diff = (results - sv_accept)\n",
    "diff[:,:4] /= 0.01*pinit[:4]\n",
    "#diff[:,:4] /= 0.01*sv_accept[:,:4]\n",
    "diff[:,4:diff.shape[1]] /= params['err_init'][4:diff.shape[1]].values\n",
    "\n",
    "#diff = np.array([d for d in diff if np.all((d > -10) & (d < 10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.681Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the cost\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10), facecolor='white')\n",
    "\n",
    "cost = cost[cost<600]\n",
    "cost_mean, cost_err = cost.mean(), cost.std()\n",
    "ax.hist(cost, bins=np.linspace(np.max([0, cost_mean - 5*cost_err]), cost_mean + 5*cost_err, 25), histtype='stepfilled')\n",
    "ax.set_xlabel(r'$NLL_{fit}$')\n",
    "ax.set_ylabel('Entries')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.686Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# branching fraction scans\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), facecolor='white', sharex=False, sharey=False)\n",
    "\n",
    "beta_val = sv_accept[:,:4]\n",
    "beta_obs = results[:,:4]\n",
    "\n",
    "ax = axes[0][0]\n",
    "ax.plot(beta_val[:,0], beta_obs[:,0], 'C0o', alpha=0.1)\n",
    "ax.plot([0.1, 0.12], [0.1, 0.12], 'r:')\n",
    "ax.set_xlim(0.102, 0.12)\n",
    "ax.set_ylim(0.102, 0.12)\n",
    "ax.set_ylabel(r'$B_{obs.}$')\n",
    "ax.set_title(r'$W\\rightarrow e$', size=20)\n",
    "\n",
    "ax = axes[0][1]\n",
    "ax.plot(beta_val[:,1], beta_obs[:,1], 'C0o', alpha=0.1)\n",
    "ax.plot([0.1, 0.12], [0.1, 0.12], 'r:')\n",
    "ax.set_xlim(0.102, 0.12)\n",
    "ax.set_ylim(0.102, 0.12)\n",
    "ax.set_title(r'$W\\rightarrow\\mu$', size=20)\n",
    "\n",
    "ax = axes[1][0]\n",
    "ax.plot(beta_val[:,2], beta_obs[:,2], 'C0o', alpha=0.1)\n",
    "ax.plot([0.1, 0.12], [0.1, 0.12], 'r:')\n",
    "ax.set_xlim(0.102, 0.12)\n",
    "ax.set_ylim(0.102, 0.12)\n",
    "ax.set_ylabel(r'$B_{obs.}$')\n",
    "ax.set_xlabel(r'$B_{true}$')\n",
    "ax.set_title(r'$W\\rightarrow\\tau$', size=20)\n",
    "\n",
    "ax = axes[1][1]\n",
    "ax.plot(beta_val[:,3], beta_obs[:,3], 'C0o', alpha=0.1)\n",
    "ax.plot([0.64, 0.72], [0.64, 0.72], 'r:')\n",
    "ax.set_xlim(0.64, 0.695)\n",
    "ax.set_ylim(0.64, 0.695)\n",
    "ax.set_xlabel(r'$B_{true}$')\n",
    "ax.set_title(r'$W\\rightarrow h$', size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/beta_scan.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.694Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# branching fraction scans\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), facecolor='white', sharex=True, sharey=False)\n",
    "bins = np.linspace(-5, 5, 80)\n",
    "\n",
    "ax = axes[0][0]\n",
    "dbeta = 100*(beta_obs[:,0] - beta_val[:,0])/beta_val[:,0]\n",
    "ax.hist(dbeta, bins[::2], alpha=0.9, density=True)\n",
    "ax.plot(bins, norm.pdf(bins, loc=dbeta.mean(), scale=dbeta.std()))\n",
    "ax.text(0.1, 0.9, r'$\\mu =$' + f'{dbeta.mean():.3f}', transform=ax.transAxes)\n",
    "ax.text(0.1, 0.8, r'$\\sigma =$' + f'{dbeta.std():.3f}', transform=ax.transAxes)\n",
    "ax.set_xlim(-5., 5.)\n",
    "ax.set_ylim(0., None)\n",
    "ax.set_ylabel('trial')\n",
    "ax.set_title(r'$W\\rightarrow e$', size=20)\n",
    "\n",
    "ax = axes[0][1]\n",
    "dbeta = 100*(beta_obs[:,1] - beta_val[:,1])/beta_val[:,1]\n",
    "ax.hist(dbeta, bins[::2], alpha=0.9, density=True)\n",
    "ax.plot(bins, norm.pdf(bins, loc=dbeta.mean(), scale=dbeta.std()))\n",
    "ax.text(0.1, 0.9, r'$\\mu =$' + f'{dbeta.mean():.3f}', transform=ax.transAxes)\n",
    "ax.text(0.1, 0.8, r'$\\sigma =$' + f'{dbeta.std():.3f}', transform=ax.transAxes)\n",
    "ax.set_title(r'$W\\rightarrow\\mu$', size=20)\n",
    "\n",
    "ax = axes[1][0]\n",
    "dbeta = 100*(beta_obs[:,2] - beta_val[:,2])/beta_val[:,2]\n",
    "ax.hist(dbeta, bins[::2], alpha=0.9, density=True)\n",
    "ax.plot(bins, norm.pdf(bins, loc=dbeta.mean(), scale=dbeta.std()))\n",
    "ax.text(0.1, 0.9, r'$\\mu =$' + f'{dbeta.mean():.3f}', transform=ax.transAxes)\n",
    "ax.text(0.1, 0.8, r'$\\sigma =$' + f'{dbeta.std():.3f}', transform=ax.transAxes)\n",
    "ax.set_ylabel('trial')\n",
    "ax.set_xlabel(r'$\\frac{B_{obs} - B_{true}}{B_{true}}$ (%)')\n",
    "ax.set_title(r'$W\\rightarrow\\tau$', size=20)\n",
    "\n",
    "ax = axes[1][1]\n",
    "dbeta = 100*(beta_obs[:,3] - beta_val[:,3])/beta_val[:,3]\n",
    "ax.hist(dbeta, bins[::2], alpha=0.9, density=True)\n",
    "ax.plot(bins, norm.pdf(bins, loc=dbeta.mean(), scale=dbeta.std()))\n",
    "ax.text(0.1, 0.9, r'$\\mu =$' + f'{dbeta.mean():.3f}', transform=ax.transAxes)\n",
    "ax.text(0.1, 0.8, r'$\\sigma =$' + f'{dbeta.std():.3f}', transform=ax.transAxes)\n",
    "ax.set_xlabel(r'$\\frac{B_{obs} - B_{true}}{B_{true}}$ (%)')\n",
    "ax.set_title(r'$W\\rightarrow h$', size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/beta_bias.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.703Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(30, 15), facecolor='white', sharex=True, gridspec_kw={'height_ratios':[3,1]})\n",
    "\n",
    "df_pulls = pd.read_csv('local_data/pulls.csv').query('active == True')\n",
    "df_pulls.loc[:4, 'ratio'] *= 100\n",
    "df_pulls = df_pulls.set_index('name')\n",
    "#df_pulls = df_pulls.drop('top_pt')\n",
    "pull_post = (df_pulls['val_fit'] - df_pulls['val_init'])/df_pulls['err_init']\n",
    "\n",
    "nparams = params[mask].shape[0]\n",
    "xticks = np.outer(np.arange(nparams), np.ones(diff.shape[0])).T\n",
    "ax = axes[0]\n",
    "ax.plot(xticks+1,  diff, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff.mean(axis=0), diff.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "#ax.errorbar(xticks[0]+1,  pull_post.values, df_pulls['ratio'], fmt='C1o', capsize=10, elinewidth=5)\n",
    "ax.fill_between([-0.5, nparams+0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "#ax.boxplot(diff)\n",
    "\n",
    "# extra dressing\n",
    "ax.set_ylabel(r'$\\delta\\theta_{post}/\\delta\\theta_{pre}$')\n",
    "ax.set_xlim(3.5, nparams+0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "ax = axes[1]\n",
    "err_ratio = diff.std(axis=0)/df_pulls['ratio'].values\n",
    "ax.plot(xticks[0]+1,  err_ratio, 'ko', alpha=0.9, markersize=10)\n",
    "#ax.errorbar(xticks[0]+1,  diff.mean(axis=0), , fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params[mask].label, size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'toys/$\\mathcal{H}_{NLL}$')\n",
    "ax.set_ylim(0.25, 1.75)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/new_pulls.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.707Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10), facecolor='white')\n",
    "\n",
    "xticks = np.outer(np.arange(4), np.ones(diff.shape[0])).T\n",
    "ax.plot(xticks+1,  diff[:,:4], 'ko', alpha=0.1, markersize=4)\n",
    "#ax.boxplot(diff)\n",
    "ax.errorbar(xticks[0,:4]+1,  diff[:,:4].mean(axis=0), diff[:,:4].std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "ax.fill_between([-0.5, nparams+0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "print(diff[:,:4].std(axis=0))\n",
    "\n",
    "# extra dressing\n",
    "ax.set_xticks(xticks[0,:4]+1)\n",
    "ax.set_xticklabels(params.label[:4], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\delta\\theta/\\theta$ (%)')\n",
    "ax.set_xlim(0.5, nparams+0.5)\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/beta.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.710Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10), facecolor='white')\n",
    "\n",
    "ip_low, ip_high = 4, 7 + fit_data._nnorm\n",
    "xticks = np.outer(np.arange(ip_high-ip_low), np.ones(diff.shape[0])).T\n",
    "diff_trim = diff[:, ip_low:ip_high]\n",
    "ax.plot(xticks+1,  diff_trim, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff_trim.mean(axis=0), diff_trim.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "# extra dressing\n",
    "ax.fill_between([0.5, ip_high - ip_low + 0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params.label[ip_low:ip_high], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\sigma_{toys}/\\sigma_{pre}$')\n",
    "ax.set_xlim(0.5, ip_high - ip_low + 0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/norm_params.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.712Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10), facecolor='white')\n",
    "\n",
    "ip_low, ip_high = 18, 46\n",
    "xticks = np.outer(np.arange(ip_high-ip_low), np.ones(diff.shape[0])).T\n",
    "diff_trim = diff[:, ip_low:ip_high]\n",
    "ax.plot(xticks+1,  diff_trim, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff_trim.mean(axis=0), diff_trim.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "# extra dressing\n",
    "ax.fill_between([0.5, ip_high - ip_low + 0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params.label[ip_low:ip_high], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\sigma_{toys}/\\sigma_{pre}$')\n",
    "ax.set_xlim(0.5, ip_high - ip_low + 0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/shape_reco_params.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.718Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10), facecolor='white')\n",
    "\n",
    "ip_low, ip_high = 47, 65\n",
    "xticks = np.outer(np.arange(ip_high-ip_low), np.ones(diff.shape[0])).T\n",
    "diff_trim = diff[:, ip_low:ip_high]\n",
    "ax.plot(xticks+1,  diff_trim, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff_trim.mean(axis=0), diff_trim.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "# extra dressing\n",
    "ax.fill_between([0.5, ip_high - ip_low + 0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params.label[ip_low:ip_high], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\sigma_{toys}/\\sigma_{pre}$')\n",
    "ax.set_xlim(0.5, ip_high - ip_low + 0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/shape_btag_params.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-10T18:00:49.721Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10), facecolor='white')\n",
    "\n",
    "ip_low, ip_high = 65, 84\n",
    "xticks = np.outer(np.arange(ip_high-ip_low), np.ones(diff.shape[0])).T\n",
    "diff_trim = diff[:, ip_low:ip_high]\n",
    "ax.plot(xticks+1,  diff_trim, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff_trim.mean(axis=0), diff_trim.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "# extra dressing\n",
    "ax.fill_between([0.5, ip_high - ip_low + 0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params.label[ip_low:ip_high], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\sigma_{toys}/\\sigma_{pre}$')\n",
    "ax.set_xlim(0.5, ip_high - ip_low + 0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/shape_jes_params.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "29px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "49px",
    "left": "0px",
    "right": "1493.87px",
    "top": "90.9965px",
    "width": "242px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "677.85px",
    "left": "1071px",
    "right": "20px",
    "top": "170px",
    "width": "659px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "012f8bbe2fdb410dae6e2cde9d7fe5cb": {
     "views": []
    },
    "080556076f174648bddf64f17a54c523": {
     "views": []
    },
    "0ad83b5f67484ae5b8fd8dd43ccc39bd": {
     "views": []
    },
    "15acd81a9adc493683d9b63813f000bf": {
     "views": []
    },
    "1840cb6fded848b4ae95ec8d3db15ab2": {
     "views": []
    },
    "1dd83f822e074642ae4255b15ee661cf": {
     "views": []
    },
    "1e71a878e6474912a0efc497ecc5d65b": {
     "views": []
    },
    "2022ed83777b4963b630b5c46239e218": {
     "views": []
    },
    "21c4c57bfc48495194663e6a4fbac488": {
     "views": []
    },
    "22c45c75435348c0b9501d493d69fdca": {
     "views": []
    },
    "2635f668a1af4a9db2642e705d7c73ff": {
     "views": []
    },
    "2ad5ddd9347e451b9290e5b4179ab9a2": {
     "views": []
    },
    "2c062b5778024117984822b63b0593d7": {
     "views": []
    },
    "2f5eab2f6fb24192b76a5ffe99195d44": {
     "views": []
    },
    "31632517325046e8b0cb62e4f4ed2480": {
     "views": []
    },
    "3562b97192ed4d42bbab17f77c290f6b": {
     "views": []
    },
    "38a7cc053723492b921cf9f084ed243c": {
     "views": []
    },
    "3b5750b20e1745879ca0f965aad7b614": {
     "views": []
    },
    "3f1cbabbe2694a9dabe3f1c2e09d0ee2": {
     "views": []
    },
    "3fab6a26a70c4238a668a46d4dc88bf6": {
     "views": []
    },
    "3fb3c7a25e954a4888996976fa107737": {
     "views": []
    },
    "415db64fbc574daea8457ab600392f09": {
     "views": []
    },
    "4463de406b4645a4b562fe7917380ff9": {
     "views": []
    },
    "487e5450b5a24507932709f1fa8f59c1": {
     "views": []
    },
    "48aba73013e74e71927f71d42fb44d14": {
     "views": []
    },
    "4a19ad30f77e4fe6a2c84c8b62378a47": {
     "views": []
    },
    "4bc83ff5270d41679d76d26cdded8313": {
     "views": []
    },
    "4bdd9dd5c5c64646a27fa9096851458b": {
     "views": []
    },
    "5014cd42705f45178d5e6eeffd70f119": {
     "views": []
    },
    "5259b340b68e4fdb97fb4eaf9d98d954": {
     "views": []
    },
    "5986ef5b605a42aca10bc5834529ee06": {
     "views": []
    },
    "5c1aa44589a140eb9709734c843abde6": {
     "views": []
    },
    "600cd9ca4f4c46d4ad6fe57df107675a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "61fe369ebdd14eaa89de110f6186e6b7": {
     "views": []
    },
    "62ac836017ae47a38f8fde806c5ec9b7": {
     "views": []
    },
    "632ccbfabe91405aa1c5a77c9ea754db": {
     "views": []
    },
    "66175f618ea5472baac618f998d2c06c": {
     "views": []
    },
    "6647a620af034d26abcd327ae02364d4": {
     "views": []
    },
    "6752222d2cba43e18f344a8db7f99d24": {
     "views": []
    },
    "6b684ba1a7c24a35ba2df77016212904": {
     "views": []
    },
    "6cea898f4aca4f1e84601f843e337238": {
     "views": []
    },
    "6f8d1e87fd60462a89d693b2f3b5f007": {
     "views": []
    },
    "74078646a5eb4047b40370a0ab8b6b30": {
     "views": []
    },
    "745b0c79ff3040788ea952fce9c7d607": {
     "views": []
    },
    "757c9b805eb7445bac9a7f141f87e45f": {
     "views": []
    },
    "76ced68e19a742e8976dbfd4e8594a1a": {
     "views": []
    },
    "783bb5e7538d4d9d8315e2698024b353": {
     "views": []
    },
    "794993d66efe4ab29a8d35aad8cfe079": {
     "views": []
    },
    "8375e24bae7541528d7cdc0f379d1d4c": {
     "views": []
    },
    "8554945ec15041a7bf8004dbc3fc5f11": {
     "views": []
    },
    "878a34e26cce4f18bb8232a682ebe964": {
     "views": []
    },
    "8921a75116a549198eb7b7f4a24ab672": {
     "views": []
    },
    "909f4504f0b049bda8b641defa177062": {
     "views": []
    },
    "910b9d32a3fb45ec99da1f9df1add816": {
     "views": []
    },
    "9d15ce601cd34f0699b7a7a0ce1d17dc": {
     "views": []
    },
    "a26638c9fee247b3891aac027a0918cc": {
     "views": []
    },
    "a9d2bf44a3ad447bb3eecde71363c198": {
     "views": []
    },
    "ad366bf4c95f4cdba62d47ba9501efc9": {
     "views": []
    },
    "ad8e1842ec314a94b6ed4b62c4c0a450": {
     "views": []
    },
    "af525094db304d2a812ae1312b00889b": {
     "views": []
    },
    "b0697c4343da491f9a35bf02681dad8f": {
     "views": []
    },
    "b07ff307919e4268bc8bec8379c47a5d": {
     "views": []
    },
    "b0e85c726ca141079333afb27edc63d4": {
     "views": []
    },
    "bdcc1e5df7a8432b9f40d8249a46f90a": {
     "views": []
    },
    "be1065f37fa24e818d31c3bb075947a3": {
     "views": []
    },
    "c296c8df2f734e268c6c1204536e7142": {
     "views": []
    },
    "c4bfd3e447f0426da144b76abc202129": {
     "views": []
    },
    "cced93184d4445218a2b14567579333d": {
     "views": []
    },
    "d5bd2e4d5f85482e9345f3a7a69380d0": {
     "views": []
    },
    "d798fa64e8be4a7d9ec1cbeece3b1be9": {
     "views": []
    },
    "d7aec0d6d05f442b991ab40af944811d": {
     "views": []
    },
    "db469cea2c8e4180bf6890de80329c1d": {
     "views": []
    },
    "e671857510c54634b6f0fa55bf1fa228": {
     "views": []
    },
    "ebf52deafaf64b0c826533dafdf993c0": {
     "views": []
    },
    "ed1e5439da9c41199a7bbbda21b556f8": {
     "views": []
    },
    "f585cf5db5024280af5b567f0e4fd771": {
     "views": []
    },
    "f6ba8f8800af47adabed847063bda8db": {
     "views": []
    },
    "f9bae72f14e44705b5c38a3ddc69fee8": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
