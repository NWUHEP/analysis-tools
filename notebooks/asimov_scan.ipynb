{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bias studies with full systematics\n",
    "\n",
    "To assess the impact of various sources of systematic, we will rely on an Asimov dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:14:30.532Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## imports and configuration\n",
    "%cd '/home/naodell/work/wbr/analysis'\n",
    "#%load_ext autoreload\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.optimize import minimize, basinhopping\n",
    "from scipy.stats import norm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scripts.plot_tools as pt\n",
    "import scripts.fit_helpers as fh\n",
    "from nllfit.nllfitter import ScanParameters\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "rc_params = {\n",
    "             'figure.figsize': (10, 10),\n",
    "             'axes.labelsize': 20,\n",
    "             'axes.facecolor': 'white',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'legend.fontsize': 20,\n",
    "             'xtick.labelsize':20,\n",
    "             'ytick.labelsize':20,\n",
    "             'font.size':18,\n",
    "             'font.sans-serif':['Arial', 'sans-serif'],\n",
    "             'mathtext.sf':'Arial',\n",
    "             'lines.markersize':8.,\n",
    "             'lines.linewidth':2.5,\n",
    "            }\n",
    "matplotlib.rcParams.update(rc_params)\n",
    "\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T02:13:49.022598Z",
     "start_time": "2020-03-02T02:13:44.172102Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# configure, get the input data, and do any additional processing that is needed\n",
    "input_dir  = f'local_data/templates/updated_e_sf/'\n",
    "processes = ['ttbar', 't', 'ww', 'wjets', 'zjets_alt', 'diboson', 'fakes'] \n",
    "selections = [\n",
    "              'ee', 'mumu',  \n",
    "              'emu', \n",
    "              'mutau', 'etau', \n",
    "              'mu4j', 'e4j'\n",
    "             ]\n",
    "plot_labels = fh.fancy_labels\n",
    "\n",
    "# initialize fit data\n",
    "fit_data = fh.FitData(input_dir, selections, processes, process_cut=0.05)\n",
    "params = fit_data._parameters\n",
    "params_pre = fit_data.get_params_init().values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T02:13:49.027287Z",
     "start_time": "2020-03-02T02:13:49.023871Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# generate scan points\n",
    "beta_scan_vals = ScanParameters(['beta_e', 'beta_mu', 'beta_tau'], \n",
    "                                [(0.104, 0.118), (0.104, 0.118), (0.104, 0.118)], \n",
    "                                [5, 5, 5]\n",
    "                                )\n",
    "scan_vals = np.array(beta_scan_vals.get_scan_vals()[0])\n",
    "beta_h = np.transpose([1 - np.sum(scan_vals, axis=1)])\n",
    "scan_vals = np.hstack((scan_vals, beta_h, np.outer(np.ones(scan_vals.shape[0]), params_pre[4:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.027Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naodell/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0f1d794d2144039c3a885659dba16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 134.806564\n",
      "         Iterations: 76\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 113\n",
      "  False 2401.1671635241673 2844.233480443386 134.80656383671402\n",
      " scan vals:  [0.104 0.104 0.104 0.688]\n",
      " fit vals: [0.1046 0.1039 0.1026 0.6889]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 119.136078\n",
      "         Iterations: 68\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 120\n",
      "  False 2387.740302102886 2729.23712852464 119.13607783176576\n",
      " scan vals:  [0.104  0.104  0.1075 0.6845]\n",
      " fit vals: [0.1037 0.1038 0.105  0.6874]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 116.851907\n",
      "         Iterations: 71\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 109\n",
      "  False 2402.1449534939147 2669.109743295081 116.85190686865728\n",
      " scan vals:  [0.104 0.104 0.111 0.681]\n",
      " fit vals: [0.104  0.1034 0.107  0.6856]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 119.433492\n",
      "         Iterations: 76\n",
      "         Function evaluations: 176\n",
      "         Gradient evaluations: 165\n",
      "  False 2394.9415245010928 2613.930165490221 119.43349216408392\n",
      " scan vals:  [0.104  0.104  0.1145 0.6775]\n",
      " fit vals: [0.105  0.1048 0.1144 0.6759]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 120.209507\n",
      "         Iterations: 71\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 110\n",
      "  False 2401.4362577152433 2598.5727513667343 120.20950736890805\n",
      " scan vals:  [0.104 0.104 0.118 0.674]\n",
      " fit vals: [0.1042 0.1045 0.119  0.6723]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 122.100128\n",
      "         Iterations: 62\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 101\n",
      "  False 2406.6299951349415 2664.7367613420297 122.10012773865962\n",
      " scan vals:  [0.104  0.1075 0.104  0.6845]\n",
      " fit vals: [0.104  0.1076 0.1043 0.6841]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 122.442167\n",
      "         Iterations: 59\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 138\n",
      "  False 2376.685368131871 2571.8509354537796 122.4421665597113\n",
      " scan vals:  [0.104  0.1075 0.1075 0.681 ]\n",
      " fit vals: [0.1044 0.1075 0.1074 0.6807]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 123.159760\n",
      "         Iterations: 57\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 117\n",
      "  False 2391.70121421778 2550.9086342304436 123.15976046659995\n",
      " scan vals:  [0.104  0.1075 0.111  0.6775]\n",
      " fit vals: [0.1035 0.1072 0.1133 0.676 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 116.107179\n",
      "         Iterations: 65\n",
      "         Function evaluations: 153\n",
      "         Gradient evaluations: 142\n",
      "  False 2388.1237693488183 2537.8926422084437 116.10717899401402\n",
      " scan vals:  [0.104  0.1075 0.1145 0.674 ]\n",
      " fit vals: [0.1046 0.1084 0.1148 0.6722]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 111.678937\n",
      "         Iterations: 55\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 93\n",
      "  False 2391.626732574201 2558.0608401531135 111.67893714125437\n",
      " scan vals:  [0.104  0.1075 0.118  0.6705]\n",
      " fit vals: [0.1034 0.1075 0.1186 0.6705]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 119.815113\n",
      "         Iterations: 52\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 126\n",
      "  False 2392.996847188623 3357.5854277598883 119.81511267562024\n",
      " scan vals:  [0.104 0.111 0.104 0.681]\n",
      " fit vals: [0.104  0.1109 0.1037 0.6814]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 112.449690\n",
      "         Iterations: 49\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 80\n",
      "  False 2387.8641170272963 3324.6531260958614 112.44968988384909\n",
      " scan vals:  [0.104  0.111  0.1075 0.6775]\n",
      " fit vals: [0.1043 0.1119 0.1104 0.6735]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 127.342234\n",
      "         Iterations: 48\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 118\n",
      "  False 2396.6256701849256 3332.607530320463 127.34223381025639\n",
      " scan vals:  [0.104 0.111 0.111 0.674]\n",
      " fit vals: [0.1037 0.1111 0.1111 0.6741]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 136.543488\n",
      "         Iterations: 57\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 132\n",
      "  False 2402.1062648778698 3363.825176865091 136.54348781230235\n",
      " scan vals:  [0.104  0.111  0.1145 0.6705]\n",
      " fit vals: [0.103  0.1104 0.1131 0.6736]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 112.430402\n",
      "         Iterations: 54\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "  False 2395.7200116976965 3409.3177436783 112.43040232515426\n",
      " scan vals:  [0.104 0.111 0.118 0.667]\n",
      " fit vals: [0.1048 0.1122 0.1191 0.6638]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 116.669283\n",
      "         Iterations: 69\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 125\n",
      "  False 2393.401429487531 4913.2892054109025 116.66928257430358\n",
      " scan vals:  [0.104  0.1145 0.104  0.6775]\n",
      " fit vals: [0.1034 0.1137 0.1054 0.6776]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 120.332492\n",
      "         Iterations: 70\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 121\n",
      "  False 2400.7192176276353 4924.848553905174 120.33249174024067\n",
      " scan vals:  [0.104  0.1145 0.1075 0.674 ]\n",
      " fit vals: [0.1042 0.1145 0.1102 0.671 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 118.390255\n",
      "         Iterations: 62\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 93\n",
      "  False 2391.9743557117513 4947.4023327224595 118.39025498195078\n",
      " scan vals:  [0.104  0.1145 0.111  0.6705]\n",
      " fit vals: [0.1048 0.1153 0.1113 0.6686]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 124.674909\n",
      "         Iterations: 68\n",
      "         Function evaluations: 148\n",
      "         Gradient evaluations: 137\n",
      "  False 2399.8871716478634 5013.235305179859 124.67490915190382\n",
      " scan vals:  [0.104  0.1145 0.1145 0.667 ]\n",
      " fit vals: [0.1038 0.1148 0.1127 0.6687]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 113.370425\n",
      "         Iterations: 68\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 132\n",
      "  False 2404.9087880737297 5102.407158705563 113.3704253394685\n",
      " scan vals:  [0.104  0.1145 0.118  0.6635]\n",
      " fit vals: [0.1027 0.1136 0.1151 0.6685]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 136.691529\n",
      "         Iterations: 69\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 157\n",
      "  False 2399.5601625206323 7285.788275403615 136.69152903963257\n",
      " scan vals:  [0.104 0.118 0.104 0.674]\n",
      " fit vals: [0.104  0.1181 0.1035 0.6744]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 131.664397\n",
      "         Iterations: 72\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 106\n",
      "  False 2391.495130293506 7311.236385456607 131.66439742102875\n",
      " scan vals:  [0.104  0.118  0.1075 0.6705]\n",
      " fit vals: [0.1043 0.1191 0.1082 0.6683]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 130.938089\n",
      "         Iterations: 65\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 129\n",
      "  False 2412.0145090485235 7392.438682489396 130.93808879744577\n",
      " scan vals:  [0.104 0.118 0.111 0.667]\n",
      " fit vals: [0.1042 0.118  0.1106 0.6672]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 110.115860\n",
      "         Iterations: 71\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 106\n",
      "  False 2390.798283356045 7458.649940898058 110.11585969969188\n",
      " scan vals:  [0.104  0.118  0.1145 0.6635]\n",
      " fit vals: [0.104  0.1181 0.1184 0.6594]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 126.470763\n",
      "         Iterations: 71\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 135\n",
      "  False 2405.1872564272912 7586.828391880048 126.47076317012618\n",
      " scan vals:  [0.104 0.118 0.118 0.66 ]\n",
      " fit vals: [0.1033 0.1176 0.1177 0.6614]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 120.542982\n",
      "         Iterations: 62\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 113\n",
      "  False 2397.484749946216 2465.812219244202 120.54298164090446\n",
      " scan vals:  [0.1075 0.104  0.104  0.6845]\n",
      " fit vals: [0.1081 0.1039 0.1035 0.6846]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 119.354807\n",
      "         Iterations: 63\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 121\n",
      "  False 2383.8471086619675 2380.456179106304 119.35480710005521\n",
      " scan vals:  [0.1075 0.104  0.1075 0.681 ]\n",
      " fit vals: [0.1067 0.1034 0.1089 0.681 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 117.093239\n",
      "         Iterations: 64\n",
      "         Function evaluations: 178\n",
      "         Gradient evaluations: 166\n",
      "  False 2397.4268465254518 2349.4338323914144 117.09323932489269\n",
      " scan vals:  [0.1075 0.104  0.111  0.6775]\n",
      " fit vals: [0.1076 0.1036 0.1135 0.6752]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 109.790061\n",
      "         Iterations: 61\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 107\n",
      "  False 2383.85401032766 2317.9017424204835 109.79006058822945\n",
      " scan vals:  [0.1075 0.104  0.1145 0.674 ]\n",
      " fit vals: [0.1074 0.1036 0.1144 0.6746]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 107.284267\n",
      "         Iterations: 61\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 112\n",
      "  False 2396.1137044905686 2338.4201421573184 107.28426670721839\n",
      " scan vals:  [0.1075 0.104  0.118  0.6705]\n",
      " fit vals: [0.1069 0.103  0.1161 0.674 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 124.986440\n",
      "         Iterations: 52\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 124\n",
      "  False 2390.0273263430818 2344.1448683942754 124.98643992989383\n",
      " scan vals:  [0.1075 0.1075 0.104  0.681 ]\n",
      " fit vals: [0.1072 0.1074 0.105  0.6804]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 121.677250\n",
      "         Iterations: 50\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 121\n",
      "  False 2399.1604678451927 2320.6876339527103 121.67725012475474\n",
      " scan vals:  [0.1075 0.1075 0.1075 0.6775]\n",
      " fit vals: [0.107  0.1076 0.1073 0.6781]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 129.571456\n",
      "         Iterations: 44\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "  False 2394.6121904629645 2310.6131659556754 129.57145558649847\n",
      " scan vals:  [0.1075 0.1075 0.111  0.674 ]\n",
      " fit vals: [0.1063 0.1063 0.1068 0.6806]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 98.380237\n",
      "         Iterations: 51\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 124\n",
      "  False 2390.677738185234 2327.761297129022 98.3802372416078\n",
      " scan vals:  [0.1075 0.1075 0.1145 0.6705]\n",
      " fit vals: [0.1073 0.1078 0.115  0.6699]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 125.247405\n",
      "         Iterations: 54\n",
      "         Function evaluations: 146\n",
      "         Gradient evaluations: 134\n",
      "  False 2394.3159762605974 2378.682040955493 125.24740515703934\n",
      " scan vals:  [0.1075 0.1075 0.118  0.667 ]\n",
      " fit vals: [0.1076 0.1076 0.1177 0.6671]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 151.056918\n",
      "         Iterations: 55\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 127\n",
      "  False 2404.5095747677183 3137.4840171187034 151.05691763808463\n",
      " scan vals:  [0.1075 0.111  0.104  0.6775]\n",
      " fit vals: [0.1064 0.1098 0.1041 0.6797]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 104.943016\n",
      "         Iterations: 57\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "  False 2383.954004240609 3119.991665225447 104.9430161158867\n",
      " scan vals:  [0.1075 0.111  0.1075 0.674 ]\n",
      " fit vals: [0.108  0.1113 0.1094 0.6713]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 121.204064\n",
      "         Iterations: 60\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "  False 2389.539289007003 3155.7154851408027 121.20406404266146\n",
      " scan vals:  [0.1075 0.111  0.111  0.6705]\n",
      " fit vals: [0.1069 0.11   0.1078 0.6753]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 123.192293\n",
      "         Iterations: 56\n",
      "         Function evaluations: 166\n",
      "         Gradient evaluations: 155\n",
      "  False 2405.3975603153103 3228.347401942834 123.19229273351442\n",
      " scan vals:  [0.1075 0.111  0.1145 0.667 ]\n",
      " fit vals: [0.1074 0.1109 0.1138 0.6679]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 134.554216\n",
      "         Iterations: 57\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 110\n",
      "  False 2387.940637335271 3293.9037377199243 134.554215719447\n",
      " scan vals:  [0.1075 0.111  0.118  0.6635]\n",
      " fit vals: [0.1075 0.111  0.1185 0.663 ]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 132.162604\n",
      "         Iterations: 73\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "  False 2416.2454082369086 4778.508039154596 132.16260350115428\n",
      " scan vals:  [0.1075 0.1145 0.104  0.674 ]\n",
      " fit vals: [0.1068 0.114  0.0997 0.6795]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 132.424858\n",
      "         Iterations: 63\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "  False 2396.1203143279777 4794.008105974104 132.42485771285172\n",
      " scan vals:  [0.1075 0.1145 0.1075 0.6705]\n",
      " fit vals: [0.1073 0.1146 0.1093 0.6688]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 128.185224\n",
      "         Iterations: 63\n",
      "         Function evaluations: 158\n",
      "         Gradient evaluations: 147\n",
      "  False 2391.75956474623 4852.413830357351 128.1852239478352\n",
      " scan vals:  [0.1075 0.1145 0.111  0.667 ]\n",
      " fit vals: [0.108  0.1151 0.1142 0.6627]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 130.104238\n",
      "         Iterations: 70\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 130\n",
      "  False 2391.240502045808 4941.374996373829 130.1042375855294\n",
      " scan vals:  [0.1075 0.1145 0.1145 0.6635]\n",
      " fit vals: [0.1076 0.1149 0.1166 0.6609]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 125.221706\n",
      "         Iterations: 71\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 139\n",
      "  False 2394.7573782884238 5060.701340174567 125.22170561713453\n",
      " scan vals:  [0.1075 0.1145 0.118  0.66  ]\n",
      " fit vals: [0.1083 0.115  0.1188 0.6579]\n",
      "\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 124.451292\n",
      "         Iterations: 71\n",
      "         Function evaluations: 162\n",
      "         Gradient evaluations: 151\n",
      "  False 2404.578165971622 7208.763114282153 124.45129204213352\n",
      " scan vals:  [0.1075 0.118  0.104  0.6705]\n",
      " fit vals: [0.1077 0.1179 0.1033 0.6711]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 122.637385\n",
      "         Iterations: 77\n",
      "         Function evaluations: 152\n",
      "         Gradient evaluations: 142\n",
      "  False 2403.5302402394295 7273.138790833726 122.63738502568597\n",
      " scan vals:  [0.1075 0.118  0.1075 0.667 ]\n",
      " fit vals: [0.1076 0.1174 0.1042 0.6708]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# carry out the scan #\n",
    "\n",
    "# minimizer options\n",
    "min_options = dict(\n",
    "                   #eps=1e-10,\n",
    "                   #gtol=1e-2, \n",
    "                   disp=True\n",
    "                  )\n",
    "\n",
    "results = []\n",
    "cost = []\n",
    "sv_accept = []\n",
    "mask = fit_data._pmask\n",
    "pinit = params_pre[mask]\n",
    "for sv in tqdm(scan_vals):\n",
    "    \n",
    "    # randomize n.p.\n",
    "    mask[:4] = False\n",
    "    np_random = params_pre[mask] + fit_data._perr_init[mask]*np.random.randn(mask.sum())\n",
    "    fit_data._pval_init[mask] = np_random\n",
    "    #sv[mask] = np_random\n",
    "    mask[:4] = True\n",
    "    \n",
    "    # generate data from scan values w/ statistical variation\n",
    "    sample = dict()\n",
    "    for category in fit_data._categories:\n",
    "        val, var = fit_data.mixture_model(sv, category)\n",
    "        sample[category] = [np.random.poisson(val), val]\n",
    "        #sample[category] = [val, val]\n",
    "\n",
    "    # carry out minimization\n",
    "    result = minimize(fit_data.objective, pinit,\n",
    "                      method  = 'BFGS', \n",
    "                      options = min_options,\n",
    "                      jac     = fit_data.objective_jacobian,\n",
    "                      args    = (sample)\n",
    "                     )\n",
    "    print(' ', result.success, fit_data.objective(pinit), fit_data.objective(sv[mask]), result.fun)\n",
    "    #print(' jacobian: ', result.jac)\n",
    "    #print(' init vals: ', pinit)\n",
    "    #print(' nps: ', np_random)\n",
    "    print(' scan vals: ', sv[:4])\n",
    "    print(' fit vals:', result.x[:4], end='\\n\\n')\n",
    "\n",
    "    results.append(result.x)\n",
    "    cost.append(result.fun)\n",
    "    sv_accept.append(sv[mask])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.031Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#calculate biases\n",
    "results = np.array(results)\n",
    "sv_accept = np.array(sv_accept)\n",
    "cost = np.array(cost)\n",
    "\n",
    "diff = (results - sv_accept)\n",
    "diff[:,:4] /= 0.01*pinit[:4]\n",
    "#diff[:,:4] /= 0.01*sv_accept[:,:4]\n",
    "diff[:,4:diff.shape[1]] /= params['err_init'][4:diff.shape[1]].values\n",
    "\n",
    "#diff = np.array([d for d in diff if np.all((d > -10) & (d < 10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.034Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the cost\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10), facecolor='white')\n",
    "\n",
    "cost = cost[cost<600]\n",
    "cost_mean, cost_err = cost.mean(), cost.std()\n",
    "ax.hist(cost, bins=np.linspace(np.max([0, cost_mean - 5*cost_err]), cost_mean + 5*cost_err, 25), histtype='stepfilled')\n",
    "ax.set_xlabel(r'$NLL_{fit}$')\n",
    "ax.set_ylabel('Entries')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.037Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# branching fraction scans\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), facecolor='white', sharex=False, sharey=False)\n",
    "\n",
    "beta_val = sv_accept[:,:4]\n",
    "beta_obs = results[:,:4]\n",
    "\n",
    "ax = axes[0][0]\n",
    "ax.plot(beta_val[:,0], beta_obs[:,0], 'C0o', alpha=0.1)\n",
    "ax.plot([0.1, 0.12], [0.1, 0.12], 'r:')\n",
    "ax.set_xlim(0.102, 0.12)\n",
    "ax.set_ylim(0.102, 0.12)\n",
    "ax.set_ylabel(r'$B_{obs.}$')\n",
    "ax.set_title(r'$W\\rightarrow e$', size=20)\n",
    "\n",
    "ax = axes[0][1]\n",
    "ax.plot(beta_val[:,1], beta_obs[:,1], 'C0o', alpha=0.1)\n",
    "ax.plot([0.1, 0.12], [0.1, 0.12], 'r:')\n",
    "ax.set_xlim(0.102, 0.12)\n",
    "ax.set_ylim(0.102, 0.12)\n",
    "ax.set_title(r'$W\\rightarrow\\mu$', size=20)\n",
    "\n",
    "ax = axes[1][0]\n",
    "ax.plot(beta_val[:,2], beta_obs[:,2], 'C0o', alpha=0.1)\n",
    "ax.plot([0.1, 0.12], [0.1, 0.12], 'r:')\n",
    "ax.set_xlim(0.102, 0.12)\n",
    "ax.set_ylim(0.102, 0.12)\n",
    "ax.set_ylabel(r'$B_{obs.}$')\n",
    "ax.set_xlabel(r'$B_{true}$')\n",
    "ax.set_title(r'$W\\rightarrow\\tau$', size=20)\n",
    "\n",
    "ax = axes[1][1]\n",
    "ax.plot(beta_val[:,3], beta_obs[:,3], 'C0o', alpha=0.1)\n",
    "ax.plot([0.64, 0.72], [0.64, 0.72], 'r:')\n",
    "ax.set_xlim(0.64, 0.695)\n",
    "ax.set_ylim(0.64, 0.695)\n",
    "ax.set_xlabel(r'$B_{true}$')\n",
    "ax.set_title(r'$W\\rightarrow h$', size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/beta_scan.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.041Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# branching fraction scans\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), facecolor='white', sharex=True, sharey=False)\n",
    "bins = np.linspace(-5, 5, 80)\n",
    "\n",
    "ax = axes[0][0]\n",
    "dbeta = 100*(beta_obs[:,0] - beta_val[:,0])/beta_val[:,0]\n",
    "ax.hist(dbeta, bins[::2], alpha=0.9, density=True)\n",
    "ax.plot(bins, norm.pdf(bins, loc=dbeta.mean(), scale=dbeta.std()))\n",
    "ax.text(0.1, 0.9, r'$\\mu =$' + f'{dbeta.mean():.3f}', transform=ax.transAxes)\n",
    "ax.text(0.1, 0.8, r'$\\sigma =$' + f'{dbeta.std():.3f}', transform=ax.transAxes)\n",
    "ax.set_xlim(-5., 5.)\n",
    "ax.set_ylim(0., None)\n",
    "ax.set_ylabel('trial')\n",
    "ax.set_title(r'$W\\rightarrow e$', size=20)\n",
    "\n",
    "ax = axes[0][1]\n",
    "dbeta = 100*(beta_obs[:,1] - beta_val[:,1])/beta_val[:,1]\n",
    "ax.hist(dbeta, bins[::2], alpha=0.9, density=True)\n",
    "ax.plot(bins, norm.pdf(bins, loc=dbeta.mean(), scale=dbeta.std()))\n",
    "ax.text(0.1, 0.9, r'$\\mu =$' + f'{dbeta.mean():.3f}', transform=ax.transAxes)\n",
    "ax.text(0.1, 0.8, r'$\\sigma =$' + f'{dbeta.std():.3f}', transform=ax.transAxes)\n",
    "ax.set_title(r'$W\\rightarrow\\mu$', size=20)\n",
    "\n",
    "ax = axes[1][0]\n",
    "dbeta = 100*(beta_obs[:,2] - beta_val[:,2])/beta_val[:,2]\n",
    "ax.hist(dbeta, bins[::2], alpha=0.9, density=True)\n",
    "ax.plot(bins, norm.pdf(bins, loc=dbeta.mean(), scale=dbeta.std()))\n",
    "ax.text(0.1, 0.9, r'$\\mu =$' + f'{dbeta.mean():.3f}', transform=ax.transAxes)\n",
    "ax.text(0.1, 0.8, r'$\\sigma =$' + f'{dbeta.std():.3f}', transform=ax.transAxes)\n",
    "ax.set_ylabel('trial')\n",
    "ax.set_xlabel(r'$\\frac{B_{obs} - B_{true}}{B_{true}}$ (%)')\n",
    "ax.set_title(r'$W\\rightarrow\\tau$', size=20)\n",
    "\n",
    "ax = axes[1][1]\n",
    "dbeta = 100*(beta_obs[:,3] - beta_val[:,3])/beta_val[:,3]\n",
    "ax.hist(dbeta, bins[::2], alpha=0.9, density=True)\n",
    "ax.plot(bins, norm.pdf(bins, loc=dbeta.mean(), scale=dbeta.std()))\n",
    "ax.text(0.1, 0.9, r'$\\mu =$' + f'{dbeta.mean():.3f}', transform=ax.transAxes)\n",
    "ax.text(0.1, 0.8, r'$\\sigma =$' + f'{dbeta.std():.3f}', transform=ax.transAxes)\n",
    "ax.set_xlabel(r'$\\frac{B_{obs} - B_{true}}{B_{true}}$ (%)')\n",
    "ax.set_title(r'$W\\rightarrow h$', size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/beta_bias.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.045Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(30, 15), facecolor='white', sharex=True, gridspec_kw={'height_ratios':[3,1]})\n",
    "\n",
    "df_pulls = pd.read_csv('local_data/pulls.csv').query('active == True')\n",
    "df_pulls.loc[:4, 'ratio'] *= 100\n",
    "df_pulls = df_pulls.set_index('name')\n",
    "#df_pulls = df_pulls.drop('top_pt')\n",
    "pull_post = (df_pulls['val_fit'] - df_pulls['val_init'])/df_pulls['err_init']\n",
    "\n",
    "nparams = params[mask].shape[0]\n",
    "xticks = np.outer(np.arange(nparams), np.ones(diff.shape[0])).T\n",
    "ax = axes[0]\n",
    "ax.plot(xticks+1,  diff, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff.mean(axis=0), diff.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "#ax.errorbar(xticks[0]+1,  pull_post.values, df_pulls['ratio'], fmt='C1o', capsize=10, elinewidth=5)\n",
    "ax.fill_between([-0.5, nparams+0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "#ax.boxplot(diff)\n",
    "\n",
    "# extra dressing\n",
    "ax.set_ylabel(r'$\\delta\\theta_{post}/\\delta\\theta_{pre}$')\n",
    "ax.set_xlim(3.5, nparams+0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "ax = axes[1]\n",
    "err_ratio = diff.std(axis=0)/df_pulls['ratio'].values\n",
    "ax.plot(xticks[0]+1,  err_ratio, 'ko', alpha=0.9, markersize=10)\n",
    "#ax.errorbar(xticks[0]+1,  diff.mean(axis=0), , fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params[mask].label, size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'toys/$\\mathcal{H}_{NLL}$')\n",
    "ax.set_ylim(0.25, 1.75)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/new_pulls.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.047Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10), facecolor='white')\n",
    "\n",
    "xticks = np.outer(np.arange(4), np.ones(diff.shape[0])).T\n",
    "ax.plot(xticks+1,  diff[:,:4], 'ko', alpha=0.1, markersize=4)\n",
    "#ax.boxplot(diff)\n",
    "ax.errorbar(xticks[0,:4]+1,  diff[:,:4].mean(axis=0), diff[:,:4].std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "ax.fill_between([-0.5, nparams+0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "print(diff[:,:4].std(axis=0))\n",
    "\n",
    "# extra dressing\n",
    "ax.set_xticks(xticks[0,:4]+1)\n",
    "ax.set_xticklabels(params.label[:4], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\delta\\theta/\\theta$ (%)')\n",
    "ax.set_xlim(0.5, nparams+0.5)\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/beta.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.050Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10), facecolor='white')\n",
    "\n",
    "ip_low, ip_high = 4, 7 + fit_data._nnorm\n",
    "xticks = np.outer(np.arange(ip_high-ip_low), np.ones(diff.shape[0])).T\n",
    "diff_trim = diff[:, ip_low:ip_high]\n",
    "ax.plot(xticks+1,  diff_trim, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff_trim.mean(axis=0), diff_trim.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "# extra dressing\n",
    "ax.fill_between([0.5, ip_high - ip_low + 0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params.label[ip_low:ip_high], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\sigma_{toys}/\\sigma_{pre}$')\n",
    "ax.set_xlim(0.5, ip_high - ip_low + 0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/norm_params.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.053Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10), facecolor='white')\n",
    "\n",
    "ip_low, ip_high = 18, 46\n",
    "xticks = np.outer(np.arange(ip_high-ip_low), np.ones(diff.shape[0])).T\n",
    "diff_trim = diff[:, ip_low:ip_high]\n",
    "ax.plot(xticks+1,  diff_trim, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff_trim.mean(axis=0), diff_trim.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "# extra dressing\n",
    "ax.fill_between([0.5, ip_high - ip_low + 0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params.label[ip_low:ip_high], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\sigma_{toys}/\\sigma_{pre}$')\n",
    "ax.set_xlim(0.5, ip_high - ip_low + 0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/shape_reco_params.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.056Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10), facecolor='white')\n",
    "\n",
    "ip_low, ip_high = 47, 65\n",
    "xticks = np.outer(np.arange(ip_high-ip_low), np.ones(diff.shape[0])).T\n",
    "diff_trim = diff[:, ip_low:ip_high]\n",
    "ax.plot(xticks+1,  diff_trim, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff_trim.mean(axis=0), diff_trim.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "# extra dressing\n",
    "ax.fill_between([0.5, ip_high - ip_low + 0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params.label[ip_low:ip_high], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\sigma_{toys}/\\sigma_{pre}$')\n",
    "ax.set_xlim(0.5, ip_high - ip_low + 0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/shape_btag_params.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T02:13:44.057Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plotting the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10), facecolor='white')\n",
    "\n",
    "ip_low, ip_high = 65, 84\n",
    "xticks = np.outer(np.arange(ip_high-ip_low), np.ones(diff.shape[0])).T\n",
    "diff_trim = diff[:, ip_low:ip_high]\n",
    "ax.plot(xticks+1,  diff_trim, 'ko', alpha=0.1, markersize=4, )\n",
    "ax.errorbar(xticks[0]+1,  diff_trim.mean(axis=0), diff_trim.std(axis=0), fmt='C0o', capsize=10, elinewidth=5)\n",
    "\n",
    "# extra dressing\n",
    "ax.fill_between([0.5, ip_high - ip_low + 0.5], [-1, -1], [1, 1], color='C0', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(xticks[0]+1)\n",
    "ax.set_xticklabels(params.label[ip_low:ip_high], size=24)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "ax.set_ylabel(r'$\\sigma_{toys}/\\sigma_{pre}$')\n",
    "ax.set_xlim(0.5, ip_high - ip_low + 0.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.grid(linestyle='--', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/systematics/bias_tests/shape_jes_params.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "29px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "49px",
    "left": "0px",
    "right": "1493.87px",
    "top": "90.9965px",
    "width": "242px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "677.85px",
    "left": "1071px",
    "right": "20px",
    "top": "170px",
    "width": "659px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "012f8bbe2fdb410dae6e2cde9d7fe5cb": {
     "views": []
    },
    "080556076f174648bddf64f17a54c523": {
     "views": []
    },
    "0ad83b5f67484ae5b8fd8dd43ccc39bd": {
     "views": []
    },
    "15acd81a9adc493683d9b63813f000bf": {
     "views": []
    },
    "1840cb6fded848b4ae95ec8d3db15ab2": {
     "views": []
    },
    "1dd83f822e074642ae4255b15ee661cf": {
     "views": []
    },
    "1e71a878e6474912a0efc497ecc5d65b": {
     "views": []
    },
    "2022ed83777b4963b630b5c46239e218": {
     "views": []
    },
    "21c4c57bfc48495194663e6a4fbac488": {
     "views": []
    },
    "22c45c75435348c0b9501d493d69fdca": {
     "views": []
    },
    "2635f668a1af4a9db2642e705d7c73ff": {
     "views": []
    },
    "2ad5ddd9347e451b9290e5b4179ab9a2": {
     "views": []
    },
    "2c062b5778024117984822b63b0593d7": {
     "views": []
    },
    "2f5eab2f6fb24192b76a5ffe99195d44": {
     "views": []
    },
    "31632517325046e8b0cb62e4f4ed2480": {
     "views": []
    },
    "3562b97192ed4d42bbab17f77c290f6b": {
     "views": []
    },
    "38a7cc053723492b921cf9f084ed243c": {
     "views": []
    },
    "3b5750b20e1745879ca0f965aad7b614": {
     "views": []
    },
    "3f1cbabbe2694a9dabe3f1c2e09d0ee2": {
     "views": []
    },
    "3fab6a26a70c4238a668a46d4dc88bf6": {
     "views": []
    },
    "3fb3c7a25e954a4888996976fa107737": {
     "views": []
    },
    "415db64fbc574daea8457ab600392f09": {
     "views": []
    },
    "4463de406b4645a4b562fe7917380ff9": {
     "views": []
    },
    "487e5450b5a24507932709f1fa8f59c1": {
     "views": []
    },
    "48aba73013e74e71927f71d42fb44d14": {
     "views": []
    },
    "4a19ad30f77e4fe6a2c84c8b62378a47": {
     "views": []
    },
    "4bc83ff5270d41679d76d26cdded8313": {
     "views": []
    },
    "4bdd9dd5c5c64646a27fa9096851458b": {
     "views": []
    },
    "5014cd42705f45178d5e6eeffd70f119": {
     "views": []
    },
    "5259b340b68e4fdb97fb4eaf9d98d954": {
     "views": []
    },
    "5986ef5b605a42aca10bc5834529ee06": {
     "views": []
    },
    "5c1aa44589a140eb9709734c843abde6": {
     "views": []
    },
    "600cd9ca4f4c46d4ad6fe57df107675a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "61fe369ebdd14eaa89de110f6186e6b7": {
     "views": []
    },
    "62ac836017ae47a38f8fde806c5ec9b7": {
     "views": []
    },
    "632ccbfabe91405aa1c5a77c9ea754db": {
     "views": []
    },
    "66175f618ea5472baac618f998d2c06c": {
     "views": []
    },
    "6647a620af034d26abcd327ae02364d4": {
     "views": []
    },
    "6752222d2cba43e18f344a8db7f99d24": {
     "views": []
    },
    "6b684ba1a7c24a35ba2df77016212904": {
     "views": []
    },
    "6cea898f4aca4f1e84601f843e337238": {
     "views": []
    },
    "6f8d1e87fd60462a89d693b2f3b5f007": {
     "views": []
    },
    "74078646a5eb4047b40370a0ab8b6b30": {
     "views": []
    },
    "745b0c79ff3040788ea952fce9c7d607": {
     "views": []
    },
    "757c9b805eb7445bac9a7f141f87e45f": {
     "views": []
    },
    "76ced68e19a742e8976dbfd4e8594a1a": {
     "views": []
    },
    "783bb5e7538d4d9d8315e2698024b353": {
     "views": []
    },
    "794993d66efe4ab29a8d35aad8cfe079": {
     "views": []
    },
    "8375e24bae7541528d7cdc0f379d1d4c": {
     "views": []
    },
    "8554945ec15041a7bf8004dbc3fc5f11": {
     "views": []
    },
    "878a34e26cce4f18bb8232a682ebe964": {
     "views": []
    },
    "8921a75116a549198eb7b7f4a24ab672": {
     "views": []
    },
    "909f4504f0b049bda8b641defa177062": {
     "views": []
    },
    "910b9d32a3fb45ec99da1f9df1add816": {
     "views": []
    },
    "9d15ce601cd34f0699b7a7a0ce1d17dc": {
     "views": []
    },
    "a26638c9fee247b3891aac027a0918cc": {
     "views": []
    },
    "a9d2bf44a3ad447bb3eecde71363c198": {
     "views": []
    },
    "ad366bf4c95f4cdba62d47ba9501efc9": {
     "views": []
    },
    "ad8e1842ec314a94b6ed4b62c4c0a450": {
     "views": []
    },
    "af525094db304d2a812ae1312b00889b": {
     "views": []
    },
    "b0697c4343da491f9a35bf02681dad8f": {
     "views": []
    },
    "b07ff307919e4268bc8bec8379c47a5d": {
     "views": []
    },
    "b0e85c726ca141079333afb27edc63d4": {
     "views": []
    },
    "bdcc1e5df7a8432b9f40d8249a46f90a": {
     "views": []
    },
    "be1065f37fa24e818d31c3bb075947a3": {
     "views": []
    },
    "c296c8df2f734e268c6c1204536e7142": {
     "views": []
    },
    "c4bfd3e447f0426da144b76abc202129": {
     "views": []
    },
    "cced93184d4445218a2b14567579333d": {
     "views": []
    },
    "d5bd2e4d5f85482e9345f3a7a69380d0": {
     "views": []
    },
    "d798fa64e8be4a7d9ec1cbeece3b1be9": {
     "views": []
    },
    "d7aec0d6d05f442b991ab40af944811d": {
     "views": []
    },
    "db469cea2c8e4180bf6890de80329c1d": {
     "views": []
    },
    "e671857510c54634b6f0fa55bf1fa228": {
     "views": []
    },
    "ebf52deafaf64b0c826533dafdf993c0": {
     "views": []
    },
    "ed1e5439da9c41199a7bbbda21b556f8": {
     "views": []
    },
    "f585cf5db5024280af5b567f0e4fd771": {
     "views": []
    },
    "f6ba8f8800af47adabed847063bda8db": {
     "views": []
    },
    "f9bae72f14e44705b5c38a3ddc69fee8": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
