{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating individual systematic contributions\n",
    "\n",
    "The effect of any individual systematic uncertainty is somewhat complicated by it's mutual covariance with the POI (i.e., the W branching fractions) and any other systematic uncertainty.  To get a rough idea of the percent-wise contribution to the total uncertainty from each individual systematic, I use the following scheme:\n",
    "\n",
    "   * the fit is carried out as in the nominal case and $\\sigma_{0}$ is estimated\n",
    "   * the fit is carried out for for each of the $n$ nuisance parameters $\\sigma_{theta}$\n",
    "   * the difference between the nominal case and the $n-1$ case is calculated,\n",
    "   * this quantity is normalized to $\\sum_{\\theta} \\sigma_{\\theta}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:09.639485Z",
     "start_time": "2020-03-04T17:25:09.500321Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/naodell/work/wbr/analysis\n",
      "{\n",
      "  \"shell_port\": 38009,\n",
      "  \"iopub_port\": 34267,\n",
      "  \"stdin_port\": 60623,\n",
      "  \"control_port\": 60263,\n",
      "  \"hb_port\": 43913,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"a60ae49e-aee33fe4b640f66032f39fd1\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-1e88735e-53e4-45fc-bccb-0eeb75431084.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "## imports and configuration\n",
    "%cd '/home/naodell/work/wbr/analysis'\n",
    "#%load_ext autoreload\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.optimize import minimize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scripts.plot_tools as pt\n",
    "import scripts.fit_helpers as fh\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "rc_params = {\n",
    "             'figure.figsize': (10, 10),\n",
    "             'axes.labelsize': 20,\n",
    "             'axes.facecolor': 'white',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'legend.fontsize': 20,\n",
    "             'xtick.labelsize':18,\n",
    "             'ytick.labelsize':18,\n",
    "             'font.size':18,\n",
    "             'font.sans-serif':['Arial', 'sans-serif'],\n",
    "             'mathtext.sf':'Arial',\n",
    "             'lines.markersize':8.,\n",
    "             'lines.linewidth':2.5,\n",
    "            }\n",
    "matplotlib.rcParams.update(rc_params)\n",
    "\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.006785Z",
     "start_time": "2020-03-04T17:25:09.640699Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# configure, get the input data, and do any additional processing that is needed\n",
    "processes = ['ttbar', 't', 'ww', 'wjets', 'zjets_alt', 'gjets', 'diboson', 'fakes'] \n",
    "selections = [\n",
    "              'ee',  'mumu',  \n",
    "              'emu', \n",
    "              'mutau', 'etau', \n",
    "              'mujet', 'ejet'\n",
    "             ]\n",
    "plot_labels = fh.fancy_labels\n",
    "\n",
    "# initialize fit data\n",
    "scenario = 'unblinded'\n",
    "infile = open(f'local_data/fit_data_{scenario}.pkl', 'rb')\n",
    "fit_data = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# get fit parameters\n",
    "parameters = fit_data._parameters#.query('active == True')[['label', 'val_init', 'err_init', 'val_fit', 'err_fit', 'type', 'group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.027567Z",
     "start_time": "2020-03-04T17:25:10.008440Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare Asimov dataset\n",
    "asimov_data = {cat:fit_data.mixture_model(parameters.val_init.values, cat) for cat in fit_data._model_data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.041626Z",
     "start_time": "2020-03-04T17:25:10.033422Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# initialize veto list\n",
    "fit_data.veto_list = [\n",
    "    # baseline\n",
    "    #'ee_cat_gt2_eq1_b', 'ee_cat_gt2_gt2_b', \n",
    "    #'mumu_cat_gt2_eq1_b', 'mumu_cat_gt2_gt2_b', \n",
    "    #'emu_cat_gt2_eq1_a', 'emu_cat_gt2_gt2_a', \n",
    "    #'etau_cat_eq2_eq1', 'etau_cat_gt3_eq1', 'etau_cat_eq2_gt2', 'etau_cat_gt3_gt2', \n",
    "    #'mutau_cat_eq2_eq1', 'mutau_cat_gt3_eq1', 'mutau_cat_eq2_gt2', 'mutau_cat_gt3_gt2', \n",
    "    #'ejet_cat_gt4_eq1', 'ejet_cat_gt4_gt2',\n",
    "    #'mujet_cat_gt4_eq1', 'mujet_cat_gt4_gt2', \n",
    "    'ejet_cat_eq3_gt2', 'mujet_cat_eq3_gt2',\n",
    "    \n",
    "    # e/mu DY CR\n",
    "    'ee_cat_gt2_eq0',  'mumu_cat_gt2_eq0', \n",
    "    \n",
    "    # e+mu additional ttbar\n",
    "    #'emu_cat_gt2_eq0', 'emu_cat_eq1_eq0_a', 'emu_cat_eq1_eq1_a', \n",
    "    \n",
    "    # e+mu WW\n",
    "    'emu_cat_eq0_eq0_a', \n",
    "    \n",
    "    # e/mu+tau additional CR\n",
    "    'mutau_cat_eq0_eq0', 'mutau_cat_eq1_eq0', \n",
    "    #'mutau_cat_gt2_eq0', 'mutau_cat_eq1_eq1', \n",
    "    'etau_cat_eq0_eq0', 'etau_cat_eq1_eq0', \n",
    "    #'etau_cat_gt2_eq0', 'etau_cat_eq1_eq1', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.052086Z",
     "start_time": "2020-03-04T17:25:10.043686Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# fit configuration #\n",
    "\n",
    "# minimizer options\n",
    "#steps = 4*[1e-4, ] \n",
    "#steps += list(0.05*fit_data._perr_init[4: fit_data._nnorm + 7])\n",
    "#steps += list(0.05*fit_data._perr_init[fit_data._npoi + fit_data._nnorm:])\n",
    "#steps = np.array(steps)\n",
    "min_options = dict(\n",
    "                   #finite_diff_rel_step=steps,\n",
    "                   #verbose=3,\n",
    "                   #eps=1e-9, \n",
    "                   gtol = 1e-3,\n",
    "                   disp = True\n",
    "                  )\n",
    "\n",
    "# configure the objective\n",
    "sample = None\n",
    "fobj = partial(fit_data.objective,\n",
    "               data = sample,\n",
    "               do_bb_lite = True,\n",
    "               lu_test = 2\n",
    "              )\n",
    "\n",
    "fobj_jac = partial(fit_data.objective_jacobian,\n",
    "                   data = sample,\n",
    "                   do_bb_lite = True,\n",
    "                   lu_test = 2\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.062418Z",
     "start_time": "2020-03-04T17:25:10.053753Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5d051ff33843008cde4244fdca8b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=103.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, br_tau_e, 0.177, 0.000, 0.177, 0.000, 768.147\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.686836\n",
      "         Iterations: 40\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 130\n",
      "[-0.998 -0.949  0.428  0.246], 180.6868361998446\n",
      "0.17699259914277624 766.2308933915298\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.664920\n",
      "         Iterations: 37\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "[-0.972 -0.95   0.435  0.241], 180.66492009518984\n",
      "5, br_tau_mu, 0.173, 0.000, 0.173, 0.000, 766.231\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.567927\n",
      "         Iterations: 37\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.975 -0.947  0.386  0.249], 180.56792663477992\n",
      "0.17267382123977806 764.5590443366984\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.796179\n",
      "         Iterations: 37\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 102\n",
      "[-0.985 -0.94   0.517  0.228], 180.79617931611372\n",
      "6, br_tau_h, 0.650, 0.001, 0.649, 0.001, 764.559\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.749348\n",
      "         Iterations: 37\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[-0.958 -0.926  0.35   0.248], 180.74934829157738\n",
      "0.6484869577487614 759.506167218933\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.607518\n",
      "         Iterations: 39\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 77\n",
      "[-1.002 -0.966  0.517  0.235], 180.60751814226046\n",
      "7, lumi, 1.000, 0.025, 1.009, 0.019, 759.506\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.179654\n",
      "         Iterations: 50\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 85\n",
      "[-0.987 -0.955  0.463  0.239], 180.1796541937128\n",
      "0.990604973012418 527.8918109037086\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.754839\n",
      "         Iterations: 50\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 88\n",
      "[-0.836 -0.77   0.214  0.226], 181.75483920846833\n",
      "8, xs_gjets, 1.000, 0.100, 1.085, 0.095, 527.892\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.344165\n",
      "         Iterations: 36\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 118\n",
      "[-0.986 -0.97   0.401  0.252], 180.34416493627376\n",
      "0.9891646095919133 526.4731309753164\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.186390\n",
      "         Iterations: 44\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 138\n",
      "[-0.96  -0.883  0.562  0.207], 181.18638971199613\n",
      "9, xs_diboson, 1.000, 0.100, 1.005, 0.100, 526.473\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.666609\n",
      "         Iterations: 40\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 91\n",
      "[-0.99  -0.96   0.464  0.241], 180.66660863526997\n",
      "0.9057022860681366 524.9417522294184\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.684503\n",
      "         Iterations: 37\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[-0.973 -0.937  0.416  0.242], 180.6845033493224\n",
      "10, xs_ww, 1.000, 0.100, 0.969, 0.033, 524.942\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.249039\n",
      "         Iterations: 37\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.979 -0.943  0.475  0.234], 180.24903945272928\n",
      "0.9358847623393444 524.0576558807722\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.218064\n",
      "         Iterations: 40\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 102\n",
      "[-0.984 -0.947  0.439  0.242], 180.218064209908\n",
      "11, xs_t, 1.000, 0.100, 0.922, 0.074, 524.058\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 182.186155\n",
      "         Iterations: 43\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 125\n",
      "[-0.325 -0.274  1.533 -0.152], 182.18615489843117\n",
      "0.8478182802591592 408.22556897390893\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.175930\n",
      "         Iterations: 45\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 123\n",
      "[-0.968 -0.934  0.474  0.231], 180.17592953855822\n",
      "13, e_fakes, 1.000, 1.000, 1.074, 0.097, 408.226\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.284985\n",
      "         Iterations: 41\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 75\n",
      "[-0.971 -0.902  0.391  0.24 ], 180.28498524125257\n",
      "0.9768664132288011 404.1185422835552\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.323237\n",
      "         Iterations: 44\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 97\n",
      "[-1.015 -1.096  0.672  0.233], 181.32323706455702\n",
      "14, mu_fakes, 1.000, 1.000, 0.867, 0.038, 404.119\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.299730\n",
      "         Iterations: 42\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 110\n",
      "[-0.848 -0.731  0.018  0.253], 181.29972993854645\n",
      "0.829234597752125 420.3822613980107\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.296632\n",
      "         Iterations: 42\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 104\n",
      "[-1.028 -1.021  0.593  0.236], 180.29663214833417\n",
      "15, e_fakes_ss, 1.000, 0.300, 1.051, 0.046, 420.382\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 182.258304\n",
      "         Iterations: 40\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 110\n",
      "[-1.036 -0.992  0.483  0.25 ], 182.2583038799625\n",
      "1.0052084838756976 420.5088595512433\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.327524\n",
      "         Iterations: 37\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 114\n",
      "[-1.001 -0.961  0.446  0.245], 180.3275242391541\n",
      "16, mu_fakes_ss, 1.000, 0.300, 1.126, 0.057, 420.509\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.533785\n",
      "         Iterations: 37\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 122\n",
      "[-1.234 -1.087  0.098  0.36 ], 181.53378522254414\n",
      "1.0686840116425296 419.4587667384403\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.196310\n",
      "         Iterations: 42\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 118\n",
      "[-1.014 -0.967  0.405  0.255], 180.19630965807426\n",
      "17, trigger_mu, 1.000, 0.005, 1.001, 0.004, 419.459\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.839371\n",
      "         Iterations: 40\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 81\n",
      "[-0.832 -1.183  0.381  0.265], 180.83937111776012\n",
      "0.9970506615868402 429.49709880334757\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.526230\n",
      "         Iterations: 42\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[-1.085 -0.776  0.505  0.219], 180.52623047725533\n",
      "18, xs_zjets_alt_pdf, 0.000, 1.000, 0.001, 0.974, 429.497\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.505716\n",
      "         Iterations: 43\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 109\n",
      "[-1.009 -0.982  0.403  0.257], 180.50571617428054\n",
      "-0.9730968214033232 428.58591410542897\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.852218\n",
      "         Iterations: 41\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.943 -0.898  0.531  0.212], 180.85221793824738\n",
      "19, xs_zjets_alt_alpha_s, 0.000, 1.000, -0.221, 0.917, 428.586\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.130063\n",
      "         Iterations: 40\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 117\n",
      "[-0.885 -0.828  0.674  0.168], 181.13006329650764\n",
      "-1.1385981125914542 433.4503710098285\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.346238\n",
      "         Iterations: 48\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 70\n",
      "[-1.021 -0.996  0.363  0.268], 180.34623767311172\n",
      "20, xs_zjets_alt_qcd_scale_0, 0.000, 1.000, -0.737, 0.542, 433.450\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.322716\n",
      "         Iterations: 38\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 84\n",
      "[-0.98  -0.944  0.448  0.239], 180.32271629227756\n",
      "-1.2797925313992589 433.3255464927824\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.322716\n",
      "         Iterations: 38\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 84\n",
      "[-0.98  -0.944  0.448  0.239], 180.32271629227756\n",
      "21, xs_zjets_alt_qcd_scale_1, 0.000, 1.000, -0.762, 0.900, 433.326\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.978929\n",
      "         Iterations: 38\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 124\n",
      "[-0.971 -0.941  0.447  0.237], 180.97892871551926\n",
      "-1.6614001197725268 433.4433335830565\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.490225\n",
      "         Iterations: 39\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 78\n",
      "[-0.987 -0.947  0.463  0.238], 180.49022535642393\n",
      "22, xs_zjets_alt_qcd_scale_2, 0.000, 1.000, -0.402, 0.332, 433.443\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.248065\n",
      "         Iterations: 38\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-1.285 -1.304  0.09   0.405], 181.24806451747355\n",
      "-0.734900398661007 427.3486225906415\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.270822\n",
      "         Iterations: 39\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.899 -0.85   0.56   0.192], 180.27082183572335\n",
      "23, xs_wjets_qcd_scale_0, 0.000, 1.000, 0.262, 0.949, 427.349\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.626187\n",
      "         Iterations: 38\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 83\n",
      "[-0.98  -0.944  0.448  0.239], 180.62618693068205\n",
      "-0.6876888688954136 427.7649610458442\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.626187\n",
      "         Iterations: 38\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 83\n",
      "[-0.98  -0.944  0.448  0.239], 180.62618693068205\n",
      "24, xs_wjets_qcd_scale_1, 0.000, 1.000, -0.142, 0.953, 427.765\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.529780\n",
      "         Iterations: 38\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 103\n",
      "[-0.988 -0.962  0.456  0.242], 180.52977981065575\n",
      "-1.0949331359848304 428.65060716629245\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.817284\n",
      "         Iterations: 41\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 116\n",
      "[-0.972 -0.931  0.448  0.235], 180.81728356455366\n",
      "25, xs_wjets_qcd_scale_2, 0.000, 1.000, 1.215, 0.651, 428.651\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.248085\n",
      "         Iterations: 37\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 121\n",
      "[-0.952 -0.923  0.248  0.264], 180.24808517929316\n",
      "0.5639226677264656 424.63561099188195\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 182.790612\n",
      "         Iterations: 41\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 109\n",
      "[-0.816 -0.804 -0.729  0.381], 182.79061163799838\n",
      "26, xs_wjets_qcd_scale_3, 0.000, 1.000, -0.286, 0.947, 424.636\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.957590\n",
      "         Iterations: 38\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[-1.005 -0.968  0.46   0.245], 180.95758959516832\n",
      "-1.233448171938987 423.7659678839469\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.449101\n",
      "         Iterations: 38\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 93\n",
      "[-0.968 -0.934  0.438  0.237], 180.44910081225112\n",
      "27, xs_wjets_qcd_scale_4, 0.000, 1.000, 1.841, 0.390, 423.766\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.274236\n",
      "         Iterations: 40\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.983 -0.96   0.279  0.269], 180.27423592946036\n",
      "1.450888919876758 505.4310888251841\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.363821\n",
      "         Iterations: 42\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 133\n",
      "[-0.952 -0.875  1.035  0.128], 181.36382057475197\n",
      "28, xs_ttbar_pdf, 0.000, 1.000, -0.363, 1.001, 505.431\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.042688\n",
      "         Iterations: 48\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 126\n",
      "[-0.958 -0.92   0.48   0.226], 181.04268782088715\n",
      "-1.3638581831242613 470.9356647691034\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.416131\n",
      "         Iterations: 52\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 107\n",
      "[-0.993 -0.961  0.434  0.246], 180.4161310286447\n",
      "29, xs_ttbar_alpha_s, 0.000, 1.000, 0.357, 1.009, 470.936\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.407002\n",
      "         Iterations: 50\n",
      "         Function evaluations: 141\n",
      "         Gradient evaluations: 129\n",
      "[-1.    -0.962  0.427  0.249], 180.40700243051725\n",
      "-0.6522937518710565 471.3526036840907\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.066782\n",
      "         Iterations: 51\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.944 -0.916  0.496  0.221], 181.0667817062001\n",
      "30, xs_ttbar_qcd_scale, 0.000, 1.000, -0.510, 0.257, 471.353\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.406262\n",
      "         Iterations: 56\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 142\n",
      "[-1.182 -1.175  0.203  0.349], 180.4062617214983\n",
      "-0.7669792049873322 5461.002945680908\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.044729\n",
      "         Iterations: 49\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 91\n",
      "[-0.575 -0.483  0.933  0.02 ], 181.04472895124164\n",
      "31, eff_tau_0, 0.000, 1.000, -0.634, 0.628, 5461.003\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.306436\n",
      "         Iterations: 41\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 118\n",
      "[-0.699 -0.744 -0.582  0.328], 181.306435765944\n",
      "-1.2619719680528827 5514.729727715333\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.285820\n",
      "         Iterations: 41\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[-1.072 -1.013  0.74   0.218], 180.28581959487033\n",
      "32, eff_tau_1, 0.000, 1.000, -0.351, 0.614, 5514.730\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.132744\n",
      "         Iterations: 40\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.755 -0.752 -0.466  0.32 ], 181.13274396030073\n",
      "-0.9656916816913736 5552.954664093666\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.363462\n",
      "         Iterations: 41\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 88\n",
      "[-1.087 -1.038  0.818  0.211], 180.36346178321787\n",
      "33, eff_tau_2, 0.000, 1.000, -0.737, 0.492, 5552.955\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 182.101137\n",
      "         Iterations: 40\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 116\n",
      "[-0.601 -0.639 -1.617  0.464], 182.10113742239824\n",
      "-1.2286027735684226 5627.043946584249\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.176276\n",
      "         Iterations: 39\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 129\n",
      "[-0.987 -0.951  0.487  0.235], 180.17627626165697\n",
      "34, eff_tau_3, 0.000, 1.000, -0.431, 0.475, 5627.044\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.242709\n",
      "         Iterations: 38\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 98\n",
      "[-0.668 -0.714 -1.202  0.419], 181.2427094610588\n",
      "-0.9062130541345447 5660.321728245753\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.320256\n",
      "         Iterations: 41\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-1.109 -1.038  1.022  0.182], 180.32025573523913\n",
      "35, eff_tau_4, 0.000, 1.000, -0.510, 0.465, 5660.322\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.577495\n",
      "         Iterations: 40\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.62  -0.673 -1.478  0.45 ], 181.57749502131057\n",
      "-0.9756177102608519 5689.33748368849\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.227343\n",
      "         Iterations: 41\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 124\n",
      "[-1.059 -1.01   0.799  0.205], 180.22734284305093\n",
      "36, eff_tau_5, 0.000, 1.000, -1.109, 0.436, 5689.337\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.646901\n",
      "         Iterations: 40\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 83\n",
      "[-0.576 -0.652 -1.704  0.476], 181.6469007227518\n",
      "-1.5452065726669928 5732.5925797780355\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.217497\n",
      "         Iterations: 40\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[-1.054 -1.002  0.803  0.203], 180.21749719939348\n",
      "37, misid_tau_e, 0.000, 1.000, 0.096, 0.996, 5732.593\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.566411\n",
      "         Iterations: 43\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 102\n",
      "[-0.97  -0.933  0.442  0.237], 180.56641091002794\n",
      "-0.9000174270919339 5739.828757395181\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.799028\n",
      "         Iterations: 38\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 88\n",
      "[-0.993 -0.965  0.447  0.245], 180.79902792722558\n",
      "38, misid_tau_h, 0.000, 1.000, -0.141, 0.613, 5739.829\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.618614\n",
      "         Iterations: 41\n",
      "         Function evaluations: 175\n",
      "         Gradient evaluations: 163\n",
      "[-0.909 -0.899  0.076  0.281], 180.6186140970694\n",
      "-0.7538974904360122 5861.793233301502\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.736268\n",
      "         Iterations: 42\n",
      "         Function evaluations: 152\n",
      "         Gradient evaluations: 141\n",
      "[-1.069 -1.002  0.882  0.192], 180.73626836973796\n",
      "39, misid_tau_0, 0.000, 1.000, 0.123, 0.654, 5861.793\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.410357\n",
      "         Iterations: 37\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[-0.97  -0.932  0.456  0.234], 180.41035712451574\n",
      "-0.5311961989105656 5896.198264074467\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.031814\n",
      "         Iterations: 39\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 75\n",
      "[-0.975 -0.961  0.443  0.242], 181.03181442957603\n",
      "40, misid_tau_1, 0.000, 1.000, 0.057, 0.799, 5896.198\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.566245\n",
      "         Iterations: 37\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 81\n",
      "[-0.965 -0.934  0.401  0.242], 180.566245268446\n",
      "-0.7417746863028982 5923.72397629619\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.794979\n",
      "         Iterations: 39\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[-0.996 -0.965  0.515  0.234], 180.7949791172418\n",
      "41, misid_tau_2, 0.000, 1.000, 0.584, 0.824, 5923.724\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.271607\n",
      "         Iterations: 43\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 102\n",
      "[-0.979 -0.95   0.452  0.239], 180.27160728113714\n",
      "-0.2393568168841146 5933.085043655902\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.398857\n",
      "         Iterations: 45\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 101\n",
      "[-0.995 -0.948  0.426  0.246], 181.3988569087361\n",
      "42, misid_tau_3, 0.000, 1.000, -0.379, 0.929, 5933.085\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.089778\n",
      "         Iterations: 43\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 116\n",
      "[-0.973 -0.944  0.391  0.247], 181.08977824168082\n",
      "-1.307738333006469 5952.519447816373\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.385876\n",
      "         Iterations: 39\n",
      "         Function evaluations: 162\n",
      "         Gradient evaluations: 151\n",
      "[-0.984 -0.948  0.484  0.234], 180.38587575756728\n",
      "43, misid_tau_4, 0.000, 1.000, -0.185, 0.951, 5952.519\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.825165\n",
      "         Iterations: 43\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 89\n",
      "[-0.973 -0.946  0.354  0.253], 180.82516489969518\n",
      "-1.135287549478955 5965.009512290626\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.546092\n",
      "         Iterations: 41\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[-0.988 -0.949  0.515  0.23 ], 180.5460915576294\n",
      "44, misid_tau_5, 0.000, 1.000, -0.267, 0.957, 5965.010\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.902459\n",
      "         Iterations: 41\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 114\n",
      "[-0.967 -0.942  0.368  0.249], 180.90245934002192\n",
      "-1.2238875570657357 5976.858422467553\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.490148\n",
      "         Iterations: 41\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 80\n",
      "[-0.991 -0.949  0.496  0.234], 180.49014842781608\n",
      "45, escale_tau, 0.000, 1.000, -0.222, 0.542, 5976.858\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.576219\n",
      "         Iterations: 39\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[-0.94  -0.905  0.225  0.262], 180.57621942520362\n",
      "-0.7638598724821098 6021.39780117958\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.786103\n",
      "         Iterations: 38\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 135\n",
      "[-1.043 -1.001  0.636  0.228], 180.7861031016647\n",
      "46, eff_reco_e, 0.000, 1.000, -0.002, 0.967, 6021.398\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.681205\n",
      "         Iterations: 42\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 109\n",
      "[-1.529 -0.883  0.526  0.305], 180.6812054672757\n",
      "-0.9687403170348826 6335.0770344966295\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.659723\n",
      "         Iterations: 45\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 92\n",
      "[-0.433 -1.008  0.372  0.173], 180.6597232394108\n",
      "48, eff_e_0, 0.000, 1.000, -1.527, 0.735, 6335.077\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 182.521045\n",
      "         Iterations: 43\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 94\n",
      "[-1.088 -0.89   0.504  0.239], 182.52104540341944\n",
      "-2.262135399737482 6494.1436594768575\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.192608\n",
      "         Iterations: 40\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 109\n",
      "[-0.985 -0.94   0.463  0.237], 180.19260820958667\n",
      "49, eff_e_1, 0.000, 1.000, 0.512, 0.905, 6494.144\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.419727\n",
      "         Iterations: 40\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 111\n",
      "[-1.05  -0.953  0.464  0.249], 180.4197265994248\n",
      "-0.39255563265281246 6507.850080561624\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.013423\n",
      "         Iterations: 40\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 114\n",
      "[-0.855 -0.933  0.428  0.22 ], 181.0134231567284\n",
      "50, eff_e_2, 0.000, 1.000, 1.034, 0.835, 6507.850\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.196158\n",
      "         Iterations: 38\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[-1.01  -0.949  0.456  0.243], 180.19615826235238\n",
      "0.1985104116495907 6500.347177621475\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.774642\n",
      "         Iterations: 43\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 99\n",
      "[-0.703 -0.924  0.346  0.208], 181.7746421619393\n",
      "51, eff_e_3, 0.000, 1.000, -1.138, 0.927, 6500.347\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 182.011711\n",
      "         Iterations: 40\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 143\n",
      "[-1.208 -0.953  0.696  0.237], 182.01171120317585\n",
      "-2.0651060678026427 6555.859699279573\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.178599\n",
      "         Iterations: 38\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 91\n",
      "[-0.979 -0.952  0.437  0.242], 180.17859860660332\n",
      "52, eff_e_4, 0.000, 1.000, -0.654, 0.474, 6555.860\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.946765\n",
      "         Iterations: 46\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 100\n",
      "[-1.215 -0.941  0.772  0.224], 180.94676494918266\n",
      "-1.1284559664410518 6681.374563353555\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.459208\n",
      "         Iterations: 41\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 110\n",
      "[-0.848 -0.953  0.265  0.249], 180.45920828077183\n",
      "53, eff_e_5, 0.000, 1.000, -0.650, 0.440, 6681.375\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.696956\n",
      "         Iterations: 39\n",
      "         Function evaluations: 160\n",
      "         Gradient evaluations: 148\n",
      "[-1.142 -0.889  0.75   0.207], 180.6969555415759\n",
      "-1.0897960164129836 7028.877530510466\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.645456\n",
      "         Iterations: 43\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 100\n",
      "[-0.832 -1.004  0.166  0.271], 180.64545563032522\n",
      "54, trigger_e_0, 0.000, 1.000, -0.992, 0.946, 7028.878\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.340115\n",
      "         Iterations: 38\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 142\n",
      "[-1.013 -0.878  0.574  0.213], 181.34011516492762\n",
      "-1.9378137797289234 7108.02137455089\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.287541\n",
      "         Iterations: 39\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.972 -0.971  0.406  0.249], 180.28754086634117\n",
      "55, trigger_e_1, 0.000, 1.000, -0.516, 0.986, 7108.021\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.972051\n",
      "         Iterations: 38\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.993 -0.919  0.489  0.23 ], 180.97205083804255\n",
      "-1.5013849388833331 7141.715792857488\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.448304\n",
      "         Iterations: 41\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 93\n",
      "[-0.974 -0.964  0.419  0.246], 180.448304336461\n",
      "56, trigger_e_2, 0.000, 1.000, -0.371, 0.992, 7141.716\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.879654\n",
      "         Iterations: 38\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 98\n",
      "[-0.989 -0.928  0.476  0.233], 180.8796539056592\n",
      "-1.3633869229360827 7164.434804863618\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.506231\n",
      "         Iterations: 39\n",
      "         Function evaluations: 97\n",
      "         Gradient evaluations: 85\n",
      "[-0.98  -0.964  0.409  0.248], 180.5062311714177\n",
      "57, trigger_e_3, 0.000, 1.000, -0.407, 0.991, 7164.435\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.900700\n",
      "         Iterations: 40\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 101\n",
      "[-0.991 -0.924  0.493  0.23 ], 180.9007004508104\n",
      "-1.3979545211727875 7190.409695503198\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.492604\n",
      "         Iterations: 39\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 85\n",
      "[-0.973 -0.959  0.418  0.245], 180.4926043373917\n",
      "58, trigger_e_4, 0.000, 1.000, -0.307, 0.911, 7190.410\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.947809\n",
      "         Iterations: 39\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 87\n",
      "[-0.96  -0.924  0.394  0.241], 180.94780937626172\n",
      "-1.217645718830571 7232.396593106603\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.461227\n",
      "         Iterations: 38\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.993 -0.962  0.462  0.242], 180.46122739191915\n",
      "59, trigger_e_5, 0.000, 1.000, -0.270, 0.529, 7232.397\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.694035\n",
      "         Iterations: 43\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 115\n",
      "[-1.078 -0.931  0.317  0.274], 181.69403539148752\n",
      "-0.7987924930305779 7442.077302700775\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.207546\n",
      "         Iterations: 38\n",
      "         Function evaluations: 161\n",
      "         Gradient evaluations: 149\n",
      "[-0.966 -0.949  0.483  0.232], 180.20754602979045\n",
      "60, escale_e, 0.000, 1.000, 1.903, 0.839, 7442.077\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.208328\n",
      "         Iterations: 38\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[-0.97  -0.945  0.43   0.241], 180.20832823018452\n",
      "1.0641493740495775 7421.931930804979\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 182.604987\n",
      "         Iterations: 43\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.877 -0.896  0.453  0.214], 182.6049874119086\n",
      "61, trigger_e_tag, 0.000, 1.000, 0.923, 0.834, 7421.932\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.202388\n",
      "         Iterations: 39\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[-1.059 -0.957  0.452  0.253], 180.20238828683708\n",
      "0.08868759481637067 7410.522701999978\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.703069\n",
      "         Iterations: 43\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 125\n",
      "[-0.459 -0.869  0.388  0.152], 181.70306918537574\n",
      "62, trigger_e_probe, 0.000, 1.000, 0.843, 0.791, 7410.523\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.223752\n",
      "         Iterations: 38\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 133\n",
      "[-0.934 -0.926  0.515  0.218], 180.22375244542837\n",
      "0.05117062393648797 7406.564432438954\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 182.652084\n",
      "         Iterations: 43\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 126\n",
      "[-0.728 -0.86   0.881  0.114], 182.65208356285473\n",
      "63, e_prefire, 0.000, 1.000, -0.491, 0.925, 7406.564\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.945809\n",
      "         Iterations: 43\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 81\n",
      "[-1.343 -0.957  0.653  0.267], 180.94580949618629\n",
      "-1.4153437213878262 7817.531778107231\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.456039\n",
      "         Iterations: 43\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 96\n",
      "[-0.767 -0.944  0.324  0.225], 180.45603943517273\n",
      "64, eff_iso_mu, 0.000, 1.000, 0.226, 0.985, 7817.532\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.534290\n",
      "         Iterations: 41\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 114\n",
      "[-0.972 -0.983  0.377  0.255], 180.53429041540926\n",
      "-0.7587719808757587 7851.154725780556\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.840566\n",
      "         Iterations: 38\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 77\n",
      "[-1.    -0.897  0.554  0.217], 180.84056563920177\n",
      "65, eff_id_mu, 0.000, 1.000, 0.634, 0.741, 7851.155\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.300122\n",
      "         Iterations: 41\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 93\n",
      "[-0.938 -0.973  0.355  0.252], 180.3001215929058\n",
      "-0.10757670607897829 7872.259348545162\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.304397\n",
      "         Iterations: 43\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 103\n",
      "[-1.118 -0.867  0.715  0.205], 181.30439733363733\n",
      "66, eff_mu_0, 0.000, 1.000, 0.459, 0.983, 7872.259\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.416856\n",
      "         Iterations: 38\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 92\n",
      "[-0.968 -0.923  0.349  0.25 ], 180.41685590420383\n",
      "-0.5233190551093373 7874.0789293023045\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.026702\n",
      "         Iterations: 38\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 83\n",
      "[-1.004 -0.992  0.62   0.223], 181.0267024577978\n",
      "67, eff_mu_1, 0.000, 1.000, 0.234, 0.975, 7874.079\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.614987\n",
      "         Iterations: 38\n",
      "         Function evaluations: 161\n",
      "         Gradient evaluations: 149\n",
      "[-0.984 -0.935  0.349  0.254], 180.61498675270676\n",
      "-0.741942173899353 7877.286348762848\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.737198\n",
      "         Iterations: 38\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 109\n",
      "[-0.976 -0.954  0.572  0.22 ], 180.73719781570946\n",
      "68, eff_mu_2, 0.000, 1.000, 0.008, 0.980, 7877.286\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.711535\n",
      "         Iterations: 38\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 78\n",
      "[-0.979 -0.954  0.348  0.257], 180.71153485683013\n",
      "-0.9716619823105412 7882.433568636917\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.639985\n",
      "         Iterations: 37\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 84\n",
      "[-0.981 -0.934  0.55   0.221], 180.63998519092385\n",
      "69, eff_mu_3, 0.000, 1.000, 0.493, 0.861, 7882.434\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.221107\n",
      "         Iterations: 41\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 116\n",
      "[-0.992 -0.969  0.393  0.254], 180.22110719034146\n",
      "-0.36818609895755733 7891.312025955606\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.593129\n",
      "         Iterations: 38\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 114\n",
      "[-0.941 -0.828  0.804  0.156], 181.59312900218387\n",
      "70, eff_mu_4, 0.000, 1.000, -0.063, 0.955, 7891.312\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.830387\n",
      "         Iterations: 43\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 84\n",
      "[-1.023 -1.024  0.461  0.257], 180.83038721230304\n",
      "-1.0182881638285077 7902.426816096293\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.539243\n",
      "         Iterations: 38\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 105\n",
      "[-0.951 -0.89   0.443  0.226], 180.53924297809223\n",
      "71, eff_mu_5, 0.000, 1.000, -0.033, 0.998, 7902.427\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.702457\n",
      "         Iterations: 38\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[-0.989 -0.955  0.476  0.238], 180.70245714684162\n",
      "-1.030966093242661 7904.36593587708\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.650211\n",
      "         Iterations: 38\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 116\n",
      "[-0.972 -0.934  0.435  0.238], 180.65021097180954\n",
      "72, eff_mu_6, 0.000, 1.000, 0.230, 0.990, 7904.366\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.422918\n",
      "         Iterations: 45\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 108\n",
      "[-0.987 -0.964  0.488  0.237], 180.42291766414618\n",
      "-0.7597616696298046 7907.299200737157\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.016801\n",
      "         Iterations: 43\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 96\n",
      "[-0.973 -0.919  0.381  0.245], 181.01680059732846\n",
      "73, eff_mu_7, 0.000, 1.000, -0.826, 0.979, 7907.299\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.490831\n",
      "         Iterations: 38\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 132\n",
      "[-0.987 -1.007  0.534  0.236], 181.49083143092128\n",
      "-1.8052765931266634 7913.000256560619\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.246746\n",
      "         Iterations: 38\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.981 -0.931  0.429  0.24 ], 180.24674563757893\n",
      "74, escale_mu, 0.000, 1.000, -0.586, 0.678, 7913.000\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.881165\n",
      "         Iterations: 38\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 72\n",
      "[-0.985 -0.981  0.765  0.194], 180.88116474883606\n",
      "-1.2639681287151876 7938.393301884481\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.474754\n",
      "         Iterations: 43\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 129\n",
      "[-0.991 -0.931  0.237  0.273], 180.47475420713656\n",
      "75, pileup, 0.000, 1.000, -0.308, 0.616, 7938.393\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.887047\n",
      "         Iterations: 47\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[-0.803 -1.063  0.874  0.161], 180.88704691915376\n",
      "-0.9237572490147854 7197.6285614117305\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.467571\n",
      "         Iterations: 49\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 132\n",
      "[-1.114 -0.878  0.151  0.298], 180.46757132039755\n",
      "76, isr, 0.000, 1.000, 0.441, 0.246, 7197.629\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.894934\n",
      "         Iterations: 40\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.862 -0.812  1.359  0.05 ], 180.8949342444001\n",
      "0.19519917927862296 7066.632669447359\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.498411\n",
      "         Iterations: 44\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 83\n",
      "[-1.08  -1.055 -0.246  0.386], 180.4984111159916\n",
      "77, fsr, 0.000, 1.000, 0.051, 0.084, 7066.633\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.970077\n",
      "         Iterations: 39\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[-1.007 -1.003  0.746  0.204], 180.97007738725196\n",
      "-0.03259070550584236 6872.993822309199\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.427835\n",
      "         Iterations: 42\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[-0.976 -0.919  0.275  0.262], 180.42783450611566\n",
      "78, hdamp, 0.000, 1.000, 0.030, 0.115, 6872.994\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.430841\n",
      "         Iterations: 42\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 84\n",
      "[-1.041 -0.994  0.443  0.258], 180.43084148982163\n",
      "-0.084852440117805 7011.890815027719\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.993923\n",
      "         Iterations: 44\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 84\n",
      "[-0.885 -0.876  0.411  0.219], 180.99392272668229\n",
      "79, tune, 0.000, 1.000, 0.135, 0.205, 7011.891\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.048864\n",
      "         Iterations: 42\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.997 -0.867  1.237  0.101], 181.04886391251162\n",
      "-0.06912501897704651 7006.525387261806\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.401653\n",
      "         Iterations: 40\n",
      "         Function evaluations: 143\n",
      "         Gradient evaluations: 131\n",
      "[-0.988 -0.999 -0.028  0.327], 180.40165314674368\n",
      "80, ww_scale, 0.000, 1.000, 0.167, 0.979, 7006.525\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.638288\n",
      "         Iterations: 38\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 119\n",
      "[-0.983 -0.947  0.446  0.24 ], 180.6382883535834\n",
      "-0.8118711650026433 7005.834533186759\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.680340\n",
      "         Iterations: 38\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[-0.977 -0.942  0.466  0.235], 180.68033984484353\n",
      "81, ww_resum, 0.000, 1.000, -0.431, 1.081, 7005.835\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.767421\n",
      "         Iterations: 38\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 113\n",
      "[-0.979 -0.943  0.453  0.238], 180.76742070778073\n",
      "-1.5116242371632274 7005.6975101031085\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.755852\n",
      "         Iterations: 43\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 107\n",
      "[-0.984 -0.949  0.451  0.24 ], 180.75585176102172\n",
      "82, top_pt, 1.000, 1.000, 1.440, 0.068, 7005.698\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.915518\n",
      "         Iterations: 40\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 126\n",
      "[-0.702 -0.706  0.46   0.153], 180.9155179910795\n",
      "1.371523937952149 7007.29441614019\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.450591\n",
      "         Iterations: 42\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 96\n",
      "[-1.156 -1.097  0.448  0.292], 180.45059134192292\n",
      "83, btag_bfragmentation, 0.000, 1.000, 0.130, 0.989, 7007.294\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.710762\n",
      "         Iterations: 42\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 102\n",
      "[-0.974 -0.936  0.476  0.232], 180.71076213540005\n",
      "-0.8580337325898624 7238.309266237394\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.641902\n",
      "         Iterations: 45\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 122\n",
      "[-0.985 -0.953  0.427  0.245], 180.64190184886806\n",
      "84, btag_btempcorr, 0.000, 1.000, 0.074, 0.989, 7238.309\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.779822\n",
      "         Iterations: 41\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 133\n",
      "[-0.982 -0.936  0.449  0.238], 180.77982205318258\n",
      "-0.915018333110246 7468.14660059122\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.581212\n",
      "         Iterations: 43\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 95\n",
      "[-0.979 -0.952  0.452  0.24 ], 180.58121162960234\n",
      "85, btag_cb, 0.000, 1.000, -0.065, 0.994, 7468.147\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.819830\n",
      "         Iterations: 43\n",
      "         Function evaluations: 153\n",
      "         Gradient evaluations: 141\n",
      "[-0.979 -0.939  0.463  0.236], 180.81983038684626\n",
      "-1.0596505565171053 7651.6639173706535\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.549831\n",
      "         Iterations: 43\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[-0.981 -0.951  0.44   0.242], 180.54983117055016\n",
      "86, btag_cfragmentation, 0.000, 1.000, 0.000, 1.000, 7651.664\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.676620\n",
      "         Iterations: 43\n",
      "         Function evaluations: 155\n",
      "         Gradient evaluations: 144\n",
      "[-0.982 -0.947  0.459  0.238], 180.67662009730765\n",
      "-0.999646375058296 7654.0319147914315\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.675058\n",
      "         Iterations: 38\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[-0.977 -0.944  0.455  0.237], 180.67505797632532\n",
      "87, btag_dmux, 0.000, 1.000, 0.030, 0.998, 7654.032\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.700855\n",
      "         Iterations: 41\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 117\n",
      "[-0.982 -0.944  0.452  0.239], 180.7008546510516\n",
      "-0.968640371832088 7745.657440578034\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.650889\n",
      "         Iterations: 38\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 114\n",
      "[-0.981 -0.951  0.431  0.243], 180.65088924158024\n",
      "88, btag_gluonsplitting, 0.000, 1.000, 0.259, 0.958, 7745.657\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.745626\n",
      "         Iterations: 45\n",
      "         Function evaluations: 171\n",
      "         Gradient evaluations: 159\n",
      "[-0.984 -0.959  0.425  0.246], 180.74562634229267\n",
      "-0.6991227472303527 8152.57965494635\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.610430\n",
      "         Iterations: 43\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 115\n",
      "[-0.979 -0.938  0.462  0.236], 180.61042998219784\n",
      "89, btag_jes, 0.000, 1.000, 0.050, 0.991, 8152.580\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.783536\n",
      "         Iterations: 41\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[-0.976 -0.931  0.457  0.235], 180.78353645660022\n",
      "-0.9405094174985419 8407.83896446974\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.578446\n",
      "         Iterations: 43\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 93\n",
      "[-0.985 -0.956  0.442  0.243], 180.57844603011893\n",
      "90, btag_jetaway, 0.000, 1.000, 0.180, 0.963, 8407.839\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.794872\n",
      "         Iterations: 43\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 115\n",
      "[-0.988 -0.951  0.434  0.244], 180.79487200314753\n",
      "-0.7824711026174721 8876.019391132191\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.569535\n",
      "         Iterations: 46\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 124\n",
      "[-0.975 -0.943  0.471  0.234], 180.569535058715\n",
      "91, btag_ksl, 0.000, 1.000, 0.000, 1.000, 8876.019\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.677792\n",
      "         Iterations: 38\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 70\n",
      "[-0.98  -0.947  0.441  0.241], 180.67779152799014\n",
      "-0.9995500429684997 8881.253250839321\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.674467\n",
      "         Iterations: 43\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 113\n",
      "[-0.982 -0.947  0.442  0.241], 180.67446732960934\n",
      "92, btag_l2c, 0.000, 1.000, -0.066, 0.996, 8881.253\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.804111\n",
      "         Iterations: 43\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 130\n",
      "[-0.98  -0.94   0.448  0.238], 180.80411053288537\n",
      "-1.0622122943814967 9030.318558005647\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.562408\n",
      "         Iterations: 43\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.982 -0.953  0.45   0.24 ], 180.56240834014585\n",
      "93, btag_ltothers, 0.000, 1.000, 0.661, 0.875, 9030.319\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.577336\n",
      "         Iterations: 45\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 127\n",
      "[-0.98  -0.946  0.51   0.229], 180.5773360511461\n",
      "-0.2138221241107745 9279.430617200931\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.785288\n",
      "         Iterations: 44\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[-0.985 -0.948  0.375  0.252], 180.78528756757527\n",
      "94, btag_mudr, 0.000, 1.000, -0.060, 0.995, 9279.431\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.816234\n",
      "         Iterations: 41\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 95\n",
      "[-0.984 -0.945  0.455  0.239], 180.81623429282055\n",
      "-1.0551617948970127 9459.983729815507\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.552478\n",
      "         Iterations: 43\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 139\n",
      "[-0.979 -0.947  0.457  0.238], 180.55247817273238\n",
      "95, btag_mupt, 0.000, 1.000, 0.113, 0.995, 9459.984\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.686818\n",
      "         Iterations: 41\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 123\n",
      "[-0.988 -0.944  0.449  0.24 ], 180.68681751041075\n",
      "-0.8819015428361346 9657.810558068995\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.664061\n",
      "         Iterations: 43\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 92\n",
      "[-0.98  -0.954  0.439  0.242], 180.66406141219892\n",
      "96, btag_ptrel, 0.000, 1.000, 0.011, 0.998, 9657.811\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.715226\n",
      "         Iterations: 41\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 116\n",
      "[-0.991 -0.946  0.449  0.241], 180.7152263013925\n",
      "-0.9862611705426159 9784.531682478353\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.637472\n",
      "         Iterations: 38\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-0.974 -0.948  0.438  0.24 ], 180.63747151386556\n",
      "97, btag_sampledependence, 0.000, 1.000, 0.835, 0.784, 9784.532\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.580908\n",
      "         Iterations: 45\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 102\n",
      "[-0.985 -0.964  0.511  0.233], 180.5809082161772\n",
      "0.05159627598028582 9703.612645336472\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.780261\n",
      "         Iterations: 44\n",
      "         Function evaluations: 97\n",
      "         Gradient evaluations: 86\n",
      "[-0.975 -0.925  0.378  0.246], 180.78026075887055\n",
      "98, btag_pileup, 0.000, 1.000, -0.052, 0.994, 9703.613\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.818534\n",
      "         Iterations: 41\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 81\n",
      "[-0.98  -0.937  0.474  0.234], 180.81853403896238\n",
      "-1.046295831518919 9941.215348866315\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.550613\n",
      "         Iterations: 43\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 109\n",
      "[-0.982 -0.953  0.428  0.244], 180.55061283807996\n",
      "99, btag_statistic, 0.000, 1.000, 0.352, 0.945, 9941.215\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.693513\n",
      "         Iterations: 43\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 83\n",
      "[-0.986 -0.957  0.443  0.243], 180.69351309268097\n",
      "-0.5926727621857959 10450.892788404284\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.658601\n",
      "         Iterations: 43\n",
      "         Function evaluations: 160\n",
      "         Gradient evaluations: 148\n",
      "[-0.976 -0.935  0.454  0.236], 180.65860071698467\n",
      "100, ctag, 0.000, 1.000, 0.311, 0.820, 10450.893\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.404729\n",
      "         Iterations: 41\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 121\n",
      "[-0.807 -0.762  0.465  0.179], 180.40472865205845\n",
      "-0.5088750317755721 10649.903347872201\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.041185\n",
      "         Iterations: 39\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 78\n",
      "[-1.299 -1.283  0.407  0.352], 181.04118487206557\n",
      "101, mistag, 0.000, 1.000, 0.273, 0.924, 10649.903\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.410507\n",
      "         Iterations: 41\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.929 -0.903  0.405  0.231], 180.4105070475662\n",
      "-0.6504536536198746 10851.22256998236\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.038277\n",
      "         Iterations: 40\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-1.075 -1.017  0.533  0.252], 181.0382772849865\n",
      "102, jer, 0.000, 1.000, 0.196, 0.877, 10851.223\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.486698\n",
      "         Iterations: 43\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 135\n",
      "[-1.03  -0.98   0.58   0.231], 180.48669826457623\n",
      "-0.6815187477611278 10866.934770438033\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.899896\n",
      "         Iterations: 40\n",
      "         Function evaluations: 155\n",
      "         Gradient evaluations: 143\n",
      "[-0.914 -0.901  0.233  0.256], 180.8998962875943\n",
      "103, jes_subtotal_pileup, 0.000, 1.000, 0.061, 0.879, 10866.935\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.862517\n",
      "         Iterations: 54\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 101\n",
      "[-0.941 -0.9    0.52   0.214], 180.86251718250324\n",
      "-0.8189155487635712 13296.14004908232\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.518189\n",
      "         Iterations: 55\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 125\n",
      "[-1.009 -0.98   0.405  0.256], 180.51818865567876\n",
      "104, jes_subtotal_relative, 0.000, 1.000, 0.109, 0.894, 13296.140\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.836722\n",
      "         Iterations: 57\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 115\n",
      "[-0.926 -0.874  0.557  0.201], 180.83672191375885\n",
      "-0.7848015724038888 15939.651594113571\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.537229\n",
      "         Iterations: 52\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 125\n",
      "[-1.023 -1.002  0.371  0.268], 180.5372294615002\n",
      "105, jes_subtotal_pt, 0.000, 1.000, 0.107, 0.976, 15939.652\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.679808\n",
      "         Iterations: 56\n",
      "         Function evaluations: 143\n",
      "         Gradient evaluations: 131\n",
      "[-0.949 -0.915  0.487  0.223], 180.6798080483229\n",
      "-0.8685710140383024 17435.263157400157\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.672156\n",
      "         Iterations: 46\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 106\n",
      "[-1.013 -0.982  0.415  0.256], 180.67215585180153\n",
      "106, jes_subtotal_scale, 0.000, 1.000, 0.134, 0.985, 17435.263\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.631952\n",
      "         Iterations: 50\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.949 -0.918  0.493  0.223], 180.63195209213453\n",
      "-0.8509828830135971 18654.396429118926\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.721896\n",
      "         Iterations: 50\n",
      "         Function evaluations: 148\n",
      "         Gradient evaluations: 136\n",
      "[-1.017 -0.981  0.406  0.258], 180.7218960333924\n",
      "107, jes_subtotal_absolute, 0.000, 1.000, 0.115, 0.962, 18654.396\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.680950\n",
      "         Iterations: 50\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 102\n",
      "[-0.938 -0.907  0.501  0.218], 180.68094978435798\n",
      "-0.8469439015788118 20736.568724069322\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.670459\n",
      "         Iterations: 46\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 123\n",
      "[-1.025 -0.986  0.401  0.261], 180.67045946987565\n",
      "108, jes_flavor_qcd, 0.000, 1.000, 0.034, 0.791, 20736.569\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 181.068843\n",
      "         Iterations: 55\n",
      "         Function evaluations: 163\n",
      "         Gradient evaluations: 152\n",
      "[-0.8   -0.76   0.75   0.131], 181.06884259134154\n",
      "-0.7577234710841897 25613.491542471256\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 180.395134\n",
      "         Iterations: 54\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-1.072 -1.041  0.308  0.292], 180.39513422842475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate impacts for each nuisance parameter\n",
    "\n",
    "# initialize parameter data\n",
    "fit_data._pmask     = parameters['active'].values.astype(bool)\n",
    "fit_data._pval_init = parameters['val_fit'].values.copy()\n",
    "\n",
    "impacts_up, impacts_down = dict(), dict()\n",
    "iparam = 4\n",
    "mask = fit_data._pmask\n",
    "p_init = fit_data._pval_fit\n",
    "p_fit = parameters.val_fit.values\n",
    "for pname, pdata in tqdm(parameters.iloc[4:].iterrows(), total=parameters['active'].sum() - 4):\n",
    "    if not pdata.active:\n",
    "        iparam += 1\n",
    "        continue\n",
    "    \n",
    "    tqdm.write(f'{iparam}, {pname}, {pdata.val_init:.3f}, {pdata.err_init:.3f}, {pdata.val_fit:.3f}, {pdata.err_fit:.3f}, {fobj(p_init[mask]):.3f}')\n",
    "\n",
    "    # calculate impacts from up/down variations of n.p. on p.o.i.\n",
    "    mask[iparam] = False\n",
    "    p_init[iparam] = pdata.val_fit + pdata.err_fit\n",
    "    res = minimize(fobj, p_fit[mask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "    impacts_up[pname] = res.x\n",
    "    tqdm.write(f'{(res.x[:4]-p_fit[:4])*100/p_fit[:4]}, {res.fun}')\n",
    "                    \n",
    "    p_init[iparam] = pdata.val_fit - pdata.err_fit\n",
    "    print(p_init[iparam], fobj(p_init[mask]))\n",
    "    res = minimize(fobj, p_fit[mask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "    impacts_down[pname] = res.x\n",
    "    tqdm.write(f'{(res.x[:4]-p_fit[:4])*100/p_fit[:4]}, {res.fun}')\n",
    "    \n",
    "    mask[iparam] = True\n",
    "    iparam += 1\n",
    "    \n",
    "#impact_data = pd.DataFrame(impact_data, index=parameters.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.063350Z",
     "start_time": "2020-03-04T17:25:09.549Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'impacts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e71a3e43b098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert impacts to dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimpacts_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpacts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mp_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpacts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mp_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#impacts_up = pd.DataFrame(impacts_np, columns=['beta_e_up', 'beta_mu_up', 'beta_tau_up', 'beta_h_up'], index=list(p_labels_fancy[4:]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'active == 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'impacts' is not defined"
     ]
    }
   ],
   "source": [
    "# convert impacts to dataframes \n",
    "impacts_np = np.hstack([100*(impacts[:,0,:4] - p_fit[:4])/p_fit[:4], 100*(impacts[:,1,:4] - p_fit[:4])/p_fit[:4]])\n",
    "\n",
    "#impacts_up = pd.DataFrame(impacts_np, columns=['beta_e_up', 'beta_mu_up', 'beta_tau_up', 'beta_h_up'], index=list(p_labels_fancy[4:]))\n",
    "p_labels = parameters.query('active == 1').index.values[4:]\n",
    "column_labels = [\n",
    "    'beta_e_up', 'beta_mu_up', 'beta_tau_up', 'beta_h_up', \n",
    "    'beta_e_down', 'beta_mu_down', 'beta_tau_down', 'beta_h_down'\n",
    "   ] \n",
    "impacts_np = pd.DataFrame(impacts_np, \n",
    "                          columns=column_labels,\n",
    "                          index=list(p_labels)\n",
    "                         )\n",
    "impacts_np.to_csv('local_data/impacts_asimov.csv')\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "pd.set_option('display.max_columns', parameters.shape[0])\n",
    "pd.set_option('display.max_rows', parameters.shape[0])\n",
    "\n",
    "var = 'beta_tau'\n",
    "sorted_ind = impacts_np.abs().sort_values(by=f'{var}_down', ascending=False).index\n",
    "impacts_np.reindex(sorted_ind)[['beta_tau_up', 'beta_tau_down']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.064422Z",
     "start_time": "2020-03-04T17:25:09.557Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert to errors\n",
    "err_no_bb = parameters['err_no_bb'].values[mask]\n",
    "errs_np = np.array([np.concatenate([e[:i+4], [0], e[i+4:]]) for i, e in enumerate(errs)])\n",
    "errs_np = err_no_bb**2 - errs_np**2\n",
    "#errs_np[errs_np < 0] = 0\n",
    "#errs_np = np.sqrt(errs_np)\n",
    "##errs_np = np.vstack([errs_np, err_stat, err_mc_stat, err_syst, stderr])\n",
    "#\n",
    "errs_np = pd.DataFrame(errs_np[:,:4], columns=['beta_e', 'beta_mu', 'beta_tau', 'beta_h'], index=list(p_labels))\n",
    "##errs_np = pd.DataFrame(errs_np[:,:4], columns=['beta_e', 'beta_mu', 'beta_tau', 'beta_h'], index=list(p_labels_fancy[4:]) + ['stat.', 'MC stat.', 'syst. total', 'total'])\n",
    "##beta_stderr = stderr.iloc[:,:4].multiply(100)/params_init[:4]\n",
    "100*errs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.065367Z",
     "start_time": "2020-03-04T17:25:09.560Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# print table\n",
    "\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1).to_latex('local_data/errors.tex')\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1).to_csv('local_data/errors.csv')\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1)\n",
    "beta_errs.to_latex('local_data/summary_errors.tex', escape=False)\n",
    "beta_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.066536Z",
     "start_time": "2020-03-04T17:25:09.563Z"
    }
   },
   "outputs": [],
   "source": [
    "jes_mask = np.array([True if ('jes' in pname and 'btag' not in pname) else False for pname in beta_errs.index])\n",
    "btag_mask = np.array([True if 'btag' in pname else False for pname in beta_errs.index])\n",
    "tau_misid_mask = np.array([True if ('misid_tau' in pname and pname not in ['misid_tau_e', 'misid_tau_h']) else False for pname in beta_errs.index])\n",
    "\n",
    "btag_errs = beta_errs[btag_mask]\n",
    "jes_errs = beta_errs[jes_mask]\n",
    "tau_misid_errs = beta_errs[tau_misid_mask]\n",
    "\n",
    "summary_errs = beta_errs[~btag_mask&~jes_mask&~tau_misid_mask].copy()\n",
    "summary_errs.index = [fit_data._parameters.loc[p].label if p in fit_data._parameters.index else p for p in summary_errs.index]\n",
    "summary_errs.loc['b-tag',:] = np.sqrt(np.sum(btag_errs**2))\n",
    "summary_errs.loc['JES',:]  = np.sqrt(np.sum(jes_errs**2))\n",
    "summary_errs.loc[r'$\\sf jet\\rightarrow\\tau$',:]  = np.sqrt(np.sum(tau_misid_errs**2))\n",
    "\n",
    "summary_errs = summary_errs.divide(params_init[:4]/100, axis=1)\n",
    "summary_errs.to_latex('local_data/summary_errors.tex', escape=False)\n",
    "summary_errs.to_csv('local_data/summary_errors.csv')\n",
    "summary_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "29px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "49px",
    "left": "0px",
    "right": "1493.87px",
    "top": "90.9965px",
    "width": "242px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 744.85,
   "position": {
    "height": "40px",
    "left": "919px",
    "right": "20px",
    "top": "59px",
    "width": "678px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  },
  "widgets": {
   "state": {
    "012f8bbe2fdb410dae6e2cde9d7fe5cb": {
     "views": []
    },
    "080556076f174648bddf64f17a54c523": {
     "views": []
    },
    "0ad83b5f67484ae5b8fd8dd43ccc39bd": {
     "views": []
    },
    "15acd81a9adc493683d9b63813f000bf": {
     "views": []
    },
    "1840cb6fded848b4ae95ec8d3db15ab2": {
     "views": []
    },
    "1dd83f822e074642ae4255b15ee661cf": {
     "views": []
    },
    "1e71a878e6474912a0efc497ecc5d65b": {
     "views": []
    },
    "2022ed83777b4963b630b5c46239e218": {
     "views": []
    },
    "21c4c57bfc48495194663e6a4fbac488": {
     "views": []
    },
    "22c45c75435348c0b9501d493d69fdca": {
     "views": []
    },
    "2635f668a1af4a9db2642e705d7c73ff": {
     "views": []
    },
    "2ad5ddd9347e451b9290e5b4179ab9a2": {
     "views": []
    },
    "2c062b5778024117984822b63b0593d7": {
     "views": []
    },
    "2f5eab2f6fb24192b76a5ffe99195d44": {
     "views": []
    },
    "31632517325046e8b0cb62e4f4ed2480": {
     "views": []
    },
    "3562b97192ed4d42bbab17f77c290f6b": {
     "views": []
    },
    "38a7cc053723492b921cf9f084ed243c": {
     "views": []
    },
    "3b5750b20e1745879ca0f965aad7b614": {
     "views": []
    },
    "3f1cbabbe2694a9dabe3f1c2e09d0ee2": {
     "views": []
    },
    "3fab6a26a70c4238a668a46d4dc88bf6": {
     "views": []
    },
    "3fb3c7a25e954a4888996976fa107737": {
     "views": []
    },
    "415db64fbc574daea8457ab600392f09": {
     "views": []
    },
    "4463de406b4645a4b562fe7917380ff9": {
     "views": []
    },
    "487e5450b5a24507932709f1fa8f59c1": {
     "views": []
    },
    "48aba73013e74e71927f71d42fb44d14": {
     "views": []
    },
    "4a19ad30f77e4fe6a2c84c8b62378a47": {
     "views": []
    },
    "4bc83ff5270d41679d76d26cdded8313": {
     "views": []
    },
    "4bdd9dd5c5c64646a27fa9096851458b": {
     "views": []
    },
    "5014cd42705f45178d5e6eeffd70f119": {
     "views": []
    },
    "5259b340b68e4fdb97fb4eaf9d98d954": {
     "views": []
    },
    "5986ef5b605a42aca10bc5834529ee06": {
     "views": []
    },
    "5c1aa44589a140eb9709734c843abde6": {
     "views": []
    },
    "600cd9ca4f4c46d4ad6fe57df107675a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "61fe369ebdd14eaa89de110f6186e6b7": {
     "views": []
    },
    "62ac836017ae47a38f8fde806c5ec9b7": {
     "views": []
    },
    "632ccbfabe91405aa1c5a77c9ea754db": {
     "views": []
    },
    "66175f618ea5472baac618f998d2c06c": {
     "views": []
    },
    "6647a620af034d26abcd327ae02364d4": {
     "views": []
    },
    "6752222d2cba43e18f344a8db7f99d24": {
     "views": []
    },
    "6b684ba1a7c24a35ba2df77016212904": {
     "views": []
    },
    "6cea898f4aca4f1e84601f843e337238": {
     "views": []
    },
    "6f8d1e87fd60462a89d693b2f3b5f007": {
     "views": []
    },
    "74078646a5eb4047b40370a0ab8b6b30": {
     "views": []
    },
    "745b0c79ff3040788ea952fce9c7d607": {
     "views": []
    },
    "757c9b805eb7445bac9a7f141f87e45f": {
     "views": []
    },
    "76ced68e19a742e8976dbfd4e8594a1a": {
     "views": []
    },
    "783bb5e7538d4d9d8315e2698024b353": {
     "views": []
    },
    "794993d66efe4ab29a8d35aad8cfe079": {
     "views": []
    },
    "8375e24bae7541528d7cdc0f379d1d4c": {
     "views": []
    },
    "8554945ec15041a7bf8004dbc3fc5f11": {
     "views": []
    },
    "878a34e26cce4f18bb8232a682ebe964": {
     "views": []
    },
    "8921a75116a549198eb7b7f4a24ab672": {
     "views": []
    },
    "909f4504f0b049bda8b641defa177062": {
     "views": []
    },
    "910b9d32a3fb45ec99da1f9df1add816": {
     "views": []
    },
    "9d15ce601cd34f0699b7a7a0ce1d17dc": {
     "views": []
    },
    "a26638c9fee247b3891aac027a0918cc": {
     "views": []
    },
    "a9d2bf44a3ad447bb3eecde71363c198": {
     "views": []
    },
    "ad366bf4c95f4cdba62d47ba9501efc9": {
     "views": []
    },
    "ad8e1842ec314a94b6ed4b62c4c0a450": {
     "views": []
    },
    "af525094db304d2a812ae1312b00889b": {
     "views": []
    },
    "b0697c4343da491f9a35bf02681dad8f": {
     "views": []
    },
    "b07ff307919e4268bc8bec8379c47a5d": {
     "views": []
    },
    "b0e85c726ca141079333afb27edc63d4": {
     "views": []
    },
    "bdcc1e5df7a8432b9f40d8249a46f90a": {
     "views": []
    },
    "be1065f37fa24e818d31c3bb075947a3": {
     "views": []
    },
    "c296c8df2f734e268c6c1204536e7142": {
     "views": []
    },
    "c4bfd3e447f0426da144b76abc202129": {
     "views": []
    },
    "cced93184d4445218a2b14567579333d": {
     "views": []
    },
    "d5bd2e4d5f85482e9345f3a7a69380d0": {
     "views": []
    },
    "d798fa64e8be4a7d9ec1cbeece3b1be9": {
     "views": []
    },
    "d7aec0d6d05f442b991ab40af944811d": {
     "views": []
    },
    "db469cea2c8e4180bf6890de80329c1d": {
     "views": []
    },
    "e671857510c54634b6f0fa55bf1fa228": {
     "views": []
    },
    "ebf52deafaf64b0c826533dafdf993c0": {
     "views": []
    },
    "ed1e5439da9c41199a7bbbda21b556f8": {
     "views": []
    },
    "f585cf5db5024280af5b567f0e4fd771": {
     "views": []
    },
    "f6ba8f8800af47adabed847063bda8db": {
     "views": []
    },
    "f9bae72f14e44705b5c38a3ddc69fee8": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
