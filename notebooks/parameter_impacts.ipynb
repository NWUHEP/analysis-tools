{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating individual systematic contributions\n",
    "\n",
    "The effect of any individual systematic uncertainty is somewhat complicated by it's mutual covariance with the POI (i.e., the W branching fractions) and any other systematic uncertainty.  To get a rough idea of the percent-wise contribution to the total uncertainty from each individual systematic, I use the following scheme:\n",
    "\n",
    "   * the fit is carried out as in the nominal case and $\\sigma_{0}$ is estimated\n",
    "   * the fit is carried out for for each of the $n$ nuisance parameters $\\sigma_{theta}$\n",
    "   * the difference between the nominal case and the $n-1$ case is calculated,\n",
    "   * this quantity is normalized to $\\sum_{\\theta} \\sigma_{\\theta}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:09.639485Z",
     "start_time": "2020-03-04T17:25:09.500321Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/naodell/work/wbr/analysis\n",
      "{\n",
      "  \"shell_port\": 38009,\n",
      "  \"iopub_port\": 34267,\n",
      "  \"stdin_port\": 60623,\n",
      "  \"control_port\": 60263,\n",
      "  \"hb_port\": 43913,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"a60ae49e-aee33fe4b640f66032f39fd1\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-1e88735e-53e4-45fc-bccb-0eeb75431084.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "## imports and configuration\n",
    "%cd '/home/naodell/work/wbr/analysis'\n",
    "#%load_ext autoreload\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.optimize import minimize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scripts.plot_tools as pt\n",
    "import scripts.fit_helpers as fh\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "rc_params = {\n",
    "             'figure.figsize': (10, 10),\n",
    "             'axes.labelsize': 20,\n",
    "             'axes.facecolor': 'white',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'legend.fontsize': 20,\n",
    "             'xtick.labelsize':18,\n",
    "             'ytick.labelsize':18,\n",
    "             'font.size':18,\n",
    "             'font.sans-serif':['Arial', 'sans-serif'],\n",
    "             'mathtext.sf':'Arial',\n",
    "             'lines.markersize':8.,\n",
    "             'lines.linewidth':2.5,\n",
    "            }\n",
    "matplotlib.rcParams.update(rc_params)\n",
    "\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.006785Z",
     "start_time": "2020-03-04T17:25:09.640699Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# configure, get the input data, and do any additional processing that is needed\n",
    "processes = ['ttbar', 't', 'ww', 'wjets', 'zjets_alt', 'gjets', 'diboson', 'fakes'] \n",
    "selections = [\n",
    "              'ee',  'mumu',  \n",
    "              'emu', \n",
    "              'mutau', 'etau', \n",
    "              'mujet', 'ejet'\n",
    "             ]\n",
    "plot_labels = fh.fancy_labels\n",
    "\n",
    "# initialize fit data\n",
    "scenario = 'unblinded'\n",
    "infile = open(f'local_data/fit_data_{scenario}.pkl', 'rb')\n",
    "fit_data = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# get fit parameters\n",
    "parameters = fit_data._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.027567Z",
     "start_time": "2020-03-04T17:25:10.008440Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare Asimov dataset\n",
    "asimov_data = {cat:fit_data.mixture_model(parameters.val_init.values, cat) for cat in fit_data._model_data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.041626Z",
     "start_time": "2020-03-04T17:25:10.033422Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# initialize veto list\n",
    "fit_data.veto_list = [\n",
    "    # baseline\n",
    "    #'ee_cat_gt2_eq1_b', 'ee_cat_gt2_gt2_b', \n",
    "    #'mumu_cat_gt2_eq1_b', 'mumu_cat_gt2_gt2_b', \n",
    "    #'emu_cat_gt2_eq1_a', 'emu_cat_gt2_gt2_a', \n",
    "    #'etau_cat_eq2_eq1', 'etau_cat_gt3_eq1', 'etau_cat_eq2_gt2', 'etau_cat_gt3_gt2', \n",
    "    #'mutau_cat_eq2_eq1', 'mutau_cat_gt3_eq1', 'mutau_cat_eq2_gt2', 'mutau_cat_gt3_gt2', \n",
    "    #'ejet_cat_gt4_eq1', 'ejet_cat_gt4_gt2',\n",
    "    #'mujet_cat_gt4_eq1', 'mujet_cat_gt4_gt2', \n",
    "    'ejet_cat_eq3_gt2', 'mujet_cat_eq3_gt2',\n",
    "    \n",
    "    # e/mu DY CR\n",
    "    'ee_cat_gt2_eq0',  'mumu_cat_gt2_eq0', \n",
    "    \n",
    "    # e+mu additional ttbar\n",
    "    #'emu_cat_gt2_eq0', 'emu_cat_eq1_eq0_a', 'emu_cat_eq1_eq1_a', \n",
    "    \n",
    "    # e+mu WW\n",
    "    #'emu_cat_eq0_eq0_a', \n",
    "    \n",
    "    # e/mu+tau additional CR\n",
    "    #'mutau_cat_eq0_eq0', 'mutau_cat_eq1_eq0', \n",
    "    #'mutau_cat_gt2_eq0', 'mutau_cat_eq1_eq1', \n",
    "    #'etau_cat_eq0_eq0', 'etau_cat_eq1_eq0', \n",
    "    #'etau_cat_gt2_eq0', 'etau_cat_eq1_eq1', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.052086Z",
     "start_time": "2020-03-04T17:25:10.043686Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# fit configuration #\n",
    "\n",
    "# minimizer options\n",
    "#steps = 4*[1e-4, ] \n",
    "#steps += list(0.05*fit_data._perr_init[4: fit_data._nnorm + 7])\n",
    "#steps += list(0.05*fit_data._perr_init[fit_data._npoi + fit_data._nnorm:])\n",
    "#steps = np.array(steps)\n",
    "min_options = dict(\n",
    "                   #finite_diff_rel_step=steps,\n",
    "                   #verbose=3,\n",
    "                   #eps=1e-9, \n",
    "                   gtol = 1e-3,\n",
    "                   disp = True\n",
    "                  )\n",
    "\n",
    "# configure the objective\n",
    "sample = None\n",
    "fobj = partial(fit_data.objective,\n",
    "               data = sample,\n",
    "               do_bb_lite = True,\n",
    "               lu_test = 2\n",
    "              )\n",
    "\n",
    "fobj_jac = partial(fit_data.objective_jacobian,\n",
    "                   data = sample,\n",
    "                   do_bb_lite = True,\n",
    "                   lu_test = 2\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.062418Z",
     "start_time": "2020-03-04T17:25:10.053753Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9401c182d74dd38deaf81b311bbc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=103.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, br_tau_e, 0.177, 0.000, 0.177, 0.000, 966.331\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.373349\n",
      "         Iterations: 36\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-0.767 -0.57  -0.322  0.267], 239.37334900676063\n",
      "0.1769928611114198 963.9952954871104\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.349502\n",
      "         Iterations: 36\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 107\n",
      "[-0.759 -0.575 -0.326  0.267], 239.34950150369983\n",
      "5, br_tau_mu, 0.173, 0.000, 0.173, 0.000, 966.280\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.409884\n",
      "         Iterations: 7\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.013 -0.004  0.035 -0.003], 247.40988394275178\n",
      "0.17268021663996716 964.2744290917634\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.518845\n",
      "         Iterations: 36\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 122\n",
      "[-0.765 -0.561 -0.286  0.259], 239.51884539314915\n",
      "6, br_tau_h, 0.650, 0.001, 0.649, 0.001, 966.882\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.756261\n",
      "         Iterations: 5\n",
      "         Function evaluations: 85\n",
      "         Gradient evaluations: 73\n",
      "[ 0.02   0.005 -0.057  0.005], 247.75626080007765\n",
      "0.6484727919502018 961.7244540338156\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.252846\n",
      "         Iterations: 36\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[-0.795 -0.592 -0.221  0.259], 239.25284579138975\n",
      "7, lumi, 1.000, 0.025, 1.010, 0.018, 966.228\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.022955\n",
      "         Iterations: 48\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 132\n",
      "[-0.733 -0.569 -0.123  0.229], 239.02295455688352\n",
      "0.9920583301395017 731.7317781904732\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.810524\n",
      "         Iterations: 54\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 105\n",
      "[-0.726 -0.46  -0.498  0.271], 239.81052372593413\n",
      "8, xs_gjets, 1.000, 0.100, 1.085, 0.095, 1394.786\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.017975\n",
      "         Iterations: 36\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 135\n",
      "[-0.783 -0.607 -0.336  0.278], 239.0179750299584\n",
      "0.9893054374817674 1391.4912457744133\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.884215\n",
      "         Iterations: 36\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.716 -0.487 -0.256  0.235], 239.88421547773942\n",
      "9, xs_diboson, 1.000, 0.100, 1.016, 0.099, 1422.112\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.158103\n",
      "         Iterations: 36\n",
      "         Function evaluations: 143\n",
      "         Gradient evaluations: 131\n",
      "[-0.761 -0.59  -0.383  0.279], 239.1581027987886\n",
      "0.917220810411818 1414.0452974804712\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.561154\n",
      "         Iterations: 36\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[-0.769 -0.573 -0.27   0.259], 239.56115419872032\n",
      "10, xs_ww, 1.000, 0.100, 0.965, 0.028, 1423.797\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.768100\n",
      "         Iterations: 39\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.726 -0.58  -0.478  0.287], 239.76809983655326\n",
      "0.9369882666790111 1394.1864802209082\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.055304\n",
      "         Iterations: 45\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 102\n",
      "[-0.776 -0.548 -0.166  0.24 ], 239.05530401172078\n",
      "11, xs_t, 1.000, 0.100, 0.932, 0.072, 1405.695\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.812378\n",
      "         Iterations: 37\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[-0.239  0.012  0.828 -0.098], 240.81237848727505\n",
      "0.8593512081312181 1057.1913059452068\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.887771\n",
      "         Iterations: 37\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 67\n",
      "[-0.743 -0.571 -0.171  0.239], 238.88777131362554\n",
      "13, e_fakes, 1.000, 1.000, 1.064, 0.096, 1224.696\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.043818\n",
      "         Iterations: 36\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.729 -0.495 -0.308  0.246], 239.04381778929672\n",
      "0.9681717565505223 1200.8775749630763\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.808896\n",
      "         Iterations: 36\n",
      "         Function evaluations: 148\n",
      "         Gradient evaluations: 136\n",
      "[-0.822 -0.712 -0.196  0.278], 239.80889585042152\n",
      "14, mu_fakes, 1.000, 1.000, 0.868, 0.037, 1276.652\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.998043\n",
      "         Iterations: 37\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.627 -0.352 -0.536  0.244], 239.99804278498289\n",
      "0.8306860875623046 1179.221169014171\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.958320\n",
      "         Iterations: 41\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 140\n",
      "[-0.786 -0.61  -0.195  0.256], 238.95831950803478\n",
      "15, e_fakes_ss, 1.000, 0.300, 0.995, 0.035, 1193.806\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.699418\n",
      "         Iterations: 37\n",
      "         Function evaluations: 152\n",
      "         Gradient evaluations: 140\n",
      "[-0.676 -0.636 -0.446  0.283], 239.69941844354503\n",
      "0.9603234300085188 1185.1610518123375\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.116890\n",
      "         Iterations: 37\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[-0.808 -0.54  -0.184  0.246], 239.1168900548365\n",
      "16, mu_fakes_ss, 1.000, 0.300, 1.126, 0.052, 1192.528\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.002000\n",
      "         Iterations: 40\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 115\n",
      "[-0.839 -0.582 -0.404  0.294], 239.00199968135942\n",
      "1.074280621845549 1211.6541524267143\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.927123\n",
      "         Iterations: 39\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-0.494 -0.512  0.128  0.141], 239.92712348769962\n",
      "17, trigger_mu, 1.000, 0.005, 1.000, 0.004, 1231.137\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.888449\n",
      "         Iterations: 36\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[-0.568 -0.805 -0.349  0.277], 239.88844931848746\n",
      "0.9959215207142859 1133.2090644387897\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.995947\n",
      "         Iterations: 36\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[-0.815 -0.438 -0.301  0.25 ], 238.99594742140312\n",
      "18, xs_zjets_alt_pdf, 0.000, 1.000, 0.130, 0.968, 1237.146\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.152544\n",
      "         Iterations: 37\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.758 -0.596 -0.294  0.265], 239.1525441383316\n",
      "-0.8376267619374022 1213.4887383371297\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.566610\n",
      "         Iterations: 41\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 118\n",
      "[-0.75  -0.532 -0.255  0.247], 239.5666101357751\n",
      "19, xs_zjets_alt_alpha_s, 0.000, 1.000, -0.320, 0.904, 1241.221\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.817363\n",
      "         Iterations: 38\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.747 -0.48  -0.201  0.229], 239.81736324333934\n",
      "-1.223800424525765 1314.8797504545632\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.047235\n",
      "         Iterations: 37\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.762 -0.616 -0.454  0.295], 239.04723511625113\n",
      "20, xs_zjets_alt_qcd_scale_0, 0.000, 1.000, -0.828, 0.474, 1258.841\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 241.056656\n",
      "         Iterations: 38\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 95\n",
      "[-0.299 -0.282  0.507  0.012], 241.0566556136691\n",
      "-1.3020005646344883 1244.5838419401985\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.858337\n",
      "         Iterations: 37\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 122\n",
      "[-0.698 -0.499 -0.2    0.225], 238.8583372005332\n",
      "21, xs_zjets_alt_qcd_scale_1, 0.000, 1.000, -0.869, 0.458, 1233.732\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.923207\n",
      "         Iterations: 37\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 69\n",
      "[-0.671 -0.54  -0.475  0.271], 239.9232065524345\n",
      "-1.3269557844135471 1208.7131088403644\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.986623\n",
      "         Iterations: 37\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[-0.771 -0.583 -0.268  0.261], 238.98662288963274\n",
      "22, xs_zjets_alt_qcd_scale_2, 0.000, 1.000, -0.241, 0.306, 1213.954\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.932333\n",
      "         Iterations: 37\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.956 -0.776 -0.797  0.407], 239.93233342591716\n",
      "-0.547042396329479 1172.6225839283632\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.955531\n",
      "         Iterations: 37\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.692 -0.488 -0.074  0.201], 238.95553078224594\n",
      "23, xs_wjets_qcd_scale_0, 0.000, 1.000, -0.438, 0.761, 1194.261\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.241424\n",
      "         Iterations: 37\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 120\n",
      "[-0.835 -0.637  0.058  0.227], 239.24142354555502\n",
      "-1.1981736358040183 1193.975968915189\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.527515\n",
      "         Iterations: 37\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 94\n",
      "[-0.659 -0.475 -0.827  0.316], 239.52751478182742\n",
      "24, xs_wjets_qcd_scale_1, 0.000, 1.000, -0.314, 0.928, 1192.790\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.379909\n",
      "         Iterations: 37\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 125\n",
      "[-0.829 -0.648 -0.455  0.311], 239.37990901606236\n",
      "-1.2415864360217816 1190.3214695197014\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.288153\n",
      "         Iterations: 37\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[-0.702 -0.489 -0.309  0.241], 239.28815348253593\n",
      "25, xs_wjets_qcd_scale_2, 0.000, 1.000, 1.379, 0.600, 1191.894\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.870070\n",
      "         Iterations: 38\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 119\n",
      "[-0.755 -0.545 -0.203  0.242], 238.8700698005237\n",
      "0.7785218302476906 1200.1321393996445\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.426837\n",
      "         Iterations: 37\n",
      "         Function evaluations: 148\n",
      "         Gradient evaluations: 136\n",
      "[-0.68  -0.512 -0.853  0.329], 240.42683717765533\n",
      "26, xs_wjets_qcd_scale_3, 0.000, 1.000, -0.215, 0.953, 1210.384\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.522280\n",
      "         Iterations: 37\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 108\n",
      "[-0.785 -0.582 -0.279  0.265], 239.5222796596308\n",
      "-1.1677142547692485 1208.9007256932493\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.196518\n",
      "         Iterations: 37\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 96\n",
      "[-0.772 -0.579 -0.312  0.267], 239.19651802941536\n",
      "27, xs_wjets_qcd_scale_4, 0.000, 1.000, 1.846, 0.365, 1209.962\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.026607\n",
      "         Iterations: 38\n",
      "         Function evaluations: 165\n",
      "         Gradient evaluations: 153\n",
      "[-0.734 -0.558 -0.491  0.287], 239.02660712148875\n",
      "1.4815919967744435 1679.2944773295415\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.865281\n",
      "         Iterations: 42\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 96\n",
      "[-0.783 -0.569  0.201  0.185], 239.8652811725632\n",
      "28, xs_ttbar_pdf, 0.000, 1.000, -0.424, 1.003, 1838.575\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.897678\n",
      "         Iterations: 50\n",
      "         Function evaluations: 163\n",
      "         Gradient evaluations: 152\n",
      "[-0.735 -0.538 -0.244  0.244], 239.8976780601804\n",
      "-1.4276773121925956 1227.8370641218191\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.016567\n",
      "         Iterations: 51\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.775 -0.587 -0.297  0.267], 239.01656681510818\n",
      "29, xs_ttbar_alpha_s, 0.000, 1.000, 0.383, 1.015, 1633.958\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.033849\n",
      "         Iterations: 55\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.779 -0.586 -0.297  0.267], 239.0338492300385\n",
      "-0.6313320532041141 2055.8751787905603\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.871875\n",
      "         Iterations: 53\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 139\n",
      "[-0.734 -0.544 -0.252  0.246], 239.87187489254902\n",
      "30, xs_ttbar_qcd_scale, 0.000, 1.000, -0.475, 0.251, 1414.250\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.945594\n",
      "         Iterations: 58\n",
      "         Function evaluations: 164\n",
      "         Gradient evaluations: 152\n",
      "[-0.872 -0.695 -0.487  0.33 ], 238.94559427424838\n",
      "-0.7259681087359385 2587.5786196680638\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.199661\n",
      "         Iterations: 45\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[-0.319 -0.104  0.556 -0.022], 240.19966079681504\n",
      "31, eff_tau_0, 0.000, 1.000, -0.834, 0.489, 1035.121\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.192552\n",
      "         Iterations: 38\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 124\n",
      "[-0.714 -0.538 -0.846  0.338], 240.1925524404893\n",
      "-1.3236178205239215 1053.41078422261\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.917578\n",
      "         Iterations: 41\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 84\n",
      "[-0.783 -0.583 -0.167  0.246], 238.91757821203151\n",
      "32, eff_tau_1, 0.000, 1.000, -0.085, 0.361, 1041.691\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.423932\n",
      "         Iterations: 37\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 124\n",
      "[-0.792 -0.503 -0.644  0.312], 239.4239317399469\n",
      "-0.4465409969181274 1048.537263768505\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.278368\n",
      "         Iterations: 40\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 105\n",
      "[-0.736 -0.6    0.049  0.207], 239.27836768028564\n",
      "33, eff_tau_2, 0.000, 1.000, -0.415, 0.274, 1042.587\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.535756\n",
      "         Iterations: 36\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[-0.844 -0.474 -0.922  0.361], 239.53575620552564\n",
      "-0.6893094203521879 1055.3744166211097\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.181292\n",
      "         Iterations: 40\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.653 -0.569  0.277  0.152], 239.18129234732598\n",
      "34, eff_tau_3, 0.000, 1.000, -0.871, 0.366, 1047.178\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.370494\n",
      "         Iterations: 42\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 121\n",
      "[-0.705 -0.449 -1.263  0.389], 240.3704941997145\n",
      "-1.237330251320853 1060.1146760670263\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.880448\n",
      "         Iterations: 36\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.768 -0.607 -0.051  0.229], 238.88044756793465\n",
      "35, eff_tau_4, 0.000, 1.000, -0.398, 0.423, 1052.042\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.642648\n",
      "         Iterations: 39\n",
      "         Function evaluations: 155\n",
      "         Gradient evaluations: 143\n",
      "[-0.688 -0.477 -1.288  0.395], 239.6426482974626\n",
      "-0.8213893274562429 1058.1693690411737\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.087384\n",
      "         Iterations: 35\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 116\n",
      "[-0.854 -0.652  0.179  0.213], 239.08738416353376\n",
      "36, eff_tau_5, 0.000, 1.000, -1.370, 0.388, 1053.696\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.086480\n",
      "         Iterations: 39\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 132\n",
      "[-0.611 -0.418 -1.579  0.42 ], 240.08648045555265\n",
      "-1.7575006795350954 1041.3427693025367\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.946648\n",
      "         Iterations: 44\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[-0.821 -0.618  0.224  0.195], 238.94664763781822\n",
      "37, misid_tau_e, 0.000, 1.000, 0.127, 0.995, 1037.950\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.483886\n",
      "         Iterations: 6\n",
      "         Function evaluations: 85\n",
      "         Gradient evaluations: 73\n",
      "[ 0.01   0.004 -0.067  0.009], 247.48388580848007\n",
      "-0.8685257579285816 1039.7951349085588\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.467161\n",
      "         Iterations: 37\n",
      "         Function evaluations: 158\n",
      "         Gradient evaluations: 146\n",
      "[-0.793 -0.602 -0.279  0.269], 239.46716087308778\n",
      "38, misid_tau_h, 0.000, 1.000, -0.345, 0.592, 1037.749\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.661646\n",
      "         Iterations: 37\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 78\n",
      "[-0.599 -0.488 -0.955  0.329], 239.6616458341318\n",
      "-0.9372696629636434 1083.2782632328522\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.107641\n",
      "         Iterations: 38\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 119\n",
      "[-0.848 -0.616 -0.018  0.238], 239.10764123731255\n",
      "39, misid_tau_0, 0.000, 1.000, 0.405, 0.602, 1051.127\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.071887\n",
      "         Iterations: 38\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 95\n",
      "[-0.817 -0.616 -0.264  0.273], 239.07188724689\n",
      "-0.197073273314339 1055.3962966950057\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.755286\n",
      "         Iterations: 38\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.704 -0.567 -0.461  0.279], 239.75528563869617\n",
      "40, misid_tau_1, 0.000, 1.000, -0.107, 0.718, 1044.196\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.491254\n",
      "         Iterations: 37\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 126\n",
      "[-0.751 -0.545 -0.273  0.252], 239.49125416925293\n",
      "-0.824507611709957 1052.15881552329\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.218574\n",
      "         Iterations: 38\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 108\n",
      "[-0.829 -0.606 -0.261  0.273], 239.21857420270328\n",
      "41, misid_tau_2, 0.000, 1.000, 0.043, 0.742, 1045.008\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.473443\n",
      "         Iterations: 37\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.694 -0.544 -0.473  0.275], 239.47344339872592\n",
      "-0.6987751799903655 1052.5502344596766\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.228928\n",
      "         Iterations: 37\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.851 -0.61  -0.225  0.271], 239.22892759617338\n",
      "42, misid_tau_3, 0.000, 1.000, 0.063, 0.895, 1044.629\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.766268\n",
      "         Iterations: 5\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.015  0.027 -0.164  0.02 ], 247.76626788874302\n",
      "-0.8314427566653368 1048.6708456811486\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.605175\n",
      "         Iterations: 36\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 122\n",
      "[-0.796 -0.575 -0.106  0.237], 239.6051752012435\n",
      "43, misid_tau_4, 0.000, 1.000, -0.277, 0.947, 1044.385\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.962091\n",
      "         Iterations: 5\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[-0.003  0.015 -0.077  0.011], 247.96209107785697\n",
      "-1.2241392254886319 1047.8144322341384\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.140502\n",
      "         Iterations: 35\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.801 -0.584 -0.248  0.263], 239.1405024353359\n",
      "44, misid_tau_5, 0.000, 1.000, -0.346, 0.956, 1044.926\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 248.140940\n",
      "         Iterations: 5\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 72\n",
      "[-0.002  0.013 -0.064  0.008], 248.14093952318663\n",
      "-1.3021771877901176 1046.841128304787\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.191402\n",
      "         Iterations: 37\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 133\n",
      "[-0.782 -0.574 -0.274  0.262], 239.19140182224305\n",
      "45, escale_tau, 0.000, 1.000, 0.254, 0.242, 1045.147\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.752131\n",
      "         Iterations: 40\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 137\n",
      "[-0.828 -0.596 -0.146  0.252], 238.7521313447649\n",
      "0.011969936106296625 1044.8085277030928\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.724128\n",
      "         Iterations: 41\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[-0.6   -0.502 -0.696  0.29 ], 240.72412825956945\n",
      "46, eff_reco_e, 0.000, 1.000, 0.538, 0.930, 1039.121\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.110809\n",
      "         Iterations: 42\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-1.092 -0.483 -0.365  0.312], 239.11080891729836\n",
      "-0.392568535837465 1069.3023137752577\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.652963\n",
      "         Iterations: 37\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 65\n",
      "[-0.282 -0.692 -0.249  0.197], 239.65296322542756\n",
      "48, eff_e_0, 0.000, 1.000, -1.689, 0.723, 1004.854\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 249.299302\n",
      "         Iterations: 6\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 65\n",
      "[-0.021 -0.003 -0.003  0.004], 249.29930152480313\n",
      "-2.4122300066661673 1047.5536655739897\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.915359\n",
      "         Iterations: 36\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 119\n",
      "[-0.78  -0.581 -0.362  0.277], 238.91535947268906\n",
      "49, eff_e_1, 0.000, 1.000, 0.483, 0.901, 1032.043\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.315704\n",
      "         Iterations: 6\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 68\n",
      "[-0.015 -0.006  0.015  0.001], 247.31570435373982\n",
      "-0.41755720317076295 1036.4691848527393\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.717331\n",
      "         Iterations: 36\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 68\n",
      "[-0.65  -0.564 -0.374  0.255], 239.7173314424964\n",
      "50, eff_e_2, 0.000, 1.000, 1.034, 0.832, 1027.314\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.902703\n",
      "         Iterations: 40\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.794 -0.563 -0.251  0.258], 238.9027027589346\n",
      "0.20117283452796753 1025.0845907448295\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.216192\n",
      "         Iterations: 37\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 128\n",
      "[-0.533 -0.557 -0.421  0.243], 240.21619187161798\n",
      "51, eff_e_3, 0.000, 1.000, -1.232, 0.922, 1016.797\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 248.857999\n",
      "         Iterations: 6\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 62\n",
      "[-0.007 -0.006  0.023 -0.002], 248.85799873961594\n",
      "-2.153987372687993 1027.2695192501135\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.855771\n",
      "         Iterations: 37\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 107\n",
      "[-0.773 -0.588 -0.333  0.273], 238.85577134271355\n",
      "52, eff_e_4, 0.000, 1.000, -0.621, 0.463, 1021.901\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 248.544140\n",
      "         Iterations: 5\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[-0.007 -0.008  0.09  -0.012], 248.54413986072362\n",
      "-1.083570563033379 1047.0583545228258\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.069473\n",
      "         Iterations: 41\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 93\n",
      "[-0.682 -0.593 -0.394  0.269], 239.06947261278341\n",
      "53, eff_e_5, 0.000, 1.000, -0.879, 0.428, 1034.590\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.003741\n",
      "         Iterations: 38\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.965 -0.474  0.081  0.218], 240.0037407112576\n",
      "-1.3070882424092067 1089.842654189947\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.957692\n",
      "         Iterations: 37\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[-0.716 -0.594 -0.443  0.282], 238.95769191079734\n",
      "54, trigger_e_0, 0.000, 1.000, -1.028, 0.944, 1061.371\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.218237\n",
      "         Iterations: 36\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[-0.762 -0.475 -0.158  0.224], 240.2182374956287\n",
      "-1.9715277130358313 1078.5618737581574\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.907170\n",
      "         Iterations: 41\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 109\n",
      "[-0.761 -0.59  -0.364  0.276], 238.90717016684602\n",
      "55, trigger_e_1, 0.000, 1.000, -0.534, 0.985, 1069.327\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 248.229811\n",
      "         Iterations: 6\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 96\n",
      "[ 0.004  0.005  0.024 -0.005], 248.22981119614488\n",
      "-1.5192344833463838 1076.9891237086017\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.068418\n",
      "         Iterations: 36\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[-0.8   -0.616 -0.36   0.286], 239.06841772181804\n",
      "56, trigger_e_2, 0.000, 1.000, -0.384, 0.992, 1071.681\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 248.035504\n",
      "         Iterations: 6\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 60\n",
      "[ 0.006  0.007  0.015 -0.005], 248.03550365009957\n",
      "-1.3762061171002902 1076.964770276833\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.134837\n",
      "         Iterations: 36\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[-0.786 -0.605 -0.39   0.286], 239.1348369345127\n",
      "57, trigger_e_3, 0.000, 1.000, -0.421, 0.991, 1072.935\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 248.083507\n",
      "         Iterations: 6\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 67\n",
      "[ 0.006  0.007  0.017 -0.005], 248.08350703557434\n",
      "-1.4119240323610773 1078.942468324344\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.115494\n",
      "         Iterations: 36\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[-0.79  -0.61  -0.392  0.288], 239.1154943220707\n",
      "58, trigger_e_4, 0.000, 1.000, -0.429, 0.906, 1074.480\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 248.302357\n",
      "         Iterations: 5\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 31\n",
      "[-0.011 -0.013  0.041 -0.003], 248.30235727636347\n",
      "-1.3350031891529248 1085.4140205516155\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.060348\n",
      "         Iterations: 40\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 126\n",
      "[-0.789 -0.585 -0.259  0.263], 239.06034760827785\n",
      "59, trigger_e_5, 0.000, 1.000, -0.056, 0.523, 1077.589\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.748462\n",
      "         Iterations: 38\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 80\n",
      "[-0.838 -0.538 -0.363  0.28 ], 239.7484622938135\n",
      "-0.5786623366075415 1104.8747909503581\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.025334\n",
      "         Iterations: 37\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 87\n",
      "[-0.707 -0.546 -0.263  0.244], 239.02533407912438\n",
      "60, escale_e, 0.000, 1.000, 2.043, 0.823, 1079.737\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.197060\n",
      "         Iterations: 36\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 109\n",
      "[-0.684 -0.514 -0.335  0.247], 239.1970597673074\n",
      "1.219370889327374 1078.2493544321367\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 249.548263\n",
      "         Iterations: 8\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[-0.014  0.031 -0.093  0.012], 249.54826279482612\n",
      "61, trigger_e_tag, 0.000, 1.000, 1.079, 0.816, 1078.925\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.990112\n",
      "         Iterations: 37\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 73\n",
      "[-0.853 -0.566 -0.444  0.3  ], 238.9901124000881\n",
      "0.26262419079556965 1069.6108078474706\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.927860\n",
      "         Iterations: 37\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 138\n",
      "[-0.434 -0.555 -0.301  0.208], 239.9278602407171\n",
      "62, trigger_e_probe, 0.000, 1.000, 1.480, 0.734, 1045.149\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.864249\n",
      "         Iterations: 37\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.781 -0.573 -0.46   0.292], 238.86424892741456\n",
      "0.7461881502615519 1027.8226053993083\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.522947\n",
      "         Iterations: 37\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 133\n",
      "[-0.799 -0.567  0.271  0.175], 240.5229474890008\n",
      "63, e_prefire, 0.000, 1.000, -0.289, 0.916, 1014.410\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.688310\n",
      "         Iterations: 41\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[-1.054 -0.509 -0.131  0.272], 239.6883095281426\n",
      "-1.204803000851272 1076.1464802303535\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.080827\n",
      "         Iterations: 37\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.607 -0.588 -0.39   0.255], 239.0808270807897\n",
      "64, eff_iso_mu, 0.000, 1.000, 0.182, 0.984, 1027.426\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.506374\n",
      "         Iterations: 6\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[ 0.011  0.003 -0.021  0.001], 247.50637360530865\n",
      "-0.8019770052881798 1039.7058416806772\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.573497\n",
      "         Iterations: 37\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 130\n",
      "[-0.773 -0.529 -0.316  0.26 ], 239.57349716507485\n",
      "65, eff_id_mu, 0.000, 1.000, 0.262, 0.727, 1024.762\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.108527\n",
      "         Iterations: 36\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.733 -0.588 -0.299  0.26 ], 239.10852692104206\n",
      "-0.4641610208104074 1054.235813326213\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.650058\n",
      "         Iterations: 37\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[-0.772 -0.449 -0.198  0.228], 239.65005778772678\n",
      "66, eff_mu_0, 0.000, 1.000, 0.582, 0.980, 1008.866\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.027773\n",
      "         Iterations: 8\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 62\n",
      "[-0.011  0.016  0.009 -0.002], 247.02777276028573\n",
      "-0.3982939542774485 1009.6318632046122\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.006514\n",
      "         Iterations: 36\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 74\n",
      "[-0.792 -0.633 -0.211  0.263], 240.00651391829746\n",
      "67, eff_mu_1, 0.000, 1.000, 0.342, 0.972, 1008.054\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.152479\n",
      "         Iterations: 37\n",
      "         Function evaluations: 165\n",
      "         Gradient evaluations: 153\n",
      "[-0.761 -0.571 -0.419  0.282], 239.15247921791874\n",
      "-0.630604173217334 1009.3162479178193\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.618775\n",
      "         Iterations: 36\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 70\n",
      "[-0.763 -0.588 -0.218  0.252], 239.61877464696812\n",
      "68, eff_mu_2, 0.000, 1.000, -0.072, 0.977, 1007.549\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.413086\n",
      "         Iterations: 37\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 110\n",
      "[-0.794 -0.605 -0.422  0.293], 239.41308574317316\n",
      "-1.0489760471293343 1009.5427700441785\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.323146\n",
      "         Iterations: 37\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 109\n",
      "[-0.786 -0.576 -0.236  0.257], 239.32314564553826\n",
      "69, eff_mu_3, 0.000, 1.000, 0.551, 0.848, 1007.648\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.090327\n",
      "         Iterations: 37\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[-0.767 -0.624 -0.481  0.301], 239.09032736186734\n",
      "-0.2971202171129449 1010.092504122034\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 248.194033\n",
      "         Iterations: 5\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 72\n",
      "[ 0.022 -0.001  0.059 -0.013], 248.1940325525451\n",
      "70, eff_mu_4, 0.000, 1.000, -0.031, 0.951, 1003.557\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.417634\n",
      "         Iterations: 37\n",
      "         Function evaluations: 153\n",
      "         Gradient evaluations: 141\n",
      "[-0.807 -0.645 -0.337  0.288], 239.41763402055892\n",
      "-0.9823786721632816 1006.8932267132807\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.590328\n",
      "         Iterations: 5\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[ 0.005  0.004  0.022 -0.005], 247.5903279245242\n",
      "71, eff_mu_5, 0.000, 1.000, -0.015, 0.998, 1003.645\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.363380\n",
      "         Iterations: 37\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 95\n",
      "[-0.776 -0.577 -0.271  0.261], 239.36337996537682\n",
      "-1.0131345634651312 1004.477179329563\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.364062\n",
      "         Iterations: 37\n",
      "         Function evaluations: 148\n",
      "         Gradient evaluations: 136\n",
      "[-0.759 -0.561 -0.303  0.261], 239.36406205979682\n",
      "72, eff_mu_6, 0.000, 1.000, 0.279, 0.988, 1003.650\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.092370\n",
      "         Iterations: 37\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 138\n",
      "[-0.759 -0.591 -0.333  0.271], 239.0923698863898\n",
      "-0.7098382559090342 1004.6706894901545\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.882449\n",
      "         Iterations: 6\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 61\n",
      "[-0.011  0.     0.023 -0.002], 247.88244913368163\n",
      "73, eff_mu_7, 0.000, 1.000, -0.967, 0.977, 1003.391\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.526492\n",
      "         Iterations: 37\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.744 -0.625 -0.242  0.259], 240.52649174986618\n",
      "-1.9439240615007316 1001.3100130247796\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 246.663959\n",
      "         Iterations: 6\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 87\n",
      "[-0.005  0.004  0.013 -0.002], 246.66395915826007\n",
      "74, escale_mu, 0.000, 1.000, -0.618, 0.687, 1001.845\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.630370\n",
      "         Iterations: 37\n",
      "         Function evaluations: 152\n",
      "         Gradient evaluations: 140\n",
      "[-0.72  -0.585 -0.059  0.219], 239.63036976412315\n",
      "-1.304253871220146 1005.3581395802411\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.203516\n",
      "         Iterations: 8\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 49\n",
      "[ 0.002  0.015 -0.059  0.007], 247.20351574970368\n",
      "75, pileup, 0.000, 1.000, -0.403, 0.560, 1002.566\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 240.591766\n",
      "         Iterations: 37\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[-0.455 -0.623 -0.128  0.194], 240.59176604674383\n",
      "-0.9625301196994553 832.6903376635119\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 238.858547\n",
      "         Iterations: 43\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 139\n",
      "[-0.746 -0.515 -0.265  0.245], 238.85854702553303\n",
      "76, isr, 0.000, 1.000, 0.410, 0.235, 922.211\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.451593\n",
      "         Iterations: 37\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 109\n",
      "[-0.634 -0.464  0.37   0.116], 239.45159341591028\n",
      "0.17488297786280174 892.2490006360653\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.244938\n",
      "         Iterations: 36\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.9   -0.699 -1.021  0.422], 239.24493821964825\n",
      "77, fsr, 0.000, 1.000, 0.038, 0.082, 858.873\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.249967\n",
      "         Iterations: 39\n",
      "         Function evaluations: 85\n",
      "         Gradient evaluations: 74\n",
      "[-0.807 -0.641 -0.244  0.272], 239.24996739773675\n",
      "-0.04338207751735711 786.6428899608219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.499002\n",
      "         Iterations: 42\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 121\n",
      "[-0.762 -0.539 -0.47   0.285], 239.49900209832245\n",
      "78, hdamp, 0.000, 1.000, 0.046, 0.114, 928.388\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.107127\n",
      "         Iterations: 40\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 137\n",
      "[-0.8   -0.583 -0.303  0.271], 239.10712708247343\n",
      "-0.06748777581427062 954.0209146990235\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.613955\n",
      "         Iterations: 35\n",
      "         Function evaluations: 148\n",
      "         Gradient evaluations: 136\n",
      "[-0.738 -0.56  -0.369  0.268], 239.61395515194224\n",
      "79, tune, 0.000, 1.000, 0.065, 0.201, 911.800\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.406112\n",
      "         Iterations: 37\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.744 -0.493  0.224  0.163], 239.4061118888985\n",
      "-0.13677412556920987 910.9992530874953\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.292584\n",
      "         Iterations: 36\n",
      "         Function evaluations: 169\n",
      "         Gradient evaluations: 157\n",
      "[-0.835 -0.688 -0.916  0.393], 239.29258380178666\n",
      "80, ww_scale, 0.000, 1.000, 0.622, 0.653, 912.449\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 247.355978\n",
      "         Iterations: 6\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[ 0.001  0.008  0.02  -0.005], 247.35597804306522\n",
      "-0.030774390245935757 912.7163758633619\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.589007\n",
      "         Iterations: 37\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 107\n",
      "[-0.754 -0.575 -0.177  0.242], 239.58900660357597\n",
      "81, ww_resum, 0.000, 1.000, -0.069, 0.895, 908.189\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.332810\n",
      "         Iterations: 37\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[-0.789 -0.579 -0.21   0.254], 239.3328096007094\n",
      "-0.9638120816911526 909.8629357770574\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.301553\n",
      "         Iterations: 37\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 111\n",
      "[-0.78  -0.594 -0.441  0.292], 239.30155318893011\n",
      "82, top_pt, 1.000, 1.000, 1.453, 0.067, 908.207\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.357276\n",
      "         Iterations: 41\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[-0.596 -0.386 -0.184  0.187], 239.35727550294087\n",
      "1.3857855589146728 846.5195844855604\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.349577\n",
      "         Iterations: 41\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[-0.931 -0.757 -0.336  0.326], 239.3495771615332\n",
      "83, btag_bfragmentation, 0.000, 1.000, 0.124, 0.988, 847.246\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.393196\n",
      "         Iterations: 37\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.763 -0.57  -0.285  0.26 ], 239.39319553236572\n",
      "-0.8640887014792552 947.1238532571969\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.284826\n",
      "         Iterations: 36\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[-0.74  -0.528 -0.359  0.262], 239.2848258976744\n",
      "84, btag_btempcorr, 0.000, 1.000, 0.055, 0.989, 833.683\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.472364\n",
      "         Iterations: 37\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 102\n",
      "[-0.76  -0.562 -0.314  0.263], 239.47236438098008\n",
      "-0.9343127654711247 929.4611737946019\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.221266\n",
      "         Iterations: 36\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.77  -0.572 -0.32   0.267], 239.2212657402082\n",
      "85, btag_cb, 0.000, 1.000, -0.074, 0.994, 828.358\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.496994\n",
      "         Iterations: 37\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 109\n",
      "[-0.735 -0.528 -0.227  0.24 ], 239.49699421234388\n",
      "-1.0681409609205272 900.113906721868\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.168377\n",
      "         Iterations: 36\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 118\n",
      "[-0.761 -0.553 -0.336  0.265], 239.16837709531046\n",
      "86, btag_cfragmentation, 0.000, 1.000, 0.001, 1.000, 833.137\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.353528\n",
      "         Iterations: 37\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 114\n",
      "[-0.769 -0.572 -0.315  0.266], 239.3535281683439\n",
      "-0.9989781229193976 834.3164944632919\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.366332\n",
      "         Iterations: 37\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[-0.769 -0.569 -0.285  0.261], 239.3663317698968\n",
      "87, btag_dmux, 0.000, 1.000, 0.020, 0.998, 833.137\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.381681\n",
      "         Iterations: 37\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 121\n",
      "[-0.742 -0.55  -0.236  0.246], 239.38168088205626\n",
      "-0.9778596254376152 868.2816314542997\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.340535\n",
      "         Iterations: 37\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 115\n",
      "[-0.768 -0.593 -0.322  0.271], 239.34053534563287\n",
      "88, btag_gluonsplitting, 0.000, 1.000, 0.225, 0.958, 832.428\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.467971\n",
      "         Iterations: 38\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.735 -0.559 -0.237  0.246], 239.46797089424385\n",
      "-0.7332863107616463 996.4916943291194\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.245265\n",
      "         Iterations: 37\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 144\n",
      "[-0.759 -0.551 -0.33   0.264], 239.2452651679979\n",
      "89, btag_jes, 0.000, 1.000, 0.024, 0.990, 786.986\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.483538\n",
      "         Iterations: 37\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[-0.753 -0.553 -0.284  0.256], 239.48353754626424\n",
      "-0.9667352925497975 876.7201000235698\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.214611\n",
      "         Iterations: 36\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 102\n",
      "[-0.77  -0.571 -0.336  0.27 ], 239.21461092165876\n",
      "90, btag_jetaway, 0.000, 1.000, 0.165, 0.963, 784.906\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.484629\n",
      "         Iterations: 38\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.727 -0.541 -0.341  0.259], 239.484629132861\n",
      "-0.7972695650966904 945.7025725079602\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.226106\n",
      "         Iterations: 37\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 126\n",
      "[-0.754 -0.554 -0.321  0.262], 239.22610615251855\n",
      "91, btag_ksl, 0.000, 1.000, 0.002, 1.000, 754.738\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.346851\n",
      "         Iterations: 37\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.771 -0.572 -0.319  0.267], 239.34685121556268\n",
      "-0.9981055739465589 756.6047082756422\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.366901\n",
      "         Iterations: 37\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 133\n",
      "[-0.768 -0.567 -0.285  0.261], 239.36690111537135\n",
      "92, btag_l2c, 0.000, 1.000, -0.080, 0.996, 754.736\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.502927\n",
      "         Iterations: 37\n",
      "         Function evaluations: 146\n",
      "         Gradient evaluations: 134\n",
      "[-0.758 -0.556 -0.284  0.257], 239.50292722106826\n",
      "-1.0758881826128175 799.9098010265793\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.235901\n",
      "         Iterations: 41\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.753 -0.579 -0.362  0.273], 239.23590120764243\n",
      "93, btag_ltothers, 0.000, 1.000, 0.625, 0.874, 757.956\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.224757\n",
      "         Iterations: 41\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-0.732 -0.544 -0.229  0.242], 239.22475742046544\n",
      "-0.2484425669545477 844.6240353666468\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.474324\n",
      "         Iterations: 40\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[-0.756 -0.568 -0.425  0.281], 239.47432399022685\n",
      "94, btag_mudr, 0.000, 1.000, -0.071, 0.995, 575.884\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.520864\n",
      "         Iterations: 40\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 79\n",
      "[-0.752 -0.561 -0.306  0.26 ], 239.5208638181774\n",
      "-1.0665547335260372 616.1023821683517\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.233395\n",
      "         Iterations: 37\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 108\n",
      "[-0.717 -0.539 -0.212  0.236], 239.23339483347482\n",
      "95, btag_mupt, 0.000, 1.000, 0.090, 0.995, 578.447\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.352040\n",
      "         Iterations: 37\n",
      "         Function evaluations: 143\n",
      "         Gradient evaluations: 132\n",
      "[-0.742 -0.534 -0.229  0.242], 239.35203978521434\n",
      "-0.9046351699975527 622.6129282547362\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.326492\n",
      "         Iterations: 37\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.736 -0.53  -0.319  0.255], 239.3264921790218\n",
      "96, btag_ptrel, 0.000, 1.000, 0.001, 0.997, 574.281\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.412579\n",
      "         Iterations: 37\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 110\n",
      "[-0.766 -0.566 -0.319  0.266], 239.4125785600933\n",
      "-0.9964119675175626 601.2831985434533\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.309471\n",
      "         Iterations: 37\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 142\n",
      "[-0.775 -0.59  -0.3    0.268], 239.30947050849574\n",
      "97, btag_sampledependence, 0.000, 1.000, 0.795, 0.780, 574.254\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.245243\n",
      "         Iterations: 43\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 90\n",
      "[-0.74  -0.55  -0.19   0.238], 239.24524316144215\n",
      "0.01488156807541563 569.5476803533682\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.507718\n",
      "         Iterations: 41\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.741 -0.537 -0.39   0.268], 239.5077180907771\n",
      "98, btag_pileup, 0.000, 1.000, -0.061, 0.994, 387.574\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.519799\n",
      "         Iterations: 37\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.751 -0.559 -0.278  0.255], 239.51979868532106\n",
      "-1.0553507644877231 413.2364638586801\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.213852\n",
      "         Iterations: 37\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.752 -0.531 -0.309  0.256], 239.21385213127374\n",
      "99, btag_statistic, 0.000, 1.000, 0.325, 0.944, 388.908\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.383828\n",
      "         Iterations: 46\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[-0.739 -0.553 -0.208  0.241], 239.3838279936608\n",
      "-0.6194488651140445 450.42032238538627\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.331418\n",
      "         Iterations: 41\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-0.774 -0.577 -0.298  0.265], 239.33141814993508\n",
      "100, ctag, 0.000, 1.000, 0.166, 0.804, 365.538\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.116676\n",
      "         Iterations: 37\n",
      "         Function evaluations: 148\n",
      "         Gradient evaluations: 136\n",
      "[-0.614 -0.398 -0.243  0.202], 239.1166760173601\n",
      "-0.6383299096746398 387.6524775087809\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.656379\n",
      "         Iterations: 37\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 71\n",
      "[-1.019 -0.811 -0.437  0.365], 239.6563794156088\n",
      "101, mistag, 0.000, 1.000, 0.218, 0.913, 360.563\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.118262\n",
      "         Iterations: 36\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[-0.666 -0.485 -0.272  0.229], 239.11826150780223\n",
      "-0.6946585298303871 379.57379025835905\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.606609\n",
      "         Iterations: 37\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 137\n",
      "[-0.836 -0.604 -0.289  0.278], 239.6066091887504\n",
      "102, jer, 0.000, 1.000, -0.280, 0.711, 355.187\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.141301\n",
      "         Iterations: 36\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[-0.923 -0.7   -0.205  0.294], 239.14130137068298\n",
      "-0.9908190560977166 376.74031101392956\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.590625\n",
      "         Iterations: 38\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 110\n",
      "[-0.546 -0.371 -0.4    0.212], 239.59062461322358\n",
      "103, jes_subtotal_pileup, 0.000, 1.000, 0.017, 0.877, 359.526\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.549546\n",
      "         Iterations: 59\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 121\n",
      "[-0.732 -0.537 -0.199  0.236], 239.54954602134455\n",
      "-0.8604091750246097 758.3321708189205\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.193839\n",
      "         Iterations: 52\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[-0.792 -0.599 -0.317  0.275], 239.19383854860047\n",
      "104, jes_subtotal_relative, 0.000, 1.000, 0.094, 0.890, 355.162\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.450885\n",
      "         Iterations: 54\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 92\n",
      "[-0.727 -0.519 -0.191  0.231], 239.4508847669679\n",
      "-0.7963668668849384 697.878844122602\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.272037\n",
      "         Iterations: 56\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 124\n",
      "[-0.794 -0.617 -0.35   0.283], 239.2720373698902\n",
      "105, jes_subtotal_pt, 0.000, 1.000, 0.143, 0.972, 333.455\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.318993\n",
      "         Iterations: 50\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 124\n",
      "[-0.736 -0.545 -0.22   0.241], 239.31899282084572\n",
      "-0.8293361433277344 448.43431830644727\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.395896\n",
      "         Iterations: 53\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 123\n",
      "[-0.796 -0.607 -0.34   0.28 ], 239.39589589572117\n",
      "106, jes_subtotal_scale, 0.000, 1.000, 0.159, 0.982, 320.371\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.292540\n",
      "         Iterations: 46\n",
      "         Function evaluations: 143\n",
      "         Gradient evaluations: 131\n",
      "[-0.743 -0.55  -0.205  0.241], 239.29254010971363\n",
      "-0.8227493628079063 395.5165801116091\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.424745\n",
      "         Iterations: 44\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 107\n",
      "[-0.802 -0.613 -0.359  0.285], 239.42474462141575\n",
      "107, jes_subtotal_absolute, 0.000, 1.000, 0.141, 0.959, 310.511\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.350882\n",
      "         Iterations: 49\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 91\n",
      "[-0.705 -0.514 -0.228  0.233], 239.3508823217788\n",
      "-0.8178182521283794 439.07212026898304\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.355501\n",
      "         Iterations: 45\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[-0.815 -0.619 -0.313  0.281], 239.35550141989395\n",
      "108, jes_flavor_qcd, 0.000, 1.000, 0.047, 0.790, 298.923\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.661915\n",
      "         Iterations: 55\n",
      "         Function evaluations: 163\n",
      "         Gradient evaluations: 151\n",
      "[-0.587 -0.384  0.022  0.152], 239.66191469910487\n",
      "-0.7427914977559895 663.169440776076\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 239.119111\n",
      "         Iterations: 56\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 125\n",
      "[-0.861 -0.673 -0.461  0.321], 239.1191106156953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n-1 systematics\n",
    "\n",
    "# initialize parameter data\n",
    "fit_data._pmask = parameters['active'].values.astype(bool)\n",
    "fit_data._pval_init = parameters['val_fit'].values.copy()\n",
    "\n",
    "impact_data = dict(errs = [], impacts_up = [], impacts_down = [])\n",
    "iparam = 4\n",
    "mask = fit_data._pmask\n",
    "#mask[17:] = False\n",
    "p_init = fit_data._pval_fit\n",
    "p_fit = parameters.val_fit.values\n",
    "for pname, pdata in tqdm(parameters.iloc[4:].iterrows(), total=parameters.active.sum() - 4):\n",
    "    if not pdata.active:\n",
    "        iparam += 1\n",
    "        continue\n",
    "        \n",
    "    tqdm.write(f'{iparam}, {pname}, {pdata.val_init:.3f}, {pdata.err_init:.3f}, {pdata.val_fit:.3f}, {pdata.err_fit:.3f}, {fobj(p_init[mask]):.3f}')\n",
    "\n",
    "    # fix parameter in fit and covariance calculation\n",
    "    mask[iparam] = False\n",
    "    #min_options['finite_diff_rel_step'] = steps[mask]\n",
    "    #err, _ = fh.calculate_covariance(fobj, p_init[mask])\n",
    "    err = 0\n",
    "    \n",
    "    # calculate impacts from up/down variations of n.p. on p.o.i.\n",
    "    p_init[iparam] = pdata.val_fit + pdata.err_fit\n",
    "    res = minimize(fobj, p_fit[mask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "    impact_up = res.x\n",
    "    tqdm.write(f'{(res.x[:4]-p_fit[:4])*100/p_fit[:4]}, {res.fun}')\n",
    "                    \n",
    "    p_init[iparam] = pdata.val_fit - pdata.err_fit\n",
    "    print(p_init[iparam], fobj(p_init[mask]))\n",
    "    res = minimize(fobj, p_fit[mask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "    impact_down = res.x\n",
    "    tqdm.write(f'{(res.x[:4]-p_fit[:4])*100/p_fit[:4]}, {res.fun}')\n",
    "    \n",
    "    impact_data['errs'].append(err)\n",
    "    impact_data['impacts_up'].append(impact_up)\n",
    "    impact_data['impacts_down'].append(impact_down)\n",
    "    \n",
    "    p_init[iparam] = pdata.val_fit\n",
    "    mask[iparam] = True\n",
    "    \n",
    "    iparam += 1\n",
    "    \n",
    "#impact_data = pd.DataFrame(impact_data, index=parameters.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.063350Z",
     "start_time": "2020-03-04T17:25:09.549Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-4008954e212d>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-4008954e212d>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    index=list(p_labels)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# convert impacts to dataframes \n",
    "impacts_np = np.hstack([100*(impacts[:,0,:4] - p_fit[:4])/p_fit[:4], 100*(impacts[:,1,:4] - p_fit[:4])/p_fit[:4]])\n",
    "\n",
    "#impacts_up = pd.DataFrame(impacts_np, columns=['beta_e_up', 'beta_mu_up', 'beta_tau_up', 'beta_h_up'], index=list(p_labels_fancy[4:]))\n",
    "p_labels = parameters.query('active == 1').index.values[4:]\n",
    "column_labels = [\n",
    "    'beta_e_up', 'beta_mu_up', 'beta_tau_up', 'beta_h_up', \n",
    "    'beta_e_down', 'beta_mu_down', 'beta_tau_down', 'beta_h_down'\n",
    "   ] \n",
    "impacts_np = pd.DataFrame(impacts_np, \n",
    "                          columns=column_labels\n",
    "                          index=list(p_labels)\n",
    "                         )\n",
    "impacts_np.to_csv('local_data/impacts_asimov.csv')\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "pd.set_option('display.max_columns', parameters.shape[0])\n",
    "pd.set_option('display.max_rows', parameters.shape[0])\n",
    "\n",
    "var = 'beta_tau'\n",
    "sorted_ind = impacts_np.abs().sort_values(by=f'{var}_down', ascending=False).index\n",
    "impacts_np.reindex(sorted_ind)[['beta_tau_up', 'beta_tau_down']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.064422Z",
     "start_time": "2020-03-04T17:25:09.557Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert to errors\n",
    "err_no_bb = parameters['err_no_bb'].values[mask]\n",
    "errs_np = np.array([np.concatenate([e[:i+4], [0], e[i+4:]]) for i, e in enumerate(errs)])\n",
    "errs_np = err_no_bb**2 - errs_np**2\n",
    "#errs_np[errs_np < 0] = 0\n",
    "#errs_np = np.sqrt(errs_np)\n",
    "##errs_np = np.vstack([errs_np, err_stat, err_mc_stat, err_syst, stderr])\n",
    "#\n",
    "errs_np = pd.DataFrame(errs_np[:,:4], columns=['beta_e', 'beta_mu', 'beta_tau', 'beta_h'], index=list(p_labels))\n",
    "##errs_np = pd.DataFrame(errs_np[:,:4], columns=['beta_e', 'beta_mu', 'beta_tau', 'beta_h'], index=list(p_labels_fancy[4:]) + ['stat.', 'MC stat.', 'syst. total', 'total'])\n",
    "##beta_stderr = stderr.iloc[:,:4].multiply(100)/params_init[:4]\n",
    "100*errs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.065367Z",
     "start_time": "2020-03-04T17:25:09.560Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# print table\n",
    "\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1).to_latex('local_data/errors.tex')\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1).to_csv('local_data/errors.csv')\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1)\n",
    "beta_errs.to_latex('local_data/summary_errors.tex', escape=False)\n",
    "beta_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.066536Z",
     "start_time": "2020-03-04T17:25:09.563Z"
    }
   },
   "outputs": [],
   "source": [
    "jes_mask = np.array([True if ('jes' in pname and 'btag' not in pname) else False for pname in beta_errs.index])\n",
    "btag_mask = np.array([True if 'btag' in pname else False for pname in beta_errs.index])\n",
    "tau_misid_mask = np.array([True if ('misid_tau' in pname and pname not in ['misid_tau_e', 'misid_tau_h']) else False for pname in beta_errs.index])\n",
    "\n",
    "btag_errs = beta_errs[btag_mask]\n",
    "jes_errs = beta_errs[jes_mask]\n",
    "tau_misid_errs = beta_errs[tau_misid_mask]\n",
    "\n",
    "summary_errs = beta_errs[~btag_mask&~jes_mask&~tau_misid_mask].copy()\n",
    "summary_errs.index = [fit_data._parameters.loc[p].label if p in fit_data._parameters.index else p for p in summary_errs.index]\n",
    "summary_errs.loc['b-tag',:] = np.sqrt(np.sum(btag_errs**2))\n",
    "summary_errs.loc['JES',:]  = np.sqrt(np.sum(jes_errs**2))\n",
    "summary_errs.loc[r'$\\sf jet\\rightarrow\\tau$',:]  = np.sqrt(np.sum(tau_misid_errs**2))\n",
    "\n",
    "summary_errs = summary_errs.divide(params_init[:4]/100, axis=1)\n",
    "summary_errs.to_latex('local_data/summary_errors.tex', escape=False)\n",
    "summary_errs.to_csv('local_data/summary_errors.csv')\n",
    "summary_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "29px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "49px",
    "left": "0px",
    "right": "1493.87px",
    "top": "90.9965px",
    "width": "242px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 744.85,
   "position": {
    "height": "40px",
    "left": "919px",
    "right": "20px",
    "top": "59px",
    "width": "678px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  },
  "widgets": {
   "state": {
    "012f8bbe2fdb410dae6e2cde9d7fe5cb": {
     "views": []
    },
    "080556076f174648bddf64f17a54c523": {
     "views": []
    },
    "0ad83b5f67484ae5b8fd8dd43ccc39bd": {
     "views": []
    },
    "15acd81a9adc493683d9b63813f000bf": {
     "views": []
    },
    "1840cb6fded848b4ae95ec8d3db15ab2": {
     "views": []
    },
    "1dd83f822e074642ae4255b15ee661cf": {
     "views": []
    },
    "1e71a878e6474912a0efc497ecc5d65b": {
     "views": []
    },
    "2022ed83777b4963b630b5c46239e218": {
     "views": []
    },
    "21c4c57bfc48495194663e6a4fbac488": {
     "views": []
    },
    "22c45c75435348c0b9501d493d69fdca": {
     "views": []
    },
    "2635f668a1af4a9db2642e705d7c73ff": {
     "views": []
    },
    "2ad5ddd9347e451b9290e5b4179ab9a2": {
     "views": []
    },
    "2c062b5778024117984822b63b0593d7": {
     "views": []
    },
    "2f5eab2f6fb24192b76a5ffe99195d44": {
     "views": []
    },
    "31632517325046e8b0cb62e4f4ed2480": {
     "views": []
    },
    "3562b97192ed4d42bbab17f77c290f6b": {
     "views": []
    },
    "38a7cc053723492b921cf9f084ed243c": {
     "views": []
    },
    "3b5750b20e1745879ca0f965aad7b614": {
     "views": []
    },
    "3f1cbabbe2694a9dabe3f1c2e09d0ee2": {
     "views": []
    },
    "3fab6a26a70c4238a668a46d4dc88bf6": {
     "views": []
    },
    "3fb3c7a25e954a4888996976fa107737": {
     "views": []
    },
    "415db64fbc574daea8457ab600392f09": {
     "views": []
    },
    "4463de406b4645a4b562fe7917380ff9": {
     "views": []
    },
    "487e5450b5a24507932709f1fa8f59c1": {
     "views": []
    },
    "48aba73013e74e71927f71d42fb44d14": {
     "views": []
    },
    "4a19ad30f77e4fe6a2c84c8b62378a47": {
     "views": []
    },
    "4bc83ff5270d41679d76d26cdded8313": {
     "views": []
    },
    "4bdd9dd5c5c64646a27fa9096851458b": {
     "views": []
    },
    "5014cd42705f45178d5e6eeffd70f119": {
     "views": []
    },
    "5259b340b68e4fdb97fb4eaf9d98d954": {
     "views": []
    },
    "5986ef5b605a42aca10bc5834529ee06": {
     "views": []
    },
    "5c1aa44589a140eb9709734c843abde6": {
     "views": []
    },
    "600cd9ca4f4c46d4ad6fe57df107675a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "61fe369ebdd14eaa89de110f6186e6b7": {
     "views": []
    },
    "62ac836017ae47a38f8fde806c5ec9b7": {
     "views": []
    },
    "632ccbfabe91405aa1c5a77c9ea754db": {
     "views": []
    },
    "66175f618ea5472baac618f998d2c06c": {
     "views": []
    },
    "6647a620af034d26abcd327ae02364d4": {
     "views": []
    },
    "6752222d2cba43e18f344a8db7f99d24": {
     "views": []
    },
    "6b684ba1a7c24a35ba2df77016212904": {
     "views": []
    },
    "6cea898f4aca4f1e84601f843e337238": {
     "views": []
    },
    "6f8d1e87fd60462a89d693b2f3b5f007": {
     "views": []
    },
    "74078646a5eb4047b40370a0ab8b6b30": {
     "views": []
    },
    "745b0c79ff3040788ea952fce9c7d607": {
     "views": []
    },
    "757c9b805eb7445bac9a7f141f87e45f": {
     "views": []
    },
    "76ced68e19a742e8976dbfd4e8594a1a": {
     "views": []
    },
    "783bb5e7538d4d9d8315e2698024b353": {
     "views": []
    },
    "794993d66efe4ab29a8d35aad8cfe079": {
     "views": []
    },
    "8375e24bae7541528d7cdc0f379d1d4c": {
     "views": []
    },
    "8554945ec15041a7bf8004dbc3fc5f11": {
     "views": []
    },
    "878a34e26cce4f18bb8232a682ebe964": {
     "views": []
    },
    "8921a75116a549198eb7b7f4a24ab672": {
     "views": []
    },
    "909f4504f0b049bda8b641defa177062": {
     "views": []
    },
    "910b9d32a3fb45ec99da1f9df1add816": {
     "views": []
    },
    "9d15ce601cd34f0699b7a7a0ce1d17dc": {
     "views": []
    },
    "a26638c9fee247b3891aac027a0918cc": {
     "views": []
    },
    "a9d2bf44a3ad447bb3eecde71363c198": {
     "views": []
    },
    "ad366bf4c95f4cdba62d47ba9501efc9": {
     "views": []
    },
    "ad8e1842ec314a94b6ed4b62c4c0a450": {
     "views": []
    },
    "af525094db304d2a812ae1312b00889b": {
     "views": []
    },
    "b0697c4343da491f9a35bf02681dad8f": {
     "views": []
    },
    "b07ff307919e4268bc8bec8379c47a5d": {
     "views": []
    },
    "b0e85c726ca141079333afb27edc63d4": {
     "views": []
    },
    "bdcc1e5df7a8432b9f40d8249a46f90a": {
     "views": []
    },
    "be1065f37fa24e818d31c3bb075947a3": {
     "views": []
    },
    "c296c8df2f734e268c6c1204536e7142": {
     "views": []
    },
    "c4bfd3e447f0426da144b76abc202129": {
     "views": []
    },
    "cced93184d4445218a2b14567579333d": {
     "views": []
    },
    "d5bd2e4d5f85482e9345f3a7a69380d0": {
     "views": []
    },
    "d798fa64e8be4a7d9ec1cbeece3b1be9": {
     "views": []
    },
    "d7aec0d6d05f442b991ab40af944811d": {
     "views": []
    },
    "db469cea2c8e4180bf6890de80329c1d": {
     "views": []
    },
    "e671857510c54634b6f0fa55bf1fa228": {
     "views": []
    },
    "ebf52deafaf64b0c826533dafdf993c0": {
     "views": []
    },
    "ed1e5439da9c41199a7bbbda21b556f8": {
     "views": []
    },
    "f585cf5db5024280af5b567f0e4fd771": {
     "views": []
    },
    "f6ba8f8800af47adabed847063bda8db": {
     "views": []
    },
    "f9bae72f14e44705b5c38a3ddc69fee8": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
