{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating individual systematic contributions\n",
    "\n",
    "The effect of any individual systematic uncertainty is somewhat complicated by it's mutual covariance with the POI (i.e., the W branching fractions) and any other systematic uncertainty.  To get a rough idea of the percent-wise contribution to the total uncertainty from each individual systematic, I use the following scheme:\n",
    "\n",
    "   * the fit is carried out as in the nominal case and $\\sigma_{0}$ is estimated\n",
    "   * the fit is carried out for for each of the $n$ nuisance parameters $\\sigma_{theta}$\n",
    "   * the difference between the nominal case and the $n-1$ case is calculated,\n",
    "   * this quantity is normalized to $\\sum_{\\theta} \\sigma_{\\theta}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:09.639485Z",
     "start_time": "2020-03-04T17:25:09.500321Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/naodell/work/wbr/analysis\n",
      "{\n",
      "  \"shell_port\": 38009,\n",
      "  \"iopub_port\": 34267,\n",
      "  \"stdin_port\": 60623,\n",
      "  \"control_port\": 60263,\n",
      "  \"hb_port\": 43913,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"a60ae49e-aee33fe4b640f66032f39fd1\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-1e88735e-53e4-45fc-bccb-0eeb75431084.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "## imports and configuration\n",
    "%cd '/home/naodell/work/wbr/analysis'\n",
    "#%load_ext autoreload\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.optimize import minimize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scripts.plot_tools as pt\n",
    "import scripts.fit_helpers as fh\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "rc_params = {\n",
    "             'figure.figsize': (10, 10),\n",
    "             'axes.labelsize': 20,\n",
    "             'axes.facecolor': 'white',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'legend.fontsize': 20,\n",
    "             'xtick.labelsize':18,\n",
    "             'ytick.labelsize':18,\n",
    "             'font.size':18,\n",
    "             'font.sans-serif':['Arial', 'sans-serif'],\n",
    "             'mathtext.sf':'Arial',\n",
    "             'lines.markersize':8.,\n",
    "             'lines.linewidth':2.5,\n",
    "            }\n",
    "matplotlib.rcParams.update(rc_params)\n",
    "\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.006785Z",
     "start_time": "2020-03-04T17:25:09.640699Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 264.651047\n",
      "         Iterations: 61\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 118\n",
      "[10.773 10.911 10.894 67.422] [10.773 10.911 10.894 67.422]\n"
     ]
    }
   ],
   "source": [
    "# configure, get the input data, and do any additional processing that is needed\n",
    "# initialize fit data\n",
    "scenario = 'unblinded'\n",
    "infile = open(f'local_data/fit_data_{scenario}.pkl', 'rb')\n",
    "fit_data = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# get fit parameters\n",
    "parameters = fit_data._parameters.copy() \n",
    "\n",
    "# fit configuration #\n",
    "# minimizer options\n",
    "#steps = 4*[1e-4, ] \n",
    "#steps += list(0.05*fit_data._perr_init[4: fit_data._nnorm + 7])\n",
    "#steps += list(0.05*fit_data._perr_init[fit_data._npoi + fit_data._nnorm:])\n",
    "#steps = np.array(steps)\n",
    "\n",
    "min_options = dict(\n",
    "                   #finite_diff_rel_step=steps,\n",
    "                   #verbose=3,\n",
    "                   #eps=1e-9, \n",
    "                   gtol = 1e-3,\n",
    "                   disp = True\n",
    "                  )\n",
    "\n",
    "# configure the objective\n",
    "sample = None\n",
    "fobj = partial(fit_data.objective,\n",
    "               data = sample,\n",
    "               do_bb_lite = True,\n",
    "               lu_test = 2\n",
    "              )\n",
    "\n",
    "fobj_jac = partial(fit_data.objective_jacobian,\n",
    "                   data = sample,\n",
    "                   do_bb_lite = True,\n",
    "                   lu_test = 2\n",
    "                  )\n",
    "\n",
    "# test fit, should be the same as parameters.val_fit\n",
    "res_mle = minimize(fobj, parameters.val_init.values[fit_data._pmask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "print(res_mle.x[:4]*100, parameters.val_fit[:4].values*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.062418Z",
     "start_time": "2020-03-04T17:25:10.053753Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c512f1f226a4f82be773b89fc96dfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=103.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, br_tau_e, 0.177, 0.000, 0.177, 0.000, 1407.219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.152966\n",
      "         Iterations: 3\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 34\n",
      "[-0.003 -0.     0.001  0.   ], 265.1529659939239\n",
      "0.1769928611114198 1403.784745010146\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.159018\n",
      "         Iterations: 5\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 61\n",
      "[ 0.002  0.004  0.001 -0.001], 265.15901785414474\n",
      "5, br_tau_mu, 0.173, 0.000, 0.173, 0.000, 1407.219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.156623\n",
      "         Iterations: 4\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 84\n",
      "[-0.002 -0.004 -0.004  0.002], 265.1566229903374\n",
      "0.17268021663996716 1402.740134477534\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.161116\n",
      "         Iterations: 8\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[-0.     0.002  0.021 -0.004], 265.16111624217734\n",
      "6, br_tau_h, 0.650, 0.001, 0.649, 0.001, 1407.219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.174935\n",
      "         Iterations: 4\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 51\n",
      "[ 0.001  0.    -0.022  0.003], 265.1749349961174\n",
      "0.6484727919502018 1401.475601253068\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.164123\n",
      "         Iterations: 8\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 84\n",
      "[-0.015  0.004  0.038 -0.004], 265.1641230803967\n",
      "7, lumi, 1.000, 0.025, 1.010, 0.018, 1407.219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.085738\n",
      "         Iterations: 45\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 112\n",
      "[-0.023 -0.102  0.17  -0.007], 265.0857381490014\n",
      "0.9920583301395017 729.4123910224648\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.193089\n",
      "         Iterations: 49\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 121\n",
      "[-0.003  0.008 -0.217  0.034], 265.19308870519166\n",
      "8, xs_gjets, 1.000, 0.100, 1.085, 0.095, 1407.219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.556373\n",
      "         Iterations: 15\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 95\n",
      "[ 0.006 -0.084 -0.127  0.033], 265.55637282356\n",
      "0.9893054374817674 1375.7747863029458\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.179117\n",
      "         Iterations: 37\n",
      "         Function evaluations: 152\n",
      "         Gradient evaluations: 140\n",
      "[ 0.005  0.01   0.017 -0.005], 265.1791168016081\n",
      "9, xs_diboson, 1.000, 0.100, 1.016, 0.099, 1407.219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.182924\n",
      "         Iterations: 11\n",
      "         Function evaluations: 55\n",
      "         Gradient evaluations: 44\n",
      "[ 0.002  0.004 -0.011  0.001], 265.18292357072033\n",
      "0.917220810411818 1397.305023217783\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.231666\n",
      "         Iterations: 10\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 75\n",
      "[ 0.004  0.015  0.014 -0.005], 265.2316655872751\n",
      "10, xs_ww, 1.000, 0.100, 0.965, 0.028, 1407.219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.166792\n",
      "         Iterations: 35\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 130\n",
      "[ 0.009 -0.058 -0.283  0.054], 265.166791664675\n",
      "0.9369882666790111 1377.826477397789\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.140097\n",
      "         Iterations: 29\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.032 -0.002  0.096 -0.01 ], 265.14009743042595\n",
      "11, xs_t, 1.000, 0.100, 0.932, 0.072, 1389.237\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.203031\n",
      "         Iterations: 40\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 108\n",
      "[ 0.238  0.273  0.468 -0.158], 265.2030311658343\n",
      "0.8593512081312181 1041.6950799676663\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.097376\n",
      "         Iterations: 41\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 92\n",
      "[-0.268 -0.336 -0.53   0.183], 265.0973764323808\n",
      "13, e_fakes, 1.000, 1.000, 1.064, 0.096, 1208.706\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.162756\n",
      "         Iterations: 38\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[ 0.053  0.095 -0.113 -0.006], 265.1627555375764\n",
      "0.9681717565505223 1184.8851289447118\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.177613\n",
      "         Iterations: 36\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[-0.062 -0.122  0.04   0.023], 265.17761330248265\n",
      "14, mu_fakes, 1.000, 1.000, 0.868, 0.037, 1260.666\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.213953\n",
      "         Iterations: 30\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.045  0.097 -0.203  0.01 ], 265.21395338431125\n",
      "0.8306860875623046 1163.2572331281754\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.212170\n",
      "         Iterations: 29\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.075 -0.16   0.17   0.01 ], 265.21216970959523\n",
      "15, e_fakes_ss, 1.000, 0.300, 0.995, 0.035, 1177.837\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.216480\n",
      "         Iterations: 37\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 96\n",
      "[ 0.051 -0.07  -0.134  0.025], 265.2164804551515\n",
      "0.9603234300085188 1169.194386305317\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.082062\n",
      "         Iterations: 35\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 93\n",
      "[-0.076  0.005  0.139 -0.011], 265.0820615821295\n",
      "16, mu_fakes_ss, 1.000, 0.300, 1.126, 0.052, 1176.559\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.184469\n",
      "         Iterations: 27\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[-0.147  0.003 -0.376  0.084], 265.1844688211933\n",
      "1.074280621845549 1195.7897035427418\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.156020\n",
      "         Iterations: 28\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[ 0.151 -0.014  0.253 -0.063], 265.1560201285303\n",
      "17, trigger_mu, 1.000, 0.005, 1.000, 0.004, 1215.346\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.220758\n",
      "         Iterations: 37\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[ 0.108 -0.217 -0.035  0.024], 265.2207576599288\n",
      "0.9959215207142859 1117.3800569472132\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.088790\n",
      "         Iterations: 39\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 122\n",
      "[-0.131  0.124 -0.039  0.007], 265.08879037356877\n",
      "18, xs_zjets_alt_pdf, 0.000, 1.000, 0.130, 0.968, 1221.356\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.149667\n",
      "         Iterations: 28\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 133\n",
      "[ 0.034  0.007  0.059 -0.016], 265.1496665655943\n",
      "-0.8376267619374022 1197.5899882847584\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.157450\n",
      "         Iterations: 31\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 114\n",
      "[-0.002 -0.025 -0.11   0.022], 265.1574497592928\n",
      "19, xs_zjets_alt_alpha_s, 0.000, 1.000, -0.320, 0.904, 1225.448\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.130304\n",
      "         Iterations: 36\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 78\n",
      "[-0.015 -0.018 -0.067  0.016], 265.130304069729\n",
      "-1.223800424525765 1299.4988784959128\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.182335\n",
      "         Iterations: 37\n",
      "         Function evaluations: 141\n",
      "         Gradient evaluations: 129\n",
      "[-0.053 -0.112 -0.185  0.057], 265.18233508100224\n",
      "20, xs_zjets_alt_qcd_scale_0, 0.000, 1.000, -0.828, 0.474, 1243.171\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.158586\n",
      "         Iterations: 45\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 136\n",
      "[ 0.179  0.069  0.299 -0.088], 265.1585864837335\n",
      "-1.3020005646344883 1229.991979247592\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.055015\n",
      "         Iterations: 37\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 139\n",
      "[-0.205 -0.157 -0.378  0.119], 265.0550146818079\n",
      "21, xs_zjets_alt_qcd_scale_1, 0.000, 1.000, -0.869, 0.458, 1218.747\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.128150\n",
      "         Iterations: 30\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.043 -0.034 -0.133  0.02 ], 265.1281495749591\n",
      "-1.3269557844135471 1194.8822790663487\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.192264\n",
      "         Iterations: 29\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[-0.085 -0.063  0.007  0.023], 265.19226356385695\n",
      "22, xs_zjets_alt_qcd_scale_2, 0.000, 1.000, -0.241, 0.306, 1199.726\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.266753\n",
      "         Iterations: 25\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 60\n",
      "[-0.125 -0.08  -0.447  0.105], 265.26675254846776\n",
      "-0.547042396329479 1158.5255874285965\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.174924\n",
      "         Iterations: 32\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.176  0.142  0.265 -0.094], 265.1749237459397\n",
      "23, xs_wjets_qcd_scale_0, 0.000, 1.000, -0.438, 0.761, 1180.090\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.069244\n",
      "         Iterations: 39\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 117\n",
      "[-0.094 -0.111  0.389 -0.03 ], 265.06924385576\n",
      "-1.1981736358040183 1180.3295546622824\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.293080\n",
      "         Iterations: 28\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[ 0.158  0.127 -0.461  0.029], 265.2930804424016\n",
      "24, xs_wjets_qcd_scale_1, 0.000, 1.000, -0.314, 0.928, 1178.811\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.135969\n",
      "         Iterations: 28\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.024 -0.064 -0.062  0.024], 265.1359689988487\n",
      "-1.2415864360217816 1176.7316932338558\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.226757\n",
      "         Iterations: 18\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 82\n",
      "[ 0.037  0.03   0.054 -0.019], 265.2267567745455\n",
      "25, xs_wjets_qcd_scale_2, 0.000, 1.000, 1.379, 0.600, 1178.013\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.181373\n",
      "         Iterations: 28\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 132\n",
      "[-0.008  0.003  0.193 -0.03 ], 265.1813730279128\n",
      "0.7785218302476906 1187.3245434126181\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.184342\n",
      "         Iterations: 31\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 132\n",
      "[ 0.055  0.008 -0.366  0.049], 265.18434190753527\n",
      "26, xs_wjets_qcd_scale_3, 0.000, 1.000, -0.215, 0.953, 1198.403\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.174099\n",
      "         Iterations: 0\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 50\n",
      "[0. 0. 0. 0.], 265.17409939673365\n",
      "-1.1677142547692485 1197.1709628725694\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.145847\n",
      "         Iterations: 9\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[-0.006  0.002  0.01  -0.001], 265.1458474902918\n",
      "27, xs_wjets_qcd_scale_4, 0.000, 1.000, 1.846, 0.365, 1198.028\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.113170\n",
      "         Iterations: 36\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 84\n",
      "[ 0.007 -0.052 -0.266  0.05 ], 265.1131701300242\n",
      "1.4815919967744435 1670.0953952180207\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.195857\n",
      "         Iterations: 38\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[ 0.018  0.001  0.43  -0.073], 265.1958573468674\n",
      "28, xs_ttbar_pdf, 0.000, 1.000, -0.424, 1.003, 1830.049\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.234624\n",
      "         Iterations: 49\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[ 0.033 -0.001 -0.017 -0.002], 265.2346236448163\n",
      "-1.4276773121925956 1219.9166431132528\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.100844\n",
      "         Iterations: 50\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 140\n",
      "[-0.033 -0.066 -0.073  0.028], 265.1008444405627\n",
      "29, xs_ttbar_alpha_s, 0.000, 1.000, 0.383, 1.015, 1625.612\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.095597\n",
      "         Iterations: 50\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 88\n",
      "[-0.061 -0.075 -0.132  0.043], 265.0955967876875\n",
      "-0.6313320532041141 2047.2870669457263\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.237789\n",
      "         Iterations: 49\n",
      "         Function evaluations: 166\n",
      "         Gradient evaluations: 154\n",
      "[-0.014 -0.004  0.023 -0.001], 265.2377894977824\n",
      "30, xs_ttbar_qcd_scale, 0.000, 1.000, -0.475, 0.251, 1406.051\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.132592\n",
      "         Iterations: 62\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 136\n",
      "[-0.28  -0.337 -0.541  0.187], 265.13259203609925\n",
      "-0.7259681087359385 2579.7242509883076\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.185872\n",
      "         Iterations: 43\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[ 0.252  0.261  0.513 -0.165], 265.18587174348096\n",
      "31, eff_tau_0, 0.000, 1.000, -0.834, 0.489, 1027.147\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.169433\n",
      "         Iterations: 41\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 110\n",
      "[ 0.048  0.003 -0.361  0.05 ], 265.16943303844323\n",
      "-1.3236178205239215 1046.5413503739237\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.153869\n",
      "         Iterations: 40\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.064 -0.088  0.3   -0.024], 265.15386915127937\n",
      "32, eff_tau_1, 0.000, 1.000, -0.085, 0.361, 1034.414\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.137105\n",
      "         Iterations: 37\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[-0.049 -0.005 -0.411  0.075], 265.1371045404534\n",
      "-0.4465409969181274 1041.2975800754973\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.126736\n",
      "         Iterations: 33\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 118\n",
      "[ 0.046 -0.056  0.293 -0.046], 265.1267355533264\n",
      "33, eff_tau_2, 0.000, 1.000, -0.415, 0.274, 1035.317\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.238780\n",
      "         Iterations: 29\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 133\n",
      "[-0.124 -0.018 -0.76   0.145], 265.23877957807923\n",
      "-0.6893094203521879 1048.3901435435664\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.087769\n",
      "         Iterations: 34\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[ 0.034 -0.143  0.479 -0.06 ], 265.0877689907653\n",
      "34, eff_tau_3, 0.000, 1.000, -0.871, 0.366, 1040.080\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.230164\n",
      "         Iterations: 30\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 71\n",
      "[ 0.031  0.026 -0.531  0.077], 265.23016352357354\n",
      "-1.237330251320853 1054.094443438508\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.035221\n",
      "         Iterations: 35\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-0.048 -0.112  0.602 -0.072], 265.03522051497646\n",
      "35, eff_tau_4, 0.000, 1.000, -0.398, 0.423, 1045.703\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.491421\n",
      "         Iterations: 10\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.025  0.019 -0.132  0.014], 265.4914206622851\n",
      "-0.8213893274562429 1052.1573205115142\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.209015\n",
      "         Iterations: 40\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.075 -0.105  0.721 -0.087], 265.20901501525725\n",
      "36, eff_tau_5, 0.000, 1.000, -1.370, 0.388, 1047.515\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.261670\n",
      "         Iterations: 31\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[ 0.077  0.038 -0.853  0.119], 265.2616704788724\n",
      "-1.7575006795350954 1037.5697622119872\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.037505\n",
      "         Iterations: 34\n",
      "         Function evaluations: 79\n",
      "         Gradient evaluations: 68\n",
      "[-0.128 -0.15   0.828 -0.089], 265.03750542845484\n",
      "37, misid_tau_e, 0.000, 1.000, 0.127, 0.995, 1033.646\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.171717\n",
      "         Iterations: 4\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 71\n",
      "[ 0.004  0.    -0.022  0.003], 265.17171710187796\n",
      "-0.8685257579285816 1035.3810931894725\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.165460\n",
      "         Iterations: 8\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 90\n",
      "[-0.027 -0.006  0.076 -0.007], 265.1654595728832\n",
      "38, misid_tau_h, 0.000, 1.000, -0.345, 0.592, 1033.461\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.084361\n",
      "         Iterations: 39\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[ 0.103  0.036 -0.555  0.067], 265.08436081865176\n",
      "-0.9372696629636434 1079.3136781249486\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.217685\n",
      "         Iterations: 31\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.19  -0.134  0.313  0.001], 265.21768508100206\n",
      "39, misid_tau_0, 0.000, 1.000, 0.405, 0.602, 1046.958\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.205838\n",
      "         Iterations: 31\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 130\n",
      "[-0.07  -0.009  0.109 -0.005], 265.2058381855688\n",
      "-0.197073273314339 1051.1475687183718\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.145601\n",
      "         Iterations: 30\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 115\n",
      "[ 0.021 -0.044 -0.233  0.041], 265.14560052809384\n",
      "40, misid_tau_1, 0.000, 1.000, -0.107, 0.718, 1040.191\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.405188\n",
      "         Iterations: 18\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 60\n",
      "[-0.026 -0.012  0.013  0.004], 265.4051875560305\n",
      "-0.824507611709957 1048.2420082778956\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.131589\n",
      "         Iterations: 29\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[ 0.001 -0.029 -0.067  0.015], 265.1315894236955\n",
      "41, misid_tau_2, 0.000, 1.000, 0.043, 0.742, 1041.015\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.323622\n",
      "         Iterations: 16\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.021 -0.028 -0.19   0.039], 265.32362202156384\n",
      "-0.6987751799903655 1048.526823793792\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.150637\n",
      "         Iterations: 36\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 85\n",
      "[-0.091 -0.063  0.124  0.005], 265.1506366507173\n",
      "42, misid_tau_3, 0.000, 1.000, 0.063, 0.895, 1040.637\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.391981\n",
      "         Iterations: 6\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[ 0.018  0.008 -0.07   0.007], 265.3919810029265\n",
      "-0.8314427566653368 1044.6266782526843\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.305155\n",
      "         Iterations: 18\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 51\n",
      "[-0.046 -0.005  0.041  0.002], 265.30515466870406\n",
      "43, misid_tau_4, 0.000, 1.000, -0.277, 0.947, 1040.397\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.262977\n",
      "         Iterations: 5\n",
      "         Function evaluations: 50\n",
      "         Gradient evaluations: 38\n",
      "[ 0.009  0.005 -0.046  0.005], 265.26297696690847\n",
      "-1.2241392254886319 1044.166243125162\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.568559\n",
      "         Iterations: 9\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 57\n",
      "[-0.027 -0.01   0.058 -0.004], 265.56855903139854\n",
      "44, misid_tau_5, 0.000, 1.000, -0.346, 0.956, 1041.015\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.371310\n",
      "         Iterations: 5\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 34\n",
      "[ 0.007  0.006 -0.043  0.005], 265.37131022838366\n",
      "-1.3021771877901176 1043.3810204187753\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.249841\n",
      "         Iterations: 19\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 109\n",
      "[-0.035 -0.009 -0.03   0.012], 265.2498411277135\n",
      "45, escale_tau, 0.000, 1.000, 0.254, 0.242, 1041.356\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 264.683192\n",
      "         Iterations: 36\n",
      "         Function evaluations: 146\n",
      "         Gradient evaluations: 134\n",
      "[-0.091 -0.076  0.188 -0.004], 264.683191955861\n",
      "0.011969936106296625 1041.0205666094082\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.623811\n",
      "         Iterations: 31\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 78\n",
      "[ 0.082 -0.003 -0.33   0.041], 265.6238107043518\n",
      "46, eff_reco_e, 0.000, 1.000, 0.538, 0.930, 1035.394\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.081170\n",
      "         Iterations: 38\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-0.516  0.029 -0.145  0.101], 265.0811697967283\n",
      "-0.392568535837465 1065.3645667400247\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.183133\n",
      "         Iterations: 36\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[ 0.454 -0.118  0.117 -0.072], 265.1831332406903\n",
      "48, eff_e_0, 0.000, 1.000, -1.689, 0.723, 1001.417\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.198449\n",
      "         Iterations: 34\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 132\n",
      "[-0.06  -0.028 -0.012  0.016], 265.1984491559287\n",
      "-2.4122300066661673 1048.190220388693\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.151092\n",
      "         Iterations: 28\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[ 0.061 -0.028 -0.199  0.027], 265.151092367525\n",
      "49, eff_e_1, 0.000, 1.000, 0.483, 0.901, 1031.458\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.329211\n",
      "         Iterations: 3\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 50\n",
      "[-0.008 -0.007  0.006  0.001], 265.32921050005285\n",
      "-0.41755720317076295 1035.682427967739\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.335109\n",
      "         Iterations: 8\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[-0.009  0.013 -0.006  0.   ], 265.33510900421754\n",
      "50, eff_e_2, 0.000, 1.000, 1.034, 0.832, 1026.962\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.350782\n",
      "         Iterations: 3\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 74\n",
      "[-0.008 -0.007  0.007  0.001], 265.35078161469625\n",
      "0.20117283452796753 1024.9409485305735\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.359623\n",
      "         Iterations: 8\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 96\n",
      "[ 0.006  0.017 -0.014 -0.002], 265.35962288150193\n",
      "51, eff_e_3, 0.000, 1.000, -1.232, 0.922, 1017.514\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.286125\n",
      "         Iterations: 3\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 71\n",
      "[-0.008 -0.006  0.006  0.001], 265.28612452553426\n",
      "-2.153987372687993 1030.6401779423654\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.268555\n",
      "         Iterations: 8\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 56\n",
      "[-0.015  0.009 -0.003  0.001], 265.26855500918043\n",
      "52, eff_e_4, 0.000, 1.000, -0.621, 0.463, 1024.136\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.165131\n",
      "         Iterations: 33\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 126\n",
      "[-0.202 -0.074  0.044  0.037], 265.1651308198323\n",
      "-1.083570563033379 1049.965756449021\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.200330\n",
      "         Iterations: 31\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[ 0.081 -0.098 -0.331  0.056], 265.2003304196451\n",
      "53, eff_e_5, 0.000, 1.000, -0.879, 0.428, 1037.210\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.198945\n",
      "         Iterations: 36\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[-0.122  0.021  0.191 -0.015], 265.1989445526646\n",
      "-1.3070882424092067 1093.6121682939179\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.117356\n",
      "         Iterations: 34\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 135\n",
      "[ 0.084 -0.068 -0.191  0.028], 265.1173561307222\n",
      "54, trigger_e_0, 0.000, 1.000, -1.028, 0.944, 1064.764\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.204816\n",
      "         Iterations: 27\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 120\n",
      "[ 0.007  0.038  0.065 -0.018], 265.2048157123529\n",
      "-1.9715277130358313 1083.9810802205059\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.157034\n",
      "         Iterations: 25\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 72\n",
      "[ 0.006 -0.051 -0.034  0.013], 265.15703425048787\n",
      "55, trigger_e_1, 0.000, 1.000, -0.534, 0.985, 1073.776\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.262510\n",
      "         Iterations: 5\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 54\n",
      "[-0.004  0.002  0.008 -0.001], 265.2625101839699\n",
      "-1.5192344833463838 1082.2498407828466\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.168112\n",
      "         Iterations: 20\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 70\n",
      "[-0.041 -0.044 -0.027  0.018], 265.16811195686574\n",
      "56, trigger_e_2, 0.000, 1.000, -0.384, 0.992, 1076.415\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.211123\n",
      "         Iterations: 5\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 82\n",
      "[ 0.001  0.006  0.008 -0.003], 265.21112281787214\n",
      "-1.3762061171002902 1082.227810084291\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.183312\n",
      "         Iterations: 9\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.024 -0.008  0.023  0.001], 265.1833117985364\n",
      "57, trigger_e_3, 0.000, 1.000, -0.421, 0.991, 1077.817\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.223498\n",
      "         Iterations: 5\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 75\n",
      "[ 0.002  0.007  0.01  -0.003], 265.2234983698106\n",
      "-1.4119240323610773 1084.4193755350996\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.192865\n",
      "         Iterations: 9\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[-0.026 -0.009  0.028  0.001], 265.19286523288656\n",
      "58, trigger_e_4, 0.000, 1.000, -0.429, 0.906, 1079.540\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.252679\n",
      "         Iterations: 26\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[ 0.019 -0.008 -0.1    0.014], 265.2526788379867\n",
      "-1.3350031891529248 1091.0462341214388\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.192517\n",
      "         Iterations: 28\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[-0.058 -0.066 -0.006  0.021], 265.1925170069797\n",
      "59, trigger_e_5, 0.000, 1.000, -0.056, 0.523, 1082.832\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.089876\n",
      "         Iterations: 32\n",
      "         Function evaluations: 143\n",
      "         Gradient evaluations: 131\n",
      "[-0.057  0.003 -0.133  0.03 ], 265.08987569466746\n",
      "-0.5786623366075415 1110.1505817981542\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.280357\n",
      "         Iterations: 26\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 65\n",
      "[-0.004 -0.05   0.106 -0.008], 265.28035652198497\n",
      "60, escale_e, 0.000, 1.000, 2.043, 0.823, 1084.983\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.213550\n",
      "         Iterations: 20\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[-0.04  -0.027  0.006  0.01 ], 265.2135495861573\n",
      "1.219370889327374 1085.9867477411929\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.242769\n",
      "         Iterations: 22\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 53\n",
      "[ 0.04   0.039 -0.128  0.008], 265.24276915261146\n",
      "61, trigger_e_tag, 0.000, 1.000, 1.079, 0.816, 1088.344\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.129746\n",
      "         Iterations: 37\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 84\n",
      "[-0.264 -0.04  -0.02   0.052], 265.12974611737184\n",
      "0.26262419079556965 1079.3131230298688\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.186597\n",
      "         Iterations: 35\n",
      "         Function evaluations: 146\n",
      "         Gradient evaluations: 134\n",
      "[ 0.212 -0.044  0.044 -0.034], 265.18659666602645\n",
      "62, trigger_e_probe, 0.000, 1.000, 1.480, 0.734, 1055.731\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.172739\n",
      "         Iterations: 29\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.034 -0.074 -0.256  0.059], 265.17273853204927\n",
      "0.7461881502615519 1039.5095343996788\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.251865\n",
      "         Iterations: 27\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.139 -0.137  0.077  0.032], 265.2518652349933\n",
      "63, e_prefire, 0.000, 1.000, -0.289, 0.916, 1027.184\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.172830\n",
      "         Iterations: 40\n",
      "         Function evaluations: 140\n",
      "         Gradient evaluations: 128\n",
      "[-0.269 -0.006  0.009  0.043], 265.17282987273745\n",
      "-1.204803000851272 1089.2680760331577\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.149769\n",
      "         Iterations: 42\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 83\n",
      "[ 0.252 -0.058 -0.164 -0.004], 265.14976915596446\n",
      "64, eff_iso_mu, 0.000, 1.000, 0.182, 0.984, 1040.282\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.245992\n",
      "         Iterations: 9\n",
      "         Function evaluations: 55\n",
      "         Gradient evaluations: 44\n",
      "[ 1.159e-02  2.477e-03 -1.398e-02  8.777e-06], 265.2459916256057\n",
      "-0.8019770052881798 1052.4167472528386\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.242354\n",
      "         Iterations: 9\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 59\n",
      "[-0.029  0.006 -0.039  0.01 ], 265.24235406560695\n",
      "65, eff_id_mu, 0.000, 1.000, 0.262, 0.727, 1037.652\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 266.056176\n",
      "         Iterations: 13\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 72\n",
      "[-0.032  0.043  0.035 -0.008], 266.0561760405075\n",
      "-0.4641610208104074 1067.0039242565738\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.117548\n",
      "         Iterations: 29\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.087 -0.034 -0.025  0.024], 265.1175479648627\n",
      "66, eff_mu_0, 0.000, 1.000, 0.582, 0.980, 1021.824\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.161906\n",
      "         Iterations: 5\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 61\n",
      "[ 0.001  0.001 -0.01   0.001], 265.16190611500457\n",
      "-0.3982939542774485 1022.358913255232\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.173306\n",
      "         Iterations: 5\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 94\n",
      "[ 0.002 -0.     0.015 -0.003], 265.1733061369866\n",
      "67, eff_mu_1, 0.000, 1.000, 0.342, 0.972, 1021.351\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.161617\n",
      "         Iterations: 9\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 84\n",
      "[-0.002  0.003 -0.011  0.002], 265.1616166992389\n",
      "-0.630604173217334 1022.3978914564689\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.157815\n",
      "         Iterations: 5\n",
      "         Function evaluations: 58\n",
      "         Gradient evaluations: 47\n",
      "[ 0.003  0.001  0.011 -0.002], 265.1578152548911\n",
      "68, eff_mu_2, 0.000, 1.000, -0.072, 0.977, 1020.963\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.165241\n",
      "         Iterations: 9\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[-0.005 -0.    -0.007  0.002], 265.1652407813877\n",
      "-1.0489760471293343 1023.0326545085828\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.157701\n",
      "         Iterations: 4\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[ 0.004  0.002  0.01  -0.003], 265.1577013813114\n",
      "69, eff_mu_3, 0.000, 1.000, 0.551, 0.848, 1021.068\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.278963\n",
      "         Iterations: 12\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 87\n",
      "[-0.035 -0.016  0.012  0.006], 265.27896306062354\n",
      "-0.2971202171129449 1023.3481888631258\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.289557\n",
      "         Iterations: 5\n",
      "         Function evaluations: 55\n",
      "         Gradient evaluations: 43\n",
      "[ 0.015  0.007  0.023 -0.007], 265.28955730783775\n",
      "70, eff_mu_4, 0.000, 1.000, -0.031, 0.951, 1017.280\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.162193\n",
      "         Iterations: 9\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 74\n",
      "[-0.024 -0.017  0.02   0.003], 265.16219345924367\n",
      "-0.9823786721632816 1020.6462515229277\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.177926\n",
      "         Iterations: 5\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 55\n",
      "[ 0.01   0.006  0.007 -0.004], 265.17792642400354\n",
      "71, eff_mu_5, 0.000, 1.000, -0.015, 0.998, 1017.368\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.151697\n",
      "         Iterations: 0\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[0. 0. 0. 0.], 265.1516972268313\n",
      "-1.0131345634651312 1018.2158378100349\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.151744\n",
      "         Iterations: 3\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 61\n",
      "[ 0.002  0.001 -0.002 -0.   ], 265.1517438453797\n",
      "72, eff_mu_6, 0.000, 1.000, 0.279, 0.988, 1017.374\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.151771\n",
      "         Iterations: 8\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 65\n",
      "[-0.008 -0.004  0.008  0.001], 265.1517709131396\n",
      "-0.7098382559090342 1018.1966578185518\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.153755\n",
      "         Iterations: 3\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.003  0.001 -0.002 -0.   ], 265.1537550610389\n",
      "73, eff_mu_7, 0.000, 1.000, -0.967, 0.977, 1017.193\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.168007\n",
      "         Iterations: 9\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[-0.011 -0.006  0.008  0.001], 265.1680067818829\n",
      "-1.9439240615007316 1016.9905829949356\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.159825\n",
      "         Iterations: 3\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 47\n",
      "[ 0.005  0.002 -0.003 -0.001], 265.1598246459092\n",
      "74, escale_mu, 0.000, 1.000, -0.618, 0.687, 1016.581\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.206904\n",
      "         Iterations: 14\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 46\n",
      "[-0.006  0.002  0.054 -0.008], 265.2069040048052\n",
      "-1.304253871220146 1020.8995630486742\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.153210\n",
      "         Iterations: 13\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.006  0.002 -0.063  0.011], 265.1532098598016\n",
      "75, pileup, 0.000, 1.000, -0.403, 0.560, 1017.683\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.254874\n",
      "         Iterations: 37\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.105 -0.111 -0.028  0.006], 265.2548740519781\n",
      "-0.9625301196994553 848.1956029781666\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 264.982969\n",
      "         Iterations: 39\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[-0.216  0.021 -0.133  0.053], 264.98296945904667\n",
      "76, isr, 0.000, 1.000, 0.410, 0.235, 937.491\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.213580\n",
      "         Iterations: 38\n",
      "         Function evaluations: 146\n",
      "         Gradient evaluations: 134\n",
      "[ 0.087  0.054  0.591 -0.118], 265.2135804395367\n",
      "0.17488297786280174 907.6004573166331\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.155885\n",
      "         Iterations: 29\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[-0.125 -0.156 -0.778  0.171], 265.1558851021095\n",
      "77, fsr, 0.000, 1.000, 0.038, 0.082, 874.321\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.148775\n",
      "         Iterations: 42\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.025 -0.068  0.102 -0.001], 265.1487751372554\n",
      "-0.04338207751735711 802.0890182696608\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.159927\n",
      "         Iterations: 39\n",
      "         Function evaluations: 146\n",
      "         Gradient evaluations: 135\n",
      "[-0.001  0.006 -0.201  0.032], 265.1599265724813\n",
      "78, hdamp, 0.000, 1.000, 0.046, 0.114, 943.838\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.235866\n",
      "         Iterations: 30\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[-0.009  0.015 -0.123  0.019], 265.2358659169671\n",
      "-0.06748777581427062 969.4670482248334\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.132682\n",
      "         Iterations: 39\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 124\n",
      "[ 0.004 -0.044 -0.072  0.018], 265.1326822816654\n",
      "79, tune, 0.000, 1.000, 0.065, 0.201, 927.251\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.190070\n",
      "         Iterations: 26\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 91\n",
      "[ 0.027  0.052  0.355 -0.07 ], 265.19006957006707\n",
      "-0.13677412556920987 926.4418525815167\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.383234\n",
      "         Iterations: 8\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "[ 0.006 -0.034 -0.101  0.021], 265.3832338527166\n",
      "80, ww_scale, 0.000, 1.000, 0.622, 0.653, 927.905\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.315944\n",
      "         Iterations: 5\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[ 0.008  0.006  0.002 -0.002], 265.3159442916192\n",
      "-0.030774390245935757 928.1528163921656\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.271033\n",
      "         Iterations: 19\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[ 0.007  0.002  0.143 -0.025], 265.2710331483896\n",
      "81, ww_resum, 0.000, 1.000, -0.069, 0.895, 924.031\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.179509\n",
      "         Iterations: 8\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 50\n",
      "[-0.005  0.005  0.002 -0.   ], 265.17950871437773\n",
      "-0.9638120816911526 925.7721115856862\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.217556\n",
      "         Iterations: 9\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 53\n",
      "[-0.01   0.006  0.012 -0.001], 265.2175555404018\n",
      "82, top_pt, 1.000, 1.000, 1.453, 0.067, 924.054\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.202359\n",
      "         Iterations: 30\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 61\n",
      "[ 0.186  0.165  0.126 -0.077], 265.2023587306968\n",
      "1.3857855589146728 862.5414819319262\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.122784\n",
      "         Iterations: 29\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 142\n",
      "[-0.152 -0.177 -0.137  0.075], 265.1227840749424\n",
      "83, btag_bfragmentation, 0.000, 1.000, 0.124, 0.988, 863.298\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.136942\n",
      "         Iterations: 35\n",
      "         Function evaluations: 79\n",
      "         Gradient evaluations: 68\n",
      "[-0.055 -0.104 -0.076  0.038], 265.1369418993444\n",
      "-0.8640887014792552 963.0686074282436\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.178325\n",
      "         Iterations: 29\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[-0.079 -0.101 -0.268  0.072], 265.17832544444997\n",
      "84, btag_btempcorr, 0.000, 1.000, 0.055, 0.989, 849.750\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.129327\n",
      "         Iterations: 32\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "[-0.038 -0.063 -0.035  0.022], 265.1293273214994\n",
      "-0.9343127654711247 945.4775267660311\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.213836\n",
      "         Iterations: 29\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[-0.105 -0.138 -0.397  0.103], 265.21383597116636\n",
      "85, btag_cb, 0.000, 1.000, -0.074, 0.994, 844.428\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.179475\n",
      "         Iterations: 30\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 67\n",
      "[-0.004  0.001  0.028 -0.004], 265.179475299242\n",
      "-1.0681409609205272 916.2635283029236\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.138217\n",
      "         Iterations: 29\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 70\n",
      "[-0.068 -0.088 -0.163  0.052], 265.1382173261189\n",
      "86, btag_cfragmentation, 0.000, 1.000, 0.001, 1.000, 849.213\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.151085\n",
      "         Iterations: 0\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 59\n",
      "[0. 0. 0. 0.], 265.1510846819447\n",
      "-0.9989781229193976 850.391507322456\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.151301\n",
      "         Iterations: 3\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 73\n",
      "[ 9.081e-04  2.049e-04 -8.131e-04 -4.627e-05], 265.15130134348095\n",
      "87, btag_dmux, 0.000, 1.000, 0.020, 0.998, 849.213\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.155838\n",
      "         Iterations: 28\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[ 0.05   0.019  0.062 -0.021], 265.1558380901168\n",
      "-0.9778596254376152 884.3377313773826\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.174722\n",
      "         Iterations: 29\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[-0.008 -0.016 -0.022  0.007], 265.17472162133504\n",
      "88, btag_gluonsplitting, 0.000, 1.000, 0.225, 0.958, 848.504\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.131917\n",
      "         Iterations: 39\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 110\n",
      "[-0.013 -0.074 -0.161  0.04 ], 265.1319167637645\n",
      "-0.7332863107616463 1012.4034019431409\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.161735\n",
      "         Iterations: 41\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.014 -0.02  -0.077  0.018], 265.1617350096192\n",
      "89, btag_jes, 0.000, 1.000, 0.024, 0.990, 803.113\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.143124\n",
      "         Iterations: 32\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.033 -0.061 -0.023  0.019], 265.1431237968226\n",
      "-0.9667352925497975 892.824136745821\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.139090\n",
      "         Iterations: 29\n",
      "         Function evaluations: 127\n",
      "         Gradient evaluations: 115\n",
      "[-0.069 -0.088 -0.228  0.062], 265.13909023570534\n",
      "90, btag_jetaway, 0.000, 1.000, 0.165, 0.963, 801.033\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.115027\n",
      "         Iterations: 36\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[ 0.016 -0.049 -0.062  0.015], 265.11502661236597\n",
      "-0.7972695650966904 961.698230107664\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.160681\n",
      "         Iterations: 41\n",
      "         Function evaluations: 155\n",
      "         Gradient evaluations: 143\n",
      "[-0.016 -0.04  -0.074  0.021], 265.16068101499144\n",
      "91, btag_ksl, 0.000, 1.000, 0.002, 1.000, 770.893\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.151690\n",
      "         Iterations: 0\n",
      "         Function evaluations: 49\n",
      "         Gradient evaluations: 37\n",
      "[0. 0. 0. 0.], 265.15168991251244\n",
      "-0.9981055739465589 772.757676129772\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.152168\n",
      "         Iterations: 3\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 69\n",
      "[ 1.168e-03  9.793e-05 -8.041e-04 -7.139e-05], 265.1521676370637\n",
      "92, btag_l2c, 0.000, 1.000, -0.080, 0.996, 770.891\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.165932\n",
      "         Iterations: 30\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[ 0.008  0.008  0.044 -0.01 ], 265.16593167367546\n",
      "-1.0758881826128175 816.1501310794231\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.141772\n",
      "         Iterations: 29\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 118\n",
      "[-0.03  -0.027 -0.037  0.015], 265.1417715734244\n",
      "93, btag_ltothers, 0.000, 1.000, 0.625, 0.874, 774.117\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.163153\n",
      "         Iterations: 44\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[-0.021 -0.043  0.012  0.008], 265.163153146498\n",
      "-0.2484425669545477 860.6298238409538\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.162059\n",
      "         Iterations: 41\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 108\n",
      "[ 0.005 -0.035 -0.077  0.017], 265.1620585381151\n",
      "94, btag_mudr, 0.000, 1.000, -0.071, 0.995, 592.436\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.184033\n",
      "         Iterations: 30\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.01   0.01   0.037 -0.009], 265.18403285057235\n",
      "-1.0665547335260372 632.7304655636809\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.119708\n",
      "         Iterations: 29\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 95\n",
      "[-0.039 -0.038 -0.059  0.022], 265.11970763311433\n",
      "95, btag_mupt, 0.000, 1.000, 0.090, 0.995, 595.004\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.176272\n",
      "         Iterations: 32\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 79\n",
      "[-0.018 -0.017 -0.003  0.006], 265.1762719593496\n",
      "-0.9046351699975527 639.0882287343219\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.132311\n",
      "         Iterations: 29\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 110\n",
      "[-0.05  -0.071 -0.165  0.046], 265.13231082983197\n",
      "96, btag_ptrel, 0.000, 1.000, 0.001, 0.997, 590.847\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.168740\n",
      "         Iterations: 28\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[ 0.02   0.004  0.005 -0.005], 265.16873981602214\n",
      "-0.9964119675175626 617.8472729261188\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.149783\n",
      "         Iterations: 29\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[-0.009 -0.012 -0.025  0.007], 265.1497825076395\n",
      "97, btag_sampledependence, 0.000, 1.000, 0.795, 0.780, 590.819\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.168412\n",
      "         Iterations: 47\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 107\n",
      "[ 0.012 -0.044  0.007  0.004], 265.168412450387\n",
      "0.01488156807541563 586.1246474952459\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.155429\n",
      "         Iterations: 48\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 123\n",
      "[ 0.002 -0.018 -0.104  0.019], 265.15542941355545\n",
      "98, btag_pileup, 0.000, 1.000, -0.061, 0.994, 404.771\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.174124\n",
      "         Iterations: 32\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[-0.006 -0.003  0.011 -0.   ], 265.17412403741355\n",
      "-1.0553507644877231 430.4985508182659\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.135460\n",
      "         Iterations: 29\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[-0.07  -0.095 -0.193  0.058], 265.13545961629507\n",
      "99, btag_statistic, 0.000, 1.000, 0.325, 0.944, 406.109\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.159017\n",
      "         Iterations: 40\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.024 -0.045 -0.08   0.024], 265.15901704347556\n",
      "-0.6194488651140445 467.4200891264337\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.153249\n",
      "         Iterations: 41\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 84\n",
      "[ 0.01  -0.015 -0.039  0.007], 265.1532490006201\n",
      "100, ctag, 0.000, 1.000, 0.166, 0.804, 382.845\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.152767\n",
      "         Iterations: 28\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 65\n",
      "[ 0.201  0.202  0.103 -0.081], 265.15276683238073\n",
      "-0.6383299096746398 404.8531185763261\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.258435\n",
      "         Iterations: 32\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 77\n",
      "[-0.214 -0.241 -0.154  0.098], 265.25843482805624\n",
      "101, mistag, 0.000, 1.000, 0.218, 0.913, 377.897\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.119124\n",
      "         Iterations: 34\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 84\n",
      "[ 0.031  0.009 -0.053  0.002], 265.11912378825906\n",
      "-0.6946585298303871 396.75605940931524\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.203635\n",
      "         Iterations: 27\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.013 -0.059 -0.203  0.044], 265.20363455602427\n",
      "102, jer, 0.000, 1.000, -0.280, 0.711, 372.568\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.144813\n",
      "         Iterations: 40\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 96\n",
      "[-0.202 -0.203  0.016  0.063], 265.14481338811544\n",
      "-0.9908190560977166 394.3990465601222\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.148046\n",
      "         Iterations: 36\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.177  0.102 -0.204 -0.012], 265.1480461230916\n",
      "103, jes_subtotal_pileup, 0.000, 1.000, 0.017, 0.877, 376.986\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.157677\n",
      "         Iterations: 55\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 142\n",
      "[ 0.038  0.02   0.073 -0.021], 265.157677464558\n",
      "-0.8604091750246097 775.7776062365243\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.166775\n",
      "         Iterations: 61\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 138\n",
      "[-0.033 -0.066 -0.107  0.033], 265.16677468023704\n",
      "104, jes_subtotal_relative, 0.000, 1.000, 0.094, 0.890, 372.623\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.164096\n",
      "         Iterations: 52\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[ 0.025  0.021  0.04  -0.014], 265.16409636255986\n",
      "-0.7963668668849384 715.2644650294257\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.159031\n",
      "         Iterations: 58\n",
      "         Function evaluations: 161\n",
      "         Gradient evaluations: 150\n",
      "[-0.045 -0.091 -0.127  0.042], 265.1590314027582\n",
      "105, jes_subtotal_pt, 0.000, 1.000, 0.143, 0.972, 350.924\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.169486\n",
      "         Iterations: 50\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 91\n",
      "[ 0.014 -0.011  0.014 -0.003], 265.16948563471095\n",
      "-0.8293361433277344 465.78475258042755\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.142179\n",
      "         Iterations: 41\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[-0.046 -0.062 -0.255  0.059], 265.14217862126793\n",
      "106, jes_subtotal_scale, 0.000, 1.000, 0.159, 0.982, 337.861\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.161125\n",
      "         Iterations: 44\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 122\n",
      "[ 0.02  -0.007  0.087 -0.016], 265.16112514261255\n",
      "-0.8227493628079063 412.87505116962353\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.144367\n",
      "         Iterations: 40\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[-0.053 -0.077 -0.061  0.031], 265.14436703998723\n",
      "107, jes_subtotal_absolute, 0.000, 1.000, 0.141, 0.959, 328.025\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.168659\n",
      "         Iterations: 50\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 132\n",
      "[ 0.034  0.007  0.026 -0.011], 265.16865886397756\n",
      "-0.8178182521283794 456.4714058118254\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.189185\n",
      "         Iterations: 40\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 110\n",
      "[-0.046 -0.092 -0.216  0.057], 265.1891845038554\n",
      "108, jes_flavor_qcd, 0.000, 1.000, 0.047, 0.790, 316.458\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.159842\n",
      "         Iterations: 53\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 137\n",
      "[ 0.112  0.093  0.153 -0.058], 265.1598424043604\n",
      "-0.7427914977559895 680.6688578386187\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 265.151234\n",
      "         Iterations: 55\n",
      "         Function evaluations: 163\n",
      "         Gradient evaluations: 151\n",
      "[-0.138 -0.173 -0.277  0.095], 265.1512338128979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate impacts for each nuisance parameter\n",
    "\n",
    "# initialize parameter data\n",
    "#fit_data._pmask     = parameters['active'].values.astype(bool)\n",
    "#fit_data._pval_init = parameters['val_fit'].values.copy()\n",
    "\n",
    "impacts_up, impacts_down = dict(), dict()\n",
    "iparam = 4\n",
    "mask = fit_data._pmask\n",
    "p_init = fit_data._pval_fit\n",
    "p_fit = parameters.val_fit.values\n",
    "for pname, pdata in tqdm(parameters.iloc[4:].iterrows(), total=parameters['active'].sum() - 4):\n",
    "    if not pdata.active:\n",
    "        iparam += 1\n",
    "        continue\n",
    "    \n",
    "    tqdm.write(f'{iparam}, {pname}, {pdata.val_init:.3f}, {pdata.err_init:.3f}, {pdata.val_fit:.3f}, {pdata.err_fit:.3f}, {fobj(p_init[mask]):.3f}')\n",
    "\n",
    "    # calculate impacts from up/down variations of n.p. on p.o.i.\n",
    "    p_tmp = p_init.copy()\n",
    "    mask[iparam] = False\n",
    "    p_init[iparam] = pdata.val_fit + pdata.err_fit\n",
    "    res = minimize(fobj, p_fit[mask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "    p_tmp[mask] = res.x\n",
    "    impacts_up[pname] = p_tmp\n",
    "    tqdm.write(f'{(res.x[:4] - p_fit[:4])*100/p_fit[:4]}, {res.fun}')\n",
    "                    \n",
    "    p_init[iparam] = pdata.val_fit - pdata.err_fit\n",
    "    print(p_init[iparam], fobj(p_init[mask]))\n",
    "    res = minimize(fobj, p_fit[mask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "    p_tmp[mask] = res.x\n",
    "    impacts_down[pname] = (p_tmp - p_fit)/p_fit\n",
    "    tqdm.write(f'{(res.x[:4] - p_fit[:4])*100/p_fit[:4]}, {res.fun}')\n",
    "    \n",
    "    mask[iparam] = True\n",
    "    p_init[iparam] = pdata.val_fit\n",
    "    iparam += 1\n",
    "    \n",
    "    #if iparam >= 10: break\n",
    "    \n",
    "# convert impacts to dataframes \n",
    "df_up = pd.DataFrame(impacts_up, index=parameters.index).add_suffix('_up')\n",
    "df_down = pd.DataFrame(impacts_down, index=parameters.index).add_suffix('_down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.063350Z",
     "start_time": "2020-03-04T17:25:09.549Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# save impacts\n",
    "impacts_np = pd.concat([df_up, df_down], axis=1)\n",
    "impacts_np.to_csv(f'local_data/impacts_{scenario}.csv')\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "pd.set_option('display.max_columns', parameters.shape[0])\n",
    "pd.set_option('display.max_rows', parameters.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.064422Z",
     "start_time": "2020-03-04T17:25:09.557Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert to errors\n",
    "err_no_bb = parameters['err_no_bb'].values[mask]\n",
    "errs_np = np.array([np.concatenate([e[:i+4], [0], e[i+4:]]) for i, e in enumerate(errs)])\n",
    "errs_np = err_no_bb**2 - errs_np**2\n",
    "#errs_np[errs_np < 0] = 0\n",
    "#errs_np = np.sqrt(errs_np)\n",
    "##errs_np = np.vstack([errs_np, err_stat, err_mc_stat, err_syst, stderr])\n",
    "#\n",
    "errs_np = pd.DataFrame(errs_np[:,:4], columns=['beta_e', 'beta_mu', 'beta_tau', 'beta_h'], index=list(p_labels))\n",
    "##errs_np = pd.DataFrame(errs_np[:,:4], columns=['beta_e', 'beta_mu', 'beta_tau', 'beta_h'], index=list(p_labels_fancy[4:]) + ['stat.', 'MC stat.', 'syst. total', 'total'])\n",
    "##beta_stderr = stderr.iloc[:,:4].multiply(100)/params_init[:4]\n",
    "100*errs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.065367Z",
     "start_time": "2020-03-04T17:25:09.560Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# print table\n",
    "\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1).to_latex('local_data/errors.tex')\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1).to_csv('local_data/errors.csv')\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1)\n",
    "beta_errs.to_latex('local_data/summary_errors.tex', escape=False)\n",
    "beta_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.066536Z",
     "start_time": "2020-03-04T17:25:09.563Z"
    }
   },
   "outputs": [],
   "source": [
    "jes_mask = np.array([True if ('jes' in pname and 'btag' not in pname) else False for pname in beta_errs.index])\n",
    "btag_mask = np.array([True if 'btag' in pname else False for pname in beta_errs.index])\n",
    "tau_misid_mask = np.array([True if ('misid_tau' in pname and pname not in ['misid_tau_e', 'misid_tau_h']) else False for pname in beta_errs.index])\n",
    "\n",
    "btag_errs = beta_errs[btag_mask]\n",
    "jes_errs = beta_errs[jes_mask]\n",
    "tau_misid_errs = beta_errs[tau_misid_mask]\n",
    "\n",
    "summary_errs = beta_errs[~btag_mask&~jes_mask&~tau_misid_mask].copy()\n",
    "summary_errs.index = [fit_data._parameters.loc[p].label if p in fit_data._parameters.index else p for p in summary_errs.index]\n",
    "summary_errs.loc['b-tag',:] = np.sqrt(np.sum(btag_errs**2))\n",
    "summary_errs.loc['JES',:]  = np.sqrt(np.sum(jes_errs**2))\n",
    "summary_errs.loc[r'$\\sf jet\\rightarrow\\tau$',:]  = np.sqrt(np.sum(tau_misid_errs**2))\n",
    "\n",
    "summary_errs = summary_errs.divide(params_init[:4]/100, axis=1)\n",
    "summary_errs.to_latex('local_data/summary_errors.tex', escape=False)\n",
    "summary_errs.to_csv('local_data/summary_errors.csv')\n",
    "summary_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "29px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "49px",
    "left": "0px",
    "right": "1493.87px",
    "top": "90.9965px",
    "width": "242px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 744.85,
   "position": {
    "height": "40px",
    "left": "919px",
    "right": "20px",
    "top": "59px",
    "width": "678px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  },
  "widgets": {
   "state": {
    "012f8bbe2fdb410dae6e2cde9d7fe5cb": {
     "views": []
    },
    "080556076f174648bddf64f17a54c523": {
     "views": []
    },
    "0ad83b5f67484ae5b8fd8dd43ccc39bd": {
     "views": []
    },
    "15acd81a9adc493683d9b63813f000bf": {
     "views": []
    },
    "1840cb6fded848b4ae95ec8d3db15ab2": {
     "views": []
    },
    "1dd83f822e074642ae4255b15ee661cf": {
     "views": []
    },
    "1e71a878e6474912a0efc497ecc5d65b": {
     "views": []
    },
    "2022ed83777b4963b630b5c46239e218": {
     "views": []
    },
    "21c4c57bfc48495194663e6a4fbac488": {
     "views": []
    },
    "22c45c75435348c0b9501d493d69fdca": {
     "views": []
    },
    "2635f668a1af4a9db2642e705d7c73ff": {
     "views": []
    },
    "2ad5ddd9347e451b9290e5b4179ab9a2": {
     "views": []
    },
    "2c062b5778024117984822b63b0593d7": {
     "views": []
    },
    "2f5eab2f6fb24192b76a5ffe99195d44": {
     "views": []
    },
    "31632517325046e8b0cb62e4f4ed2480": {
     "views": []
    },
    "3562b97192ed4d42bbab17f77c290f6b": {
     "views": []
    },
    "38a7cc053723492b921cf9f084ed243c": {
     "views": []
    },
    "3b5750b20e1745879ca0f965aad7b614": {
     "views": []
    },
    "3f1cbabbe2694a9dabe3f1c2e09d0ee2": {
     "views": []
    },
    "3fab6a26a70c4238a668a46d4dc88bf6": {
     "views": []
    },
    "3fb3c7a25e954a4888996976fa107737": {
     "views": []
    },
    "415db64fbc574daea8457ab600392f09": {
     "views": []
    },
    "4463de406b4645a4b562fe7917380ff9": {
     "views": []
    },
    "487e5450b5a24507932709f1fa8f59c1": {
     "views": []
    },
    "48aba73013e74e71927f71d42fb44d14": {
     "views": []
    },
    "4a19ad30f77e4fe6a2c84c8b62378a47": {
     "views": []
    },
    "4bc83ff5270d41679d76d26cdded8313": {
     "views": []
    },
    "4bdd9dd5c5c64646a27fa9096851458b": {
     "views": []
    },
    "5014cd42705f45178d5e6eeffd70f119": {
     "views": []
    },
    "5259b340b68e4fdb97fb4eaf9d98d954": {
     "views": []
    },
    "5986ef5b605a42aca10bc5834529ee06": {
     "views": []
    },
    "5c1aa44589a140eb9709734c843abde6": {
     "views": []
    },
    "600cd9ca4f4c46d4ad6fe57df107675a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "61fe369ebdd14eaa89de110f6186e6b7": {
     "views": []
    },
    "62ac836017ae47a38f8fde806c5ec9b7": {
     "views": []
    },
    "632ccbfabe91405aa1c5a77c9ea754db": {
     "views": []
    },
    "66175f618ea5472baac618f998d2c06c": {
     "views": []
    },
    "6647a620af034d26abcd327ae02364d4": {
     "views": []
    },
    "6752222d2cba43e18f344a8db7f99d24": {
     "views": []
    },
    "6b684ba1a7c24a35ba2df77016212904": {
     "views": []
    },
    "6cea898f4aca4f1e84601f843e337238": {
     "views": []
    },
    "6f8d1e87fd60462a89d693b2f3b5f007": {
     "views": []
    },
    "74078646a5eb4047b40370a0ab8b6b30": {
     "views": []
    },
    "745b0c79ff3040788ea952fce9c7d607": {
     "views": []
    },
    "757c9b805eb7445bac9a7f141f87e45f": {
     "views": []
    },
    "76ced68e19a742e8976dbfd4e8594a1a": {
     "views": []
    },
    "783bb5e7538d4d9d8315e2698024b353": {
     "views": []
    },
    "794993d66efe4ab29a8d35aad8cfe079": {
     "views": []
    },
    "8375e24bae7541528d7cdc0f379d1d4c": {
     "views": []
    },
    "8554945ec15041a7bf8004dbc3fc5f11": {
     "views": []
    },
    "878a34e26cce4f18bb8232a682ebe964": {
     "views": []
    },
    "8921a75116a549198eb7b7f4a24ab672": {
     "views": []
    },
    "909f4504f0b049bda8b641defa177062": {
     "views": []
    },
    "910b9d32a3fb45ec99da1f9df1add816": {
     "views": []
    },
    "9d15ce601cd34f0699b7a7a0ce1d17dc": {
     "views": []
    },
    "a26638c9fee247b3891aac027a0918cc": {
     "views": []
    },
    "a9d2bf44a3ad447bb3eecde71363c198": {
     "views": []
    },
    "ad366bf4c95f4cdba62d47ba9501efc9": {
     "views": []
    },
    "ad8e1842ec314a94b6ed4b62c4c0a450": {
     "views": []
    },
    "af525094db304d2a812ae1312b00889b": {
     "views": []
    },
    "b0697c4343da491f9a35bf02681dad8f": {
     "views": []
    },
    "b07ff307919e4268bc8bec8379c47a5d": {
     "views": []
    },
    "b0e85c726ca141079333afb27edc63d4": {
     "views": []
    },
    "bdcc1e5df7a8432b9f40d8249a46f90a": {
     "views": []
    },
    "be1065f37fa24e818d31c3bb075947a3": {
     "views": []
    },
    "c296c8df2f734e268c6c1204536e7142": {
     "views": []
    },
    "c4bfd3e447f0426da144b76abc202129": {
     "views": []
    },
    "cced93184d4445218a2b14567579333d": {
     "views": []
    },
    "d5bd2e4d5f85482e9345f3a7a69380d0": {
     "views": []
    },
    "d798fa64e8be4a7d9ec1cbeece3b1be9": {
     "views": []
    },
    "d7aec0d6d05f442b991ab40af944811d": {
     "views": []
    },
    "db469cea2c8e4180bf6890de80329c1d": {
     "views": []
    },
    "e671857510c54634b6f0fa55bf1fa228": {
     "views": []
    },
    "ebf52deafaf64b0c826533dafdf993c0": {
     "views": []
    },
    "ed1e5439da9c41199a7bbbda21b556f8": {
     "views": []
    },
    "f585cf5db5024280af5b567f0e4fd771": {
     "views": []
    },
    "f6ba8f8800af47adabed847063bda8db": {
     "views": []
    },
    "f9bae72f14e44705b5c38a3ddc69fee8": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
