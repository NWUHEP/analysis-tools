{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating individual systematic contributions\n",
    "\n",
    "The effect of any individual systematic uncertainty is somewhat complicated by it's mutual covariance with the POI (i.e., the W branching fractions) and any other systematic uncertainty.  To get a rough idea of the percent-wise contribution to the total uncertainty from each individual systematic, I use the following scheme:\n",
    "\n",
    "   * the fit is carried out as in the nominal case and $\\sigma_{0}$ is estimated\n",
    "   * the fit is carried out for for each of the $n$ nuisance parameters $\\sigma_{theta}$\n",
    "   * the difference between the nominal case and the $n-1$ case is calculated,\n",
    "   * this quantity is normalized to $\\sum_{\\theta} \\sigma_{\\theta}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:09.639485Z",
     "start_time": "2020-03-04T17:25:09.500321Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/naodell/work/wbr/analysis\n",
      "{\n",
      "  \"shell_port\": 47665,\n",
      "  \"iopub_port\": 45897,\n",
      "  \"stdin_port\": 34439,\n",
      "  \"control_port\": 41775,\n",
      "  \"hb_port\": 59995,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"b1af99e8-22621fb4710c87176d42c32e\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-cd56101a-37e8-4a42-a9bd-a2dc100f0478.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "## imports and configuration\n",
    "%cd '/home/naodell/work/wbr/analysis'\n",
    "#%load_ext autoreload\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.optimize import minimize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scripts.plot_tools as pt\n",
    "import scripts.fit_helpers as fh\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "rc_params = {\n",
    "             'figure.figsize': (10, 10),\n",
    "             'axes.labelsize': 20,\n",
    "             'axes.facecolor': 'white',\n",
    "             'axes.titlesize':'x-large',\n",
    "             'legend.fontsize': 20,\n",
    "             'xtick.labelsize':18,\n",
    "             'ytick.labelsize':18,\n",
    "             'font.size':18,\n",
    "             'font.sans-serif':['Arial', 'sans-serif'],\n",
    "             'mathtext.sf':'Arial',\n",
    "             'lines.markersize':8.,\n",
    "             'lines.linewidth':2.5,\n",
    "            }\n",
    "matplotlib.rcParams.update(rc_params)\n",
    "\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:10.006785Z",
     "start_time": "2020-03-04T17:25:09.640699Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 258.679862\n",
      "         Iterations: 74\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[10.832 10.935 10.768 67.464] [10.832 10.935 10.768 67.464]\n"
     ]
    }
   ],
   "source": [
    "# configure, get the input data, and do any additional processing that is needed\n",
    "# initialize fit data\n",
    "scenario = 'fake_factorization_tight'\n",
    "infile = open(f'local_data/fit_data_{scenario}.pkl', 'rb')\n",
    "fit_data = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# get fit parameters\n",
    "parameters = fit_data._parameters.copy() \n",
    "\n",
    "# fit configuration #\n",
    "# minimizer options\n",
    "#steps = 4*[1e-4, ] \n",
    "#steps += list(0.05*fit_data._perr_init[4: fit_data._nnorm + 7])\n",
    "#steps += list(0.05*fit_data._perr_init[fit_data._npoi + fit_data._nnorm:])\n",
    "#steps = np.array(steps)\n",
    "\n",
    "min_options = dict(\n",
    "                   #finite_diff_rel_step=steps,\n",
    "                   #verbose=3,\n",
    "                   #eps=1e-9, \n",
    "                   gtol = 1e-3,\n",
    "                   disp = True\n",
    "                  )\n",
    "\n",
    "# configure the objective\n",
    "sample = None\n",
    "fobj = partial(fit_data.objective,\n",
    "               data = sample,\n",
    "               do_bb_lite = True,\n",
    "               lu_test = 2\n",
    "              )\n",
    "\n",
    "fobj_jac = partial(fit_data.objective_jacobian,\n",
    "                   data = sample,\n",
    "                   do_bb_lite = True,\n",
    "                   lu_test = 2\n",
    "                  )\n",
    "\n",
    "# test fit, should be the same as parameters.val_fit\n",
    "res_mle = minimize(fobj, parameters.val_init.values[fit_data._pmask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "print(res_mle.x[:4]*100, parameters.val_fit[:4].values*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.062418Z",
     "start_time": "2020-03-04T17:25:10.053753Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bd28f94737416cb85d86b5293f337d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, br_tau_e, 0.177, 0.000, 0.177, 0.000, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.180292\n",
      "         Iterations: 4\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 49\n",
      "[-0.007 -0.001 -0.004  0.005], 259.18029181528436\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.180195\n",
      "         Iterations: 7\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 49\n",
      "[-0.005  0.01  -0.01   0.006], 259.18019479227775\n",
      "5, br_tau_mu, 0.173, 0.000, 0.173, 0.000, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.190198\n",
      "         Iterations: 5\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 47\n",
      "[-0.004 -0.    -0.006  0.006], 259.19019794917904\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.186276\n",
      "         Iterations: 7\n",
      "         Function evaluations: 52\n",
      "         Gradient evaluations: 40\n",
      "[-0.005  0.006 -0.002  0.001], 259.18627587203247\n",
      "6, br_tau_h, 0.650, 0.001, 0.649, 0.001, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.202908\n",
      "         Iterations: 4\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 54\n",
      "[ 0.007  0.005 -0.027  0.017], 259.2029081514671\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.193963\n",
      "         Iterations: 8\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 68\n",
      "[-0.01   0.013  0.016 -0.012], 259.19396342566137\n",
      "7, lumi, 1.000, 0.025, 1.004, 0.018, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.154520\n",
      "         Iterations: 45\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.093 -0.239  0.027  0.084], 259.1545202605199\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.258714\n",
      "         Iterations: 68\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 126\n",
      "[-0.01   0.037 -0.075  0.049], 259.2587141117579\n",
      "8, xs_gjets, 1.000, 0.100, 1.057, 0.095, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.683149\n",
      "         Iterations: 14\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 62\n",
      "[ 0.008 -0.023 -0.031  0.027], 259.68314927610726\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.207558\n",
      "         Iterations: 40\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 92\n",
      "[ 0.021 -0.01   0.074 -0.06 ], 259.207557920926\n",
      "9, xs_diboson, 1.000, 0.100, 1.013, 0.099, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.215477\n",
      "         Iterations: 14\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 64\n",
      "[-0.003 -0.002 -0.005  0.005], 259.2154770337013\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.247559\n",
      "         Iterations: 12\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 57\n",
      "[-0.004 -0.025  0.005  0.005], 259.2475593070066\n",
      "10, xs_ww, 1.000, 0.100, 0.963, 0.029, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.242361\n",
      "         Iterations: 37\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[ 0.091  0.03  -0.014 -0.032], 259.24236123108125\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.136211\n",
      "         Iterations: 39\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[ 0.064  0.157  0.094 -0.14 ], 259.1362109373926\n",
      "11, xs_t, 1.000, 0.100, 0.937, 0.073, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.216701\n",
      "         Iterations: 48\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[ 0.322  0.432  0.257 -0.439], 259.2167012311587\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.218441\n",
      "         Iterations: 40\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 84\n",
      "[-0.371 -0.428 -0.291  0.481], 259.21844073025056\n",
      "13, e_fakes_gt4_eq1, 1.000, 0.200, 1.054, 0.080, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.219045\n",
      "         Iterations: 40\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 80\n",
      "[ 0.066  0.117 -0.012 -0.05 ], 259.21904501432624\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.206875\n",
      "         Iterations: 41\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 91\n",
      "[ 0.006 -0.104  0.122 -0.063], 259.20687547294267\n",
      "14, e_fakes_gt4_gt2, 1.000, 0.200, 0.906, 0.130, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.220111\n",
      "         Iterations: 40\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[ 0.023  0.155  0.062 -0.101], 259.2201107118269\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.207321\n",
      "         Iterations: 35\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.036 -0.18  -0.051  0.105], 259.2073207835325\n",
      "15, mu_fakes_gt4_eq1, 1.000, 0.300, 0.891, 0.037, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.221351\n",
      "         Iterations: 40\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[ 0.097  0.118 -0.093 -0.001], 259.2213506994102\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.199806\n",
      "         Iterations: 41\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.118 -0.142  0.161 -0.035], 259.1998057867148\n",
      "16, mu_fakes_gt4_gt2, 1.000, 0.300, 1.007, 0.122, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.209016\n",
      "         Iterations: 40\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "[ 0.103  0.075 -0.036 -0.033], 259.20901560715663\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.200250\n",
      "         Iterations: 34\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 81\n",
      "[-0.105 -0.12   0.061  0.028], 259.2002500462566\n",
      "17, e_fakes_ss_eq0_eq0, 1.000, 0.050, 0.945, 0.032, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.159850\n",
      "         Iterations: 33\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 95\n",
      "[ 0.068 -0.059  0.088 -0.074], 259.1598497546121\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.161841\n",
      "         Iterations: 35\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.04   0.11  -0.1    0.057], 259.1618407787319\n",
      "18, e_fakes_ss_eq1_eq0, 1.000, 0.100, 0.970, 0.069, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.256530\n",
      "         Iterations: 4\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 71\n",
      "[-0.001  0.001 -0.006  0.005], 259.25653007047424\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.314316\n",
      "         Iterations: 8\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 65\n",
      "[-0.012  0.     0.002  0.003], 259.31431569884637\n",
      "19, e_fakes_ss_eq1_eq1, 1.000, 0.100, 0.985, 0.081, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.194991\n",
      "         Iterations: 4\n",
      "         Function evaluations: 48\n",
      "         Gradient evaluations: 36\n",
      "[-0.     0.005 -0.015  0.009], 259.1949907201901\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.209784\n",
      "         Iterations: 6\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 45\n",
      "[-0.007 -0.     0.003  0.001], 259.2097839048705\n",
      "20, e_fakes_ss_eq2_eq1, 1.000, 0.200, 1.040, 0.145, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.206188\n",
      "         Iterations: 4\n",
      "         Function evaluations: 54\n",
      "         Gradient evaluations: 42\n",
      "[-5.121e-04  2.973e-05 -6.253e-03  4.866e-03], 259.2061883025646\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.204253\n",
      "         Iterations: 8\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 57\n",
      "[-0.006  0.003  0.002 -0.   ], 259.2042526875511\n",
      "21, e_fakes_ss_eq2_eq2, 1.000, 0.200, 0.956, 0.198, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.208361\n",
      "         Iterations: 9\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 49\n",
      "[-0.005  0.002 -0.003  0.003], 259.2083605244753\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.149274\n",
      "         Iterations: 8\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 45\n",
      "[-0.005  0.008 -0.    -0.   ], 259.1492739309384\n",
      "22, e_fakes_ss_gt3_eq1, 1.000, 0.300, 0.954, 0.209, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.239554\n",
      "         Iterations: 4\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 61\n",
      "[-0.002 -0.003 -0.007  0.007], 259.2395540346561\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.233322\n",
      "         Iterations: 8\n",
      "         Function evaluations: 52\n",
      "         Gradient evaluations: 41\n",
      "[-0.006  0.003  0.007 -0.004], 259.2333216371212\n",
      "23, e_fakes_ss_gt3_gt2, 1.000, 0.300, 1.070, 0.281, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.194527\n",
      "         Iterations: 4\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 51\n",
      "[-0.002  0.003 -0.004  0.003], 259.19452698043136\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.171654\n",
      "         Iterations: 8\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 54\n",
      "[-0.008  0.005  0.002 -0.   ], 259.1716540706469\n",
      "24, e_fakes_ss_gt2_eq0, 1.000, 0.200, 1.021, 0.062, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.571158\n",
      "         Iterations: 4\n",
      "         Function evaluations: 54\n",
      "         Gradient evaluations: 42\n",
      "[-0.002  0.001 -0.008  0.007], 259.57115813912407\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.138947\n",
      "         Iterations: 37\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[ 0.107  0.148 -0.016 -0.071], 259.1389474664193\n",
      "25, mu_fakes_ss_eq0_eq0, 1.000, 0.050, 1.010, 0.042, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.209615\n",
      "         Iterations: 32\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 82\n",
      "[-0.052 -0.001  0.1   -0.056], 259.2096152420873\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.160382\n",
      "         Iterations: 37\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "[ 0.154  0.057 -0.085 -0.01 ], 259.1603815721687\n",
      "26, mu_fakes_ss_eq1_eq0, 1.000, 0.100, 1.031, 0.077, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.424428\n",
      "         Iterations: 9\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 56\n",
      "[-6.833e-03  1.392e-02 -2.253e-03  7.627e-05], 259.42442845174116\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.199640\n",
      "         Iterations: 37\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.114  0.101  0.033 -0.096], 259.1996403303445\n",
      "27, mu_fakes_ss_eq1_eq1, 1.000, 0.100, 0.975, 0.071, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.209084\n",
      "         Iterations: 4\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 32\n",
      "[-0.001 -0.001 -0.006  0.005], 259.20908368512755\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.243619\n",
      "         Iterations: 12\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 62\n",
      "[-0.015 -0.004  0.005  0.003], 259.2436188133248\n",
      "28, mu_fakes_ss_eq2_eq1, 1.000, 0.200, 1.099, 0.128, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.224355\n",
      "         Iterations: 4\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 49\n",
      "[ 0.001 -0.001 -0.01   0.008], 259.22435542595554\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.224455\n",
      "         Iterations: 12\n",
      "         Function evaluations: 75\n",
      "         Gradient evaluations: 64\n",
      "[-0.013 -0.006  0.004  0.004], 259.22445524248405\n",
      "29, mu_fakes_ss_eq2_eq2, 1.000, 0.200, 0.950, 0.191, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.219080\n",
      "         Iterations: 4\n",
      "         Function evaluations: 46\n",
      "         Gradient evaluations: 34\n",
      "[-0.001 -0.    -0.004  0.004], 259.21908020788044\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.166184\n",
      "         Iterations: 6\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 60\n",
      "[-0.003 -0.002  0.006 -0.003], 259.1661839293316\n",
      "30, mu_fakes_ss_gt3_eq1, 1.000, 0.300, 0.855, 0.211, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.257793\n",
      "         Iterations: 4\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 44\n",
      "[ 0.002  0.002 -0.012  0.008], 259.2577927534161\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.239865\n",
      "         Iterations: 8\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 45\n",
      "[-0.008  0.002  0.004 -0.   ], 259.2398645229888\n",
      "31, mu_fakes_ss_gt3_gt2, 1.000, 0.300, 0.955, 0.275, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.204020\n",
      "         Iterations: 4\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 44\n",
      "[-0.002 -0.    -0.004  0.004], 259.20402018010475\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.176029\n",
      "         Iterations: 2\n",
      "         Function evaluations: 51\n",
      "         Gradient evaluations: 40\n",
      "[-0.002 -0.     0.001  0.   ], 259.17602890607424\n",
      "32, mu_fakes_ss_gt2_eq0, 1.000, 0.200, 1.207, 0.091, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.245767\n",
      "         Iterations: 33\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 87\n",
      "[-0.077 -0.046 -0.026  0.062], 259.2457665476511\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.168107\n",
      "         Iterations: 38\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[ 0.171  0.148  0.04  -0.136], 259.1681074026699\n",
      "33, emu_fakes_ss, 1.000, 0.200, 1.197, 0.083, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.193093\n",
      "         Iterations: 37\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.098  0.144 -0.224  0.089], 259.19309274363917\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.433578\n",
      "         Iterations: 13\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 62\n",
      "[ 0.007 -0.097 -0.01   0.034], 259.43357791928827\n",
      "34, trigger_mu, 1.000, 0.005, 1.000, 0.004, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.240581\n",
      "         Iterations: 44\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 102\n",
      "[ 0.067 -0.28   0.004  0.055], 259.2405813774459\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.126203\n",
      "         Iterations: 43\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.084  0.232  0.06  -0.083], 259.1262029657297\n",
      "35, xs_zjets_alt_pdf, 0.000, 1.000, 0.207, 0.969, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.236166\n",
      "         Iterations: 42\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[-0.019 -0.034 -0.008  0.023], 259.2361664804058\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.136117\n",
      "         Iterations: 43\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[ 0.048  0.073  0.029 -0.061], 259.1361172314347\n",
      "36, xs_zjets_alt_alpha_s, 0.000, 1.000, -0.389, 0.908, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.090203\n",
      "         Iterations: 43\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 103\n",
      "[ 0.06   0.1    0.045 -0.085], 259.09020303626534\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.265081\n",
      "         Iterations: 42\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 96\n",
      "[-0.047 -0.108 -0.051  0.087], 259.26508090846\n",
      "37, xs_zjets_alt_qcd_scale_0, 0.000, 1.000, -0.790, 0.487, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.109778\n",
      "         Iterations: 41\n",
      "         Function evaluations: 97\n",
      "         Gradient evaluations: 85\n",
      "[ 0.238  0.18   0.183 -0.278], 259.1097782236308\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.223849\n",
      "         Iterations: 42\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 82\n",
      "[-0.243 -0.168 -0.22   0.304], 259.2238488214423\n",
      "38, xs_zjets_alt_qcd_scale_1, 0.000, 1.000, -0.743, 0.470, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.189578\n",
      "         Iterations: 39\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 91\n",
      "[-0.024 -0.109 -0.048  0.077], 259.1895783719177\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.162223\n",
      "         Iterations: 40\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 87\n",
      "[ 0.018  0.099  0.059 -0.08 ], 259.1622232982094\n",
      "39, xs_zjets_alt_qcd_scale_2, 0.000, 1.000, -0.226, 0.308, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.141870\n",
      "         Iterations: 43\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 105\n",
      "[-0.173 -0.272 -0.193  0.289], 259.1418704713112\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.220313\n",
      "         Iterations: 39\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 95\n",
      "[ 0.188  0.259  0.209 -0.302], 259.22031293179487\n",
      "40, xs_wjets_qcd_scale_0, 0.000, 1.000, 0.040, 0.801, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.129473\n",
      "         Iterations: 33\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[-0.032 -0.015  0.101 -0.06 ], 259.1294732776554\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.259530\n",
      "         Iterations: 40\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[ 0.097  0.094 -0.165  0.061], 259.25952988108054\n",
      "41, xs_wjets_qcd_scale_1, 0.000, 1.000, -0.038, 0.946, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.178832\n",
      "         Iterations: 29\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[-0.05  -0.057 -0.017  0.048], 259.17883199728374\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.200717\n",
      "         Iterations: 38\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[ 0.047  0.057  0.027 -0.054], 259.2007165156232\n",
      "42, xs_wjets_qcd_scale_2, 0.000, 1.000, 0.862, 0.707, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.225438\n",
      "         Iterations: 40\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[ 0.018  0.024  0.147 -0.124], 259.225438338669\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.124326\n",
      "         Iterations: 37\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 80\n",
      "[ 0.093  0.106 -0.044 -0.033], 259.12432632842507\n",
      "43, xs_wjets_qcd_scale_3, 0.000, 1.000, -0.172, 0.962, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.202459\n",
      "         Iterations: 4\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 53\n",
      "[ 0.001  0.002 -0.01   0.007], 259.20245893074116\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.181235\n",
      "         Iterations: 8\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 58\n",
      "[-0.01   0.005  0.004 -0.001], 259.18123504765055\n",
      "44, xs_wjets_qcd_scale_4, 0.000, 1.000, 1.802, 0.369, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.112831\n",
      "         Iterations: 42\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 82\n",
      "[ 0.004  0.012 -0.081  0.056], 259.1128312904567\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.261541\n",
      "         Iterations: 51\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 113\n",
      "[ 0.019  0.011  0.179 -0.144], 259.26154108201496\n",
      "45, xs_ttbar_pdf, 0.000, 1.000, -0.494, 1.002, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.217274\n",
      "         Iterations: 59\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 99\n",
      "[ 0.05   0.019  0.038 -0.052], 259.21727355889783\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.188656\n",
      "         Iterations: 61\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.024 -0.072  0.015  0.019], 259.188655821355\n",
      "46, xs_ttbar_alpha_s, 0.000, 1.000, 0.464, 1.013, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.181928\n",
      "         Iterations: 64\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[-0.041 -0.059 -0.009  0.04 ], 259.18192777118327\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.211068\n",
      "         Iterations: 68\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 120\n",
      "[ 0.057  0.049  0.059 -0.08 ], 259.21106804145205\n",
      "47, xs_ttbar_qcd_scale, 0.000, 1.000, -0.432, 0.257, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.256172\n",
      "         Iterations: 75\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[-0.338 -0.501 -0.223  0.439], 259.25617238947484\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.097829\n",
      "         Iterations: 54\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n",
      "[ 0.31   0.445  0.275 -0.451], 259.0978286057628\n",
      "48, eff_tau_0, 0.000, 1.000, -0.223, 0.522, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.191173\n",
      "         Iterations: 43\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 92\n",
      "[ 0.063  0.07  -0.223  0.123], 259.19117287137993\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.165251\n",
      "         Iterations: 42\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 95\n",
      "[-0.018 -0.096  0.273 -0.169], 259.16525149121526\n",
      "49, eff_tau_1, 0.000, 1.000, 0.264, 0.385, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.172550\n",
      "         Iterations: 38\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[ 0.029  0.098 -0.188  0.101], 259.1725497305784\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.170491\n",
      "         Iterations: 41\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "[ 0.063 -0.036  0.223 -0.179], 259.1704907359751\n",
      "50, eff_tau_2, 0.000, 1.000, -0.213, 0.292, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.248633\n",
      "         Iterations: 38\n",
      "         Function evaluations: 97\n",
      "         Gradient evaluations: 86\n",
      "[ 0.002  0.101 -0.333  0.219], 259.24863329685127\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.090406\n",
      "         Iterations: 40\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[ 0.028 -0.154  0.359 -0.234], 259.09040602682427\n",
      "51, eff_tau_3, 0.000, 1.000, -0.688, 0.379, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.258301\n",
      "         Iterations: 41\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[ 0.08   0.105 -0.332  0.189], 259.25830073087786\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.068385\n",
      "         Iterations: 43\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[-0.058 -0.146  0.332 -0.184], 259.06838526053053\n",
      "52, eff_tau_4, 0.000, 1.000, -0.208, 0.436, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.618139\n",
      "         Iterations: 6\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 42\n",
      "[ 0.043  0.019 -0.079  0.038], 259.6181394513655\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.232337\n",
      "         Iterations: 40\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.078 -0.132  0.467 -0.282], 259.2323371550228\n",
      "53, eff_tau_5, 0.000, 1.000, -1.175, 0.401, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.297225\n",
      "         Iterations: 47\n",
      "         Function evaluations: 125\n",
      "         Gradient evaluations: 114\n",
      "[ 0.149  0.138 -0.481  0.265], 259.2972250845703\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.038376\n",
      "         Iterations: 44\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 85\n",
      "[-0.134 -0.196  0.437 -0.22 ], 259.03837637075685\n",
      "54, misid_tau_e, 0.000, 1.000, 0.092, 0.996, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.204836\n",
      "         Iterations: 5\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 66\n",
      "[ 0.001  0.001 -0.014  0.01 ], 259.2048363507717\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.193695\n",
      "         Iterations: 9\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 71\n",
      "[-0.008 -0.008  0.017 -0.008], 259.1936945071214\n",
      "55, misid_tau_h, 0.000, 1.000, -0.210, 0.609, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.093671\n",
      "         Iterations: 42\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[ 0.189  0.14  -0.23   0.062], 259.0936713071714\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.312018\n",
      "         Iterations: 41\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 88\n",
      "[-0.083 -0.066  0.257 -0.142], 259.31201820633913\n",
      "56, misid_tau_0, 0.000, 1.000, 0.251, 0.613, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.160681\n",
      "         Iterations: 43\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 94\n",
      "[-0.     0.014  0.079 -0.063], 259.16068097760154\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.265072\n",
      "         Iterations: 32\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 69\n",
      "[ 0.086 -0.025 -0.018 -0.011], 259.2650718594582\n",
      "57, misid_tau_1, 0.000, 1.000, -0.164, 0.721, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.554571\n",
      "         Iterations: 19\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 72\n",
      "[ 0.04   0.007  0.017 -0.03 ], 259.5545714092754\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.164651\n",
      "         Iterations: 31\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 92\n",
      "[ 0.015 -0.034  0.     0.004], 259.1646507107135\n",
      "58, misid_tau_2, 0.000, 1.000, 0.121, 0.741, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.166987\n",
      "         Iterations: 41\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.058 -0.017 -0.112  0.068], 259.1669866896982\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.224351\n",
      "         Iterations: 40\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 80\n",
      "[-0.102 -0.11   0.103 -0.007], 259.2243509897606\n",
      "59, misid_tau_3, 0.000, 1.000, 0.154, 0.895, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.431040\n",
      "         Iterations: 6\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 48\n",
      "[ 0.008  0.007 -0.024  0.013], 259.43103989364334\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.301676\n",
      "         Iterations: 17\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 77\n",
      "[-0.013  0.003  0.019 -0.01 ], 259.30167573244734\n",
      "60, misid_tau_4, 0.000, 1.000, -0.224, 0.947, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.314233\n",
      "         Iterations: 6\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 53\n",
      "[ 0.007  0.005 -0.022  0.013], 259.31423342402564\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.333558\n",
      "         Iterations: 21\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[-0.013  0.003  0.027 -0.016], 259.3335584168527\n",
      "61, misid_tau_5, 0.000, 1.000, -0.282, 0.957, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.422224\n",
      "         Iterations: 6\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 55\n",
      "[ 0.007  0.004 -0.021  0.012], 259.42222351429666\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.332244\n",
      "         Iterations: 19\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "[-0.028  0.01   0.006  0.003], 259.3322444591018\n",
      "62, escale_tau, 0.000, 1.000, 0.237, 0.246, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 258.755776\n",
      "         Iterations: 44\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[-0.076 -0.055  0.139 -0.06 ], 258.755775510799\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.606525\n",
      "         Iterations: 42\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[ 0.111  0.04  -0.138  0.051], 259.6065250997311\n",
      "63, eff_reco_e, 0.000, 1.000, 0.241, 0.933, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.128696\n",
      "         Iterations: 43\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[-0.507  0.133  0.024  0.129], 259.1286955426444\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.244979\n",
      "         Iterations: 46\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 96\n",
      "[ 0.482 -0.122  0.044 -0.174], 259.2449791430631\n",
      "65, eff_e_0, 0.000, 1.000, -1.403, 0.726, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.718883\n",
      "         Iterations: 6\n",
      "         Function evaluations: 51\n",
      "         Gradient evaluations: 40\n",
      "[-3.900e-05  4.544e-03 -3.059e-02  2.156e-02], 259.71888278467907\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.172232\n",
      "         Iterations: 34\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "[ 0.059 -0.077 -0.01   0.009], 259.17223175054556\n",
      "66, eff_e_1, 0.000, 1.000, 0.556, 0.901, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.362283\n",
      "         Iterations: 5\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 43\n",
      "[-0.007 -0.008 -0.005  0.008], 259.3622829730892\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.357211\n",
      "         Iterations: 8\n",
      "         Function evaluations: 55\n",
      "         Gradient evaluations: 44\n",
      "[-0.008  0.014 -0.006  0.004], 259.3572106630306\n",
      "67, eff_e_2, 0.000, 1.000, 1.107, 0.833, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.388289\n",
      "         Iterations: 5\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 60\n",
      "[-0.008 -0.008 -0.005  0.009], 259.38828935425295\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.374256\n",
      "         Iterations: 8\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 50\n",
      "[ 0.004  0.021 -0.012  0.001], 259.3742555952822\n",
      "68, eff_e_3, 0.000, 1.000, -1.137, 0.922, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.314356\n",
      "         Iterations: 4\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 48\n",
      "[-0.007 -0.008  0.002  0.004], 259.3143561813737\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.251305\n",
      "         Iterations: 21\n",
      "         Function evaluations: 79\n",
      "         Gradient evaluations: 68\n",
      "[ 0.012 -0.011 -0.017  0.011], 259.25130493188954\n",
      "69, eff_e_4, 0.000, 1.000, -0.514, 0.463, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.169504\n",
      "         Iterations: 42\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 120\n",
      "[-0.16  -0.017  0.081  0.003], 259.1695042184754\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.282587\n",
      "         Iterations: 28\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 65\n",
      "[ 0.122 -0.028 -0.102  0.04 ], 259.2825866344294\n",
      "70, eff_e_5, 0.000, 1.000, -0.842, 0.428, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.207454\n",
      "         Iterations: 49\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.143  0.053  0.133 -0.063], 259.20745366510516\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.176483\n",
      "         Iterations: 48\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[ 0.111 -0.108 -0.069  0.043], 259.17648278577184\n",
      "71, trigger_e_0, 0.000, 1.000, -0.750, 0.945, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.344958\n",
      "         Iterations: 17\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 62\n",
      "[ 0.032  0.029  0.028 -0.042], 259.34495822850243\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.214585\n",
      "         Iterations: 33\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 91\n",
      "[ 0.023 -0.057 -0.003  0.011], 259.21458467776637\n",
      "72, trigger_e_1, 0.000, 1.000, -0.390, 0.985, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.250222\n",
      "         Iterations: 14\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 58\n",
      "[ 0.011  0.011  0.007 -0.013], 259.2502220754884\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.225333\n",
      "         Iterations: 19\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 74\n",
      "[-0.007 -0.018 -0.003  0.01 ], 259.22533333929226\n",
      "73, trigger_e_2, 0.000, 1.000, -0.281, 0.992, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.234022\n",
      "         Iterations: 4\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 46\n",
      "[-0.002 -0.001 -0.001  0.002], 259.2340219223626\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.196904\n",
      "         Iterations: 19\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[-0.007 -0.007 -0.     0.004], 259.1969036234874\n",
      "74, trigger_e_3, 0.000, 1.000, -0.309, 0.991, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.246128\n",
      "         Iterations: 4\n",
      "         Function evaluations: 41\n",
      "         Gradient evaluations: 30\n",
      "[-0.002 -0.    -0.002  0.002], 259.2461278222726\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.204372\n",
      "         Iterations: 19\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 69\n",
      "[-0.008 -0.01  -0.001  0.006], 259.20437200612855\n",
      "75, trigger_e_4, 0.000, 1.000, -0.341, 0.906, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.205321\n",
      "         Iterations: 30\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 102\n",
      "[ 0.053 -0.003 -0.013 -0.009], 259.20532141256024\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.216604\n",
      "         Iterations: 28\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[-0.02   0.001  0.029 -0.014], 259.21660393615423\n",
      "76, trigger_e_5, 0.000, 1.000, 0.078, 0.526, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.175923\n",
      "         Iterations: 39\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[-0.026  0.014 -0.012  0.014], 259.1759230593276\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.214915\n",
      "         Iterations: 50\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "[ 0.05  -0.025  0.073 -0.065], 259.2149145984074\n",
      "77, escale_e, 0.000, 1.000, 1.897, 0.830, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.238999\n",
      "         Iterations: 17\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[-0.058 -0.034 -0.005  0.034], 259.2389992858628\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.283732\n",
      "         Iterations: 25\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 83\n",
      "[ 0.082  0.087 -0.007 -0.05 ], 259.28373180701357\n",
      "78, trigger_e_tag, 0.000, 1.000, 0.535, 0.822, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.159008\n",
      "         Iterations: 41\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[-0.256 -0.004  0.025  0.077], 259.1590077847594\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.186896\n",
      "         Iterations: 40\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[ 0.292  0.038 -0.014 -0.108], 259.1868964203472\n",
      "79, trigger_e_probe, 0.000, 1.000, 1.369, 0.730, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.154591\n",
      "         Iterations: 36\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 82\n",
      "[ 0.037  0.036 -0.089  0.042], 259.1545913297236\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.236337\n",
      "         Iterations: 41\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 93\n",
      "[-0.039 -0.023  0.163 -0.101], 259.2363374672152\n",
      "80, e_prefire, 0.000, 1.000, -0.295, 0.913, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.190227\n",
      "         Iterations: 38\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[-0.276  0.02   0.021  0.079], 259.1902272190293\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.214409\n",
      "         Iterations: 41\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.305 -0.02  -0.006 -0.102], 259.21440873176533\n",
      "81, eff_iso_mu, 0.000, 1.000, 0.189, 0.984, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.280647\n",
      "         Iterations: 10\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 55\n",
      "[ 0.002 -0.01  -0.003  0.004], 259.28064729417997\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.249374\n",
      "         Iterations: 14\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 57\n",
      "[-0.01   0.051  0.015 -0.022], 259.2493742792617\n",
      "82, eff_id_mu, 0.000, 1.000, 0.009, 0.771, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.255204\n",
      "         Iterations: 41\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[ 0.004 -0.023 -0.012  0.015], 259.2552037849839\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.138743\n",
      "         Iterations: 37\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 123\n",
      "[ 0.003  0.066  0.008 -0.027], 259.13874270970217\n",
      "83, eff_mu_0, 0.000, 1.000, 0.516, 0.981, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.185174\n",
      "         Iterations: 9\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 65\n",
      "[-0.014  0.004 -0.006  0.008], 259.18517378951856\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.204511\n",
      "         Iterations: 0\n",
      "         Function evaluations: 49\n",
      "         Gradient evaluations: 37\n",
      "[0. 0. 0. 0.], 259.20451113800766\n",
      "84, eff_mu_1, 0.000, 1.000, 0.372, 0.973, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.188215\n",
      "         Iterations: 9\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 48\n",
      "[-0.009 -0.002 -0.005  0.008], 259.18821462069906\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.186747\n",
      "         Iterations: 3\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 48\n",
      "[-5.715e-05  1.578e-04 -2.798e-04  1.854e-04], 259.18674682206\n",
      "85, eff_mu_2, 0.000, 1.000, -0.008, 0.977, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.186974\n",
      "         Iterations: 7\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 50\n",
      "[-0.022 -0.003  0.002  0.007], 259.18697445047184\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.186135\n",
      "         Iterations: 3\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 42\n",
      "[ 0.001  0.001 -0.001 -0.   ], 259.1861353098143\n",
      "86, eff_mu_3, 0.000, 1.000, 0.748, 0.851, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.228671\n",
      "         Iterations: 24\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 77\n",
      "[-0.017 -0.075 -0.104  0.106], 259.2286707712603\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.316866\n",
      "         Iterations: 5\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 53\n",
      "[ 0.014  0.009  0.01  -0.015], 259.31686552297907\n",
      "87, eff_mu_4, 0.000, 1.000, -0.004, 0.952, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.195448\n",
      "         Iterations: 9\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 46\n",
      "[-0.024 -0.019  0.002  0.013], 259.1954475212545\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.202448\n",
      "         Iterations: 3\n",
      "         Function evaluations: 43\n",
      "         Gradient evaluations: 33\n",
      "[ 0.006  0.003 -0.002 -0.002], 259.202448279082\n",
      "88, eff_mu_5, 0.000, 1.000, -0.019, 0.998, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.176692\n",
      "         Iterations: 8\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 56\n",
      "[-0.009 -0.002  0.001  0.003], 259.17669181114337\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.179771\n",
      "         Iterations: 0\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 28\n",
      "[0. 0. 0. 0.], 259.1797711056952\n",
      "89, eff_mu_6, 0.000, 1.000, 0.265, 0.989, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.178259\n",
      "         Iterations: 9\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 63\n",
      "[-0.015 -0.003  0.001  0.006], 259.17825856503697\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.181350\n",
      "         Iterations: 3\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 28\n",
      "[ 0.001  0.002 -0.002  0.001], 259.1813497253934\n",
      "90, eff_mu_7, 0.000, 1.000, -1.010, 0.977, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.193213\n",
      "         Iterations: 9\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 70\n",
      "[-0.024 -0.021  0.004  0.012], 259.1932133828842\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.184632\n",
      "         Iterations: 5\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 46\n",
      "[ 0.005  0.006 -0.007  0.002], 259.18463164858264\n",
      "91, escale_mu, 0.000, 1.000, -0.565, 0.679, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.271590\n",
      "         Iterations: 14\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 62\n",
      "[-0.018 -0.009  0.027 -0.011], 259.2715900698324\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.189722\n",
      "         Iterations: 13\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "[-0.     0.003 -0.026  0.019], 259.1897224764241\n",
      "92, pileup, 0.000, 1.000, -0.583, 0.564, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.233573\n",
      "         Iterations: 51\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[ 0.184 -0.04   0.13  -0.153], 259.23357269180207\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.104108\n",
      "         Iterations: 50\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n",
      "[-0.206  0.086 -0.096  0.122], 259.10410832952977\n",
      "93, isr, 0.000, 1.000, 0.446, 0.239, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.254160\n",
      "         Iterations: 39\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.076  0.112  0.379 -0.345], 259.25415988266724\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.188344\n",
      "         Iterations: 40\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 79\n",
      "[-0.132 -0.211 -0.37   0.388], 259.1883442065796\n",
      "94, fsr, 0.000, 1.000, 0.022, 0.082, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.135093\n",
      "         Iterations: 46\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 117\n",
      "[-0.037 -0.046  0.094 -0.044], 259.13509252256875\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.207489\n",
      "         Iterations: 49\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 81\n",
      "[ 0.018  0.029 -0.091  0.053], 259.20748918868924\n",
      "95, hdamp, 0.000, 1.000, 0.062, 0.117, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.225501\n",
      "         Iterations: 40\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.035  0.03   0.013 -0.031], 259.22550073052685\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.181288\n",
      "         Iterations: 39\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 83\n",
      "[-0.025 -0.039 -0.034  0.046], 259.1812875582894\n",
      "96, tune, 0.000, 1.000, 0.023, 0.204, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.386483\n",
      "         Iterations: 19\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 58\n",
      "[ 0.01   0.056  0.088 -0.086], 259.3864833832786\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.391628\n",
      "         Iterations: 8\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 49\n",
      "[-0.003 -0.047 -0.058  0.058], 259.3916283805297\n",
      "97, ww_scale, 0.000, 1.000, 0.669, 0.658, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.262271\n",
      "         Iterations: 14\n",
      "         Function evaluations: 79\n",
      "         Gradient evaluations: 67\n",
      "[-0.013 -0.022 -0.014  0.022], 259.2622712010702\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.234361\n",
      "         Iterations: 38\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 82\n",
      "[ 0.01  -0.016  0.082 -0.06 ], 259.23436101656273\n",
      "98, ww_resum, 0.000, 1.000, -0.109, 0.896, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.209685\n",
      "         Iterations: 8\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 46\n",
      "[-0.019  0.006  0.005  0.002], 259.20968548714444\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.233512\n",
      "         Iterations: 8\n",
      "         Function evaluations: 58\n",
      "         Gradient evaluations: 46\n",
      "[-0.014  0.008  0.001  0.002], 259.23351199052775\n",
      "99, top_pt, 1.000, 1.000, 1.433, 0.069, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.196036\n",
      "         Iterations: 39\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 71\n",
      "[ 0.169  0.22   0.086 -0.192], 259.19603585586634\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.194778\n",
      "         Iterations: 40\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 92\n",
      "[-0.145 -0.256 -0.051  0.167], 259.19477823535345\n",
      "100, btag_bfragmentation, 0.000, 1.000, 0.116, 0.989, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.180268\n",
      "         Iterations: 41\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 83\n",
      "[ 0.072  0.038  0.05  -0.075], 259.1802681456654\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.242062\n",
      "         Iterations: 39\n",
      "         Function evaluations: 97\n",
      "         Gradient evaluations: 85\n",
      "[-0.036 -0.027 -0.054  0.062], 259.24206194179067\n",
      "101, btag_btempcorr, 0.000, 1.000, 0.027, 0.989, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.183411\n",
      "         Iterations: 41\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 84\n",
      "[ 0.052  0.028  0.042 -0.059], 259.18341067247485\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.245369\n",
      "         Iterations: 39\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[-0.005  0.001 -0.011  0.01 ], 259.2453694582907\n",
      "102, btag_cb, 0.000, 1.000, -0.042, 0.994, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.188947\n",
      "         Iterations: 40\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 104\n",
      "[ 0.081  0.063  0.041 -0.079], 259.18894692993257\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.198666\n",
      "         Iterations: 39\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 87\n",
      "[-0.005 -0.016  0.012 -0.002], 259.1986661100414\n",
      "103, btag_cfragmentation, 0.000, 1.000, 0.001, 1.000, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.178318\n",
      "         Iterations: 9\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 53\n",
      "[-0.004  0.001 -0.004  0.004], 259.17831835967587\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.177051\n",
      "         Iterations: 8\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 69\n",
      "[-0.004  0.006 -0.001  0.   ], 259.1770512689475\n",
      "104, btag_dmux, 0.000, 1.000, 0.017, 0.998, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.199582\n",
      "         Iterations: 33\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[ 0.055  0.049  0.006 -0.039], 259.1995817206907\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.203385\n",
      "         Iterations: 35\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.004  0.008 -0.023  0.013], 259.2033850334603\n",
      "105, btag_gluonsplitting, 0.000, 1.000, 0.202, 0.958, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.165811\n",
      "         Iterations: 48\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 93\n",
      "[ 0.02  -0.014 -0.017  0.01 ], 259.165810929585\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.195901\n",
      "         Iterations: 45\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 100\n",
      "[-0.027 -0.008 -0.028  0.033], 259.19590138481516\n",
      "106, btag_jes, 0.000, 1.000, 0.012, 0.991, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.200758\n",
      "         Iterations: 41\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 92\n",
      "[ 0.061  0.04   0.035 -0.06 ], 259.2007575530907\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.251951\n",
      "         Iterations: 39\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 96\n",
      "[-0.038 -0.046 -0.054  0.069], 259.25195060730306\n",
      "107, btag_jetaway, 0.000, 1.000, 0.165, 0.963, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.188241\n",
      "         Iterations: 43\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.056  0.011  0.014 -0.034], 259.18824094047653\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.197296\n",
      "         Iterations: 52\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 99\n",
      "[ 0.01  -0.006  0.034 -0.027], 259.19729560802887\n",
      "108, btag_ksl, 0.000, 1.000, 0.001, 1.000, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.181529\n",
      "         Iterations: 4\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 33\n",
      "[-0.002  0.    -0.005  0.004], 259.1815287813325\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.179511\n",
      "         Iterations: 0\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 33\n",
      "[0. 0. 0. 0.], 259.17951117051894\n",
      "109, btag_l2c, 0.000, 1.000, -0.072, 0.996, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.196889\n",
      "         Iterations: 40\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n",
      "[ 0.059  0.036  0.027 -0.053], 259.1968893169244\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.212676\n",
      "         Iterations: 35\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 89\n",
      "[-0.01   0.005 -0.013  0.012], 259.21267573967606\n",
      "110, btag_ltothers, 0.000, 1.000, 0.596, 0.874, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.182931\n",
      "         Iterations: 51\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[ 0.039  0.025  0.093 -0.091], 259.1829312866073\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.196354\n",
      "         Iterations: 55\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 107\n",
      "[ 0.017 -0.009 -0.014  0.007], 259.19635397519414\n",
      "111, btag_mudr, 0.000, 1.000, -0.076, 0.995, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.191034\n",
      "         Iterations: 40\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[ 0.064  0.052  0.018 -0.052], 259.191033895151\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.210870\n",
      "         Iterations: 39\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 103\n",
      "[ 0.005 -0.015  0.018 -0.011], 259.210869811349\n",
      "112, btag_mupt, 0.000, 1.000, 0.064, 0.995, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.188423\n",
      "         Iterations: 40\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.069  0.059  0.041 -0.073], 259.1884227990798\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.195199\n",
      "         Iterations: 39\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[ 0.005 -0.013  0.019 -0.012], 259.19519912561395\n",
      "113, btag_ptrel, 0.000, 1.000, -0.001, 0.998, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.185588\n",
      "         Iterations: 33\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 82\n",
      "[ 0.059  0.069 -0.015 -0.03 ], 259.1855878912721\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.219881\n",
      "         Iterations: 35\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 80\n",
      "[-0.005  0.004 -0.015  0.012], 259.21988130372324\n",
      "114, btag_sampledependence, 0.000, 1.000, 0.761, 0.781, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.204579\n",
      "         Iterations: 59\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 121\n",
      "[ 0.004 -0.038  0.071 -0.043], 259.2045786022235\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.184340\n",
      "         Iterations: 56\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 87\n",
      "[ 0.027  0.026 -0.013 -0.008], 259.18433961802486\n",
      "115, btag_pileup, 0.000, 1.000, -0.038, 0.994, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.190153\n",
      "         Iterations: 40\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 86\n",
      "[ 0.075  0.057  0.043 -0.077], 259.19015285817443\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.213093\n",
      "         Iterations: 39\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 102\n",
      "[ 0.004 -0.014  0.007 -0.002], 259.21309306153046\n",
      "116, btag_statistic, 0.000, 1.000, 0.297, 0.945, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.184981\n",
      "         Iterations: 51\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 102\n",
      "[ 0.024 -0.004  0.005 -0.011], 259.18498109152546\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.184435\n",
      "         Iterations: 54\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 107\n",
      "[ 0.    -0.012  0.026 -0.016], 259.18443528907414\n",
      "117, ctag, 0.000, 1.000, 0.314, 0.824, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.127938\n",
      "         Iterations: 43\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[ 0.186  0.233  0.043 -0.17 ], 259.1279378583555\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.297428\n",
      "         Iterations: 42\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 80\n",
      "[-0.194 -0.268 -0.014  0.161], 259.29742834248026\n",
      "118, mistag, 0.000, 1.000, 0.329, 0.923, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.144182\n",
      "         Iterations: 41\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 88\n",
      "[ 0.051  0.025  0.016 -0.038], 259.1441823872165\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.225359\n",
      "         Iterations: 39\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 84\n",
      "[-0.074 -0.067  0.026  0.027], 259.22535915023826\n",
      "119, jer, 0.000, 1.000, -0.289, 0.763, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.214080\n",
      "         Iterations: 42\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "[-0.165 -0.167  0.116  0.023], 259.2140797479203\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.191488\n",
      "         Iterations: 43\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "[ 0.18   0.141 -0.019 -0.094], 259.1914882411954\n",
      "120, jes_subtotal_pileup, 0.000, 1.000, -0.030, 0.879, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.184446\n",
      "         Iterations: 71\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 127\n",
      "[ 0.041  0.048  0.062 -0.076], 259.184445564624\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.199253\n",
      "         Iterations: 70\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.018 -0.043  0.009  0.013], 259.19925331760226\n",
      "121, jes_subtotal_relative, 0.000, 1.000, 0.070, 0.891, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.165506\n",
      "         Iterations: 69\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[ 0.057  0.086  0.087 -0.112], 259.1655062958404\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.199420\n",
      "         Iterations: 72\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 140\n",
      "[-0.029 -0.082 -0.008  0.041], 259.1994201941699\n",
      "122, jes_subtotal_pt, 0.000, 1.000, 0.111, 0.975, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.200551\n",
      "         Iterations: 62\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[ 0.046  0.055  0.069 -0.085], 259.2005508684301\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.224908\n",
      "         Iterations: 44\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 106\n",
      "[-0.047 -0.076 -0.022  0.056], 259.22490795997294\n",
      "123, jes_subtotal_scale, 0.000, 1.000, 0.132, 0.983, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.181323\n",
      "         Iterations: 55\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 116\n",
      "[ 0.076  0.07   0.09  -0.116], 259.1813234603345\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.202439\n",
      "         Iterations: 44\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 94\n",
      "[-0.025 -0.034 -0.034  0.045], 259.202438541118\n",
      "124, jes_subtotal_absolute, 0.000, 1.000, 0.147, 0.961, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.202672\n",
      "         Iterations: 64\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 127\n",
      "[ 0.091  0.099  0.074 -0.118], 259.2026720678741\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.187668\n",
      "         Iterations: 54\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 97\n",
      "[-0.025 -0.067 -0.006  0.034], 259.18766791507403\n",
      "125, jes_flavor_qcd, 0.000, 1.000, 0.062, 0.793, 948.794\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.189616\n",
      "         Iterations: 73\n",
      "         Function evaluations: 136\n",
      "         Gradient evaluations: 124\n",
      "[ 0.167  0.192  0.143 -0.225], 259.18961551785276\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 259.200062\n",
      "         Iterations: 74\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 125\n",
      "[-0.153 -0.219 -0.084  0.184], 259.2000622920472\n"
     ]
    }
   ],
   "source": [
    "# calculate impacts for each nuisance parameter\n",
    "\n",
    "# initialize parameter data\n",
    "#fit_data._pmask     = parameters['active'].values.astype(bool)\n",
    "#fit_data._pval_init = parameters['val_fit'].values.copy()\n",
    "\n",
    "impacts_up, impacts_down = dict(), dict()\n",
    "iparam = 4\n",
    "mask   = fit_data._pmask\n",
    "p_init = fit_data._pval_fit\n",
    "p_fit  = parameters.val_fit.values\n",
    "p_err  = parameters.err_fit.values\n",
    "for pname, pdata in tqdm(parameters.iloc[4:].iterrows(), total=parameters['active'].sum() - 4):\n",
    "    if not pdata.active:\n",
    "        iparam += 1\n",
    "        continue\n",
    "    \n",
    "    tqdm.write(f'{iparam}, {pname}, {pdata.val_init:.3f}, {pdata.err_init:.3f}, {pdata.val_fit:.3f}, {pdata.err_fit:.3f}, {fobj(p_init[mask]):.3f}')\n",
    "\n",
    "    # calculate impacts from up/down variations of n.p. on p.o.i.\n",
    "    p_tmp = p_init.copy()\n",
    "    mask[iparam] = False\n",
    "    p_init[iparam] = pdata.val_fit + pdata.err_fit\n",
    "    res = minimize(fobj, p_fit[mask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "    p_tmp[mask] = res.x\n",
    "    impacts_up[pname] = p_tmp - p_fit\n",
    "    tqdm.write(f'{(res.x[:4] - p_fit[:4])/p_err[:4]}, {res.fun}')\n",
    "                    \n",
    "    p_init[iparam] = pdata.val_fit - pdata.err_fit\n",
    "    res = minimize(fobj, p_fit[mask],\n",
    "                   jac     = fobj_jac,\n",
    "                   #hess    = 'cs',\n",
    "                   #method  = 'trust-constr', \n",
    "                   method  = 'BFGS', \n",
    "                   options = min_options,\n",
    "                  )\n",
    "    p_tmp[mask] = res.x\n",
    "    impacts_down[pname] = p_tmp - p_fit\n",
    "    tqdm.write(f'{(res.x[:4] - p_fit[:4])/p_err[:4]}, {res.fun}')\n",
    "    \n",
    "    mask[iparam] = True\n",
    "    p_init[iparam] = pdata.val_init\n",
    "    iparam += 1\n",
    "    \n",
    "# convert impacts to dataframes \n",
    "df_up = pd.DataFrame(impacts_up, index=parameters.index).add_suffix('_up')\n",
    "df_down = pd.DataFrame(impacts_down, index=parameters.index).add_suffix('_down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.063350Z",
     "start_time": "2020-03-04T17:25:09.549Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eff_reco_e_up</th>\n",
       "      <th>eff_reco_e_down</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beta_e</th>\n",
       "      <td>-0.0526</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_mu</th>\n",
       "      <td>0.0111</td>\n",
       "      <td>-0.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_tau</th>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta_h</th>\n",
       "      <td>0.0364</td>\n",
       "      <td>-0.0490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>br_tau_e</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>br_tau_mu</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>br_tau_h</th>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lumi</th>\n",
       "      <td>-0.0884</td>\n",
       "      <td>0.1780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_gjets</th>\n",
       "      <td>0.5015</td>\n",
       "      <td>-1.1367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_diboson</th>\n",
       "      <td>1.3701</td>\n",
       "      <td>-0.2622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_ww</th>\n",
       "      <td>-0.3068</td>\n",
       "      <td>0.0485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_t</th>\n",
       "      <td>-0.2620</td>\n",
       "      <td>-0.3812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_wjets</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_gt4_eq1</th>\n",
       "      <td>-0.2129</td>\n",
       "      <td>-0.2068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_gt4_gt2</th>\n",
       "      <td>1.6143</td>\n",
       "      <td>-0.4468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_gt4_eq1</th>\n",
       "      <td>-0.3217</td>\n",
       "      <td>-0.0975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_gt4_gt2</th>\n",
       "      <td>-0.3387</td>\n",
       "      <td>1.0270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_ss_eq0_eq0</th>\n",
       "      <td>-0.3043</td>\n",
       "      <td>0.3911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_ss_eq1_eq0</th>\n",
       "      <td>-0.4747</td>\n",
       "      <td>0.4533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_ss_eq1_eq1</th>\n",
       "      <td>0.2633</td>\n",
       "      <td>-0.7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_ss_eq2_eq1</th>\n",
       "      <td>0.2152</td>\n",
       "      <td>-0.1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_ss_eq2_eq2</th>\n",
       "      <td>0.3766</td>\n",
       "      <td>-0.5055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_ss_gt3_eq1</th>\n",
       "      <td>-0.3840</td>\n",
       "      <td>-1.0801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_ss_gt3_gt2</th>\n",
       "      <td>-0.6908</td>\n",
       "      <td>-0.5569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_fakes_ss_gt2_eq0</th>\n",
       "      <td>-0.1857</td>\n",
       "      <td>0.0245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_ss_eq0_eq0</th>\n",
       "      <td>0.4741</td>\n",
       "      <td>-0.3645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_ss_eq1_eq0</th>\n",
       "      <td>0.5477</td>\n",
       "      <td>-0.1737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_ss_eq1_eq1</th>\n",
       "      <td>0.0913</td>\n",
       "      <td>-0.0986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_ss_eq2_eq1</th>\n",
       "      <td>-0.6779</td>\n",
       "      <td>1.3545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_ss_eq2_eq2</th>\n",
       "      <td>0.2179</td>\n",
       "      <td>-0.1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_ss_gt3_eq1</th>\n",
       "      <td>0.1837</td>\n",
       "      <td>-0.4635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_ss_gt3_gt2</th>\n",
       "      <td>-2.4647</td>\n",
       "      <td>-1.1545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_fakes_ss_gt2_eq0</th>\n",
       "      <td>-0.1859</td>\n",
       "      <td>-0.7657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emu_fakes_ss</th>\n",
       "      <td>-0.5702</td>\n",
       "      <td>0.6328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_mu</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>-0.0279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_zjets_alt_pdf</th>\n",
       "      <td>-5.2342</td>\n",
       "      <td>-1.4151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_zjets_alt_alpha_s</th>\n",
       "      <td>6.4236</td>\n",
       "      <td>0.8116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_zjets_alt_qcd_scale_0</th>\n",
       "      <td>-0.0957</td>\n",
       "      <td>5.9271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_zjets_alt_qcd_scale_1</th>\n",
       "      <td>-3.6590</td>\n",
       "      <td>2.7259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_zjets_alt_qcd_scale_2</th>\n",
       "      <td>-0.9638</td>\n",
       "      <td>2.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_wjets_qcd_scale_0</th>\n",
       "      <td>2.1097</td>\n",
       "      <td>-4.6285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_wjets_qcd_scale_1</th>\n",
       "      <td>-0.4449</td>\n",
       "      <td>0.5682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_wjets_qcd_scale_2</th>\n",
       "      <td>0.0017</td>\n",
       "      <td>-0.6184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_wjets_qcd_scale_3</th>\n",
       "      <td>0.3581</td>\n",
       "      <td>0.3756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_wjets_qcd_scale_4</th>\n",
       "      <td>5.1004</td>\n",
       "      <td>2.1610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_ttbar_pdf</th>\n",
       "      <td>3.1398</td>\n",
       "      <td>-4.1877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_ttbar_alpha_s</th>\n",
       "      <td>-3.8566</td>\n",
       "      <td>5.3635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xs_ttbar_qcd_scale</th>\n",
       "      <td>-0.8873</td>\n",
       "      <td>1.4708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_tau_0</th>\n",
       "      <td>1.4697</td>\n",
       "      <td>-3.4134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_tau_1</th>\n",
       "      <td>1.6399</td>\n",
       "      <td>-4.1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_tau_2</th>\n",
       "      <td>2.1141</td>\n",
       "      <td>-5.0540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_tau_3</th>\n",
       "      <td>0.7662</td>\n",
       "      <td>-4.4459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_tau_4</th>\n",
       "      <td>1.6425</td>\n",
       "      <td>-2.4572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_tau_5</th>\n",
       "      <td>0.3240</td>\n",
       "      <td>-3.8823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misid_tau_e</th>\n",
       "      <td>0.0845</td>\n",
       "      <td>-0.0314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misid_tau_h</th>\n",
       "      <td>0.5988</td>\n",
       "      <td>3.7300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misid_tau_0</th>\n",
       "      <td>2.5270</td>\n",
       "      <td>1.0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misid_tau_1</th>\n",
       "      <td>0.2176</td>\n",
       "      <td>0.5825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misid_tau_2</th>\n",
       "      <td>-0.8609</td>\n",
       "      <td>2.2761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misid_tau_3</th>\n",
       "      <td>-0.9959</td>\n",
       "      <td>0.4782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misid_tau_4</th>\n",
       "      <td>0.1467</td>\n",
       "      <td>-0.0338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misid_tau_5</th>\n",
       "      <td>0.0902</td>\n",
       "      <td>-0.8475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>escale_tau</th>\n",
       "      <td>2.6389</td>\n",
       "      <td>1.7977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_reco_e</th>\n",
       "      <td>-24.1099</td>\n",
       "      <td>-24.1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_id_e</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_e_0</th>\n",
       "      <td>0.3011</td>\n",
       "      <td>-0.4687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_e_1</th>\n",
       "      <td>-1.5589</td>\n",
       "      <td>1.1656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_e_2</th>\n",
       "      <td>-2.2671</td>\n",
       "      <td>1.8364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_e_3</th>\n",
       "      <td>-0.3288</td>\n",
       "      <td>0.5332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_e_4</th>\n",
       "      <td>3.4770</td>\n",
       "      <td>2.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_e_5</th>\n",
       "      <td>-0.5538</td>\n",
       "      <td>1.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_e_0</th>\n",
       "      <td>1.0367</td>\n",
       "      <td>-1.8207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_e_1</th>\n",
       "      <td>0.5415</td>\n",
       "      <td>-0.9564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_e_2</th>\n",
       "      <td>0.3901</td>\n",
       "      <td>-0.6927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_e_3</th>\n",
       "      <td>0.4312</td>\n",
       "      <td>-0.7624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_e_4</th>\n",
       "      <td>2.1118</td>\n",
       "      <td>-0.7113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_e_5</th>\n",
       "      <td>1.2457</td>\n",
       "      <td>0.3982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>escale_e</th>\n",
       "      <td>1.6565</td>\n",
       "      <td>2.1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_e_tag</th>\n",
       "      <td>-2.7417</td>\n",
       "      <td>4.4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_e_probe</th>\n",
       "      <td>-6.2109</td>\n",
       "      <td>4.5450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_prefire</th>\n",
       "      <td>1.5886</td>\n",
       "      <td>-0.1796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_iso_mu</th>\n",
       "      <td>0.6857</td>\n",
       "      <td>-1.4520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_id_mu</th>\n",
       "      <td>2.6902</td>\n",
       "      <td>-5.2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_mu_0</th>\n",
       "      <td>0.9115</td>\n",
       "      <td>-1.0607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_mu_1</th>\n",
       "      <td>0.2703</td>\n",
       "      <td>-0.6783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_mu_2</th>\n",
       "      <td>-0.5061</td>\n",
       "      <td>0.6119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_mu_3</th>\n",
       "      <td>0.8014</td>\n",
       "      <td>-2.1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_mu_4</th>\n",
       "      <td>0.3730</td>\n",
       "      <td>-0.3493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_mu_5</th>\n",
       "      <td>-0.0506</td>\n",
       "      <td>-0.0428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_mu_6</th>\n",
       "      <td>0.0061</td>\n",
       "      <td>-0.0348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff_mu_7</th>\n",
       "      <td>1.0073</td>\n",
       "      <td>-0.4280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>escale_mu</th>\n",
       "      <td>-0.3061</td>\n",
       "      <td>-0.3187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pileup</th>\n",
       "      <td>5.8350</td>\n",
       "      <td>2.6160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isr</th>\n",
       "      <td>0.1404</td>\n",
       "      <td>1.0881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fsr</th>\n",
       "      <td>-0.2540</td>\n",
       "      <td>-0.0299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hdamp</th>\n",
       "      <td>-0.1149</td>\n",
       "      <td>0.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tune</th>\n",
       "      <td>0.1275</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ww_scale</th>\n",
       "      <td>0.4473</td>\n",
       "      <td>0.6939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ww_resum</th>\n",
       "      <td>0.6037</td>\n",
       "      <td>-1.0876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_pt</th>\n",
       "      <td>0.7559</td>\n",
       "      <td>-0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_bfragmentation</th>\n",
       "      <td>-0.1957</td>\n",
       "      <td>0.1076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_btempcorr</th>\n",
       "      <td>0.0313</td>\n",
       "      <td>-0.1848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_cb</th>\n",
       "      <td>-0.0058</td>\n",
       "      <td>-0.0390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_cfragmentation</th>\n",
       "      <td>0.0132</td>\n",
       "      <td>-0.0217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_dmux</th>\n",
       "      <td>-0.0493</td>\n",
       "      <td>0.0749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_gluonsplitting</th>\n",
       "      <td>-0.2047</td>\n",
       "      <td>-0.1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_jes</th>\n",
       "      <td>0.0500</td>\n",
       "      <td>-0.1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_jetaway</th>\n",
       "      <td>0.0932</td>\n",
       "      <td>-0.2958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_ksl</th>\n",
       "      <td>0.0037</td>\n",
       "      <td>-0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_l2c</th>\n",
       "      <td>-0.1121</td>\n",
       "      <td>0.0907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_ltothers</th>\n",
       "      <td>0.5380</td>\n",
       "      <td>-0.5203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_mudr</th>\n",
       "      <td>-0.1939</td>\n",
       "      <td>-0.0292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_mupt</th>\n",
       "      <td>0.0637</td>\n",
       "      <td>-0.1697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_ptrel</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>-0.1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_sampledependence</th>\n",
       "      <td>0.3327</td>\n",
       "      <td>-0.3560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_pileup</th>\n",
       "      <td>-0.0920</td>\n",
       "      <td>0.0484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>btag_statistic</th>\n",
       "      <td>0.1259</td>\n",
       "      <td>-0.3865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctag</th>\n",
       "      <td>0.7938</td>\n",
       "      <td>1.4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistag</th>\n",
       "      <td>0.3270</td>\n",
       "      <td>0.6521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jer</th>\n",
       "      <td>2.5631</td>\n",
       "      <td>1.5799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jes_subtotal_pileup</th>\n",
       "      <td>-1.3950</td>\n",
       "      <td>0.4634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jes_subtotal_relative</th>\n",
       "      <td>-0.5051</td>\n",
       "      <td>-0.4771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jes_subtotal_pt</th>\n",
       "      <td>0.0748</td>\n",
       "      <td>-0.7454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jes_subtotal_scale</th>\n",
       "      <td>0.2722</td>\n",
       "      <td>-1.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jes_subtotal_absolute</th>\n",
       "      <td>-0.9081</td>\n",
       "      <td>0.0603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jes_flavor_qcd</th>\n",
       "      <td>-0.1612</td>\n",
       "      <td>-2.7289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          eff_reco_e_up  eff_reco_e_down\n",
       "name                                                    \n",
       "beta_e                          -0.0526           0.0500\n",
       "beta_mu                          0.0111          -0.0102\n",
       "beta_tau                         0.0051           0.0093\n",
       "beta_h                           0.0364          -0.0490\n",
       "br_tau_e                         0.0000           0.0003\n",
       "br_tau_mu                        0.0002          -0.0001\n",
       "br_tau_h                        -0.0003          -0.0003\n",
       "lumi                            -0.0884           0.1780\n",
       "xs_gjets                         0.5015          -1.1367\n",
       "xs_diboson                       1.3701          -0.2622\n",
       "xs_ww                           -0.3068           0.0485\n",
       "xs_t                            -0.2620          -0.3812\n",
       "xs_wjets                         0.0000           0.0000\n",
       "e_fakes_gt4_eq1                 -0.2129          -0.2068\n",
       "e_fakes_gt4_gt2                  1.6143          -0.4468\n",
       "mu_fakes_gt4_eq1                -0.3217          -0.0975\n",
       "mu_fakes_gt4_gt2                -0.3387           1.0270\n",
       "e_fakes_ss_eq0_eq0              -0.3043           0.3911\n",
       "e_fakes_ss_eq1_eq0              -0.4747           0.4533\n",
       "e_fakes_ss_eq1_eq1               0.2633          -0.7377\n",
       "e_fakes_ss_eq2_eq1               0.2152          -0.1973\n",
       "e_fakes_ss_eq2_eq2               0.3766          -0.5055\n",
       "e_fakes_ss_gt3_eq1              -0.3840          -1.0801\n",
       "e_fakes_ss_gt3_gt2              -0.6908          -0.5569\n",
       "e_fakes_ss_gt2_eq0              -0.1857           0.0245\n",
       "mu_fakes_ss_eq0_eq0              0.4741          -0.3645\n",
       "mu_fakes_ss_eq1_eq0              0.5477          -0.1737\n",
       "mu_fakes_ss_eq1_eq1              0.0913          -0.0986\n",
       "mu_fakes_ss_eq2_eq1             -0.6779           1.3545\n",
       "mu_fakes_ss_eq2_eq2              0.2179          -0.1237\n",
       "mu_fakes_ss_gt3_eq1              0.1837          -0.4635\n",
       "mu_fakes_ss_gt3_gt2             -2.4647          -1.1545\n",
       "mu_fakes_ss_gt2_eq0             -0.1859          -0.7657\n",
       "emu_fakes_ss                    -0.5702           0.6328\n",
       "trigger_mu                       0.0108          -0.0279\n",
       "xs_zjets_alt_pdf                -5.2342          -1.4151\n",
       "xs_zjets_alt_alpha_s             6.4236           0.8116\n",
       "xs_zjets_alt_qcd_scale_0        -0.0957           5.9271\n",
       "xs_zjets_alt_qcd_scale_1        -3.6590           2.7259\n",
       "xs_zjets_alt_qcd_scale_2        -0.9638           2.0833\n",
       "xs_wjets_qcd_scale_0             2.1097          -4.6285\n",
       "xs_wjets_qcd_scale_1            -0.4449           0.5682\n",
       "xs_wjets_qcd_scale_2             0.0017          -0.6184\n",
       "xs_wjets_qcd_scale_3             0.3581           0.3756\n",
       "xs_wjets_qcd_scale_4             5.1004           2.1610\n",
       "xs_ttbar_pdf                     3.1398          -4.1877\n",
       "xs_ttbar_alpha_s                -3.8566           5.3635\n",
       "xs_ttbar_qcd_scale              -0.8873           1.4708\n",
       "eff_tau_0                        1.4697          -3.4134\n",
       "eff_tau_1                        1.6399          -4.1727\n",
       "eff_tau_2                        2.1141          -5.0540\n",
       "eff_tau_3                        0.7662          -4.4459\n",
       "eff_tau_4                        1.6425          -2.4572\n",
       "eff_tau_5                        0.3240          -3.8823\n",
       "misid_tau_e                      0.0845          -0.0314\n",
       "misid_tau_h                      0.5988           3.7300\n",
       "misid_tau_0                      2.5270           1.0366\n",
       "misid_tau_1                      0.2176           0.5825\n",
       "misid_tau_2                     -0.8609           2.2761\n",
       "misid_tau_3                     -0.9959           0.4782\n",
       "misid_tau_4                      0.1467          -0.0338\n",
       "misid_tau_5                      0.0902          -0.8475\n",
       "escale_tau                       2.6389           1.7977\n",
       "eff_reco_e                     -24.1099         -24.1099\n",
       "eff_id_e                         0.0000           0.0000\n",
       "eff_e_0                          0.3011          -0.4687\n",
       "eff_e_1                         -1.5589           1.1656\n",
       "eff_e_2                         -2.2671           1.8364\n",
       "eff_e_3                         -0.3288           0.5332\n",
       "eff_e_4                          3.4770           2.0160\n",
       "eff_e_5                         -0.5538           1.4067\n",
       "trigger_e_0                      1.0367          -1.8207\n",
       "trigger_e_1                      0.5415          -0.9564\n",
       "trigger_e_2                      0.3901          -0.6927\n",
       "trigger_e_3                      0.4312          -0.7624\n",
       "trigger_e_4                      2.1118          -0.7113\n",
       "trigger_e_5                      1.2457           0.3982\n",
       "escale_e                         1.6565           2.1214\n",
       "trigger_e_tag                   -2.7417           4.4103\n",
       "trigger_e_probe                 -6.2109           4.5450\n",
       "e_prefire                        1.5886          -0.1796\n",
       "eff_iso_mu                       0.6857          -1.4520\n",
       "eff_id_mu                        2.6902          -5.2055\n",
       "eff_mu_0                         0.9115          -1.0607\n",
       "eff_mu_1                         0.2703          -0.6783\n",
       "eff_mu_2                        -0.5061           0.6119\n",
       "eff_mu_3                         0.8014          -2.1961\n",
       "eff_mu_4                         0.3730          -0.3493\n",
       "eff_mu_5                        -0.0506          -0.0428\n",
       "eff_mu_6                         0.0061          -0.0348\n",
       "eff_mu_7                         1.0073          -0.4280\n",
       "escale_mu                       -0.3061          -0.3187\n",
       "pileup                           5.8350           2.6160\n",
       "isr                              0.1404           1.0881\n",
       "fsr                             -0.2540          -0.0299\n",
       "hdamp                           -0.1149           0.0099\n",
       "tune                             0.1275           0.0020\n",
       "ww_scale                         0.4473           0.6939\n",
       "ww_resum                         0.6037          -1.0876\n",
       "top_pt                           0.7559          -0.0009\n",
       "btag_bfragmentation             -0.1957           0.1076\n",
       "btag_btempcorr                   0.0313          -0.1848\n",
       "btag_cb                         -0.0058          -0.0390\n",
       "btag_cfragmentation              0.0132          -0.0217\n",
       "btag_dmux                       -0.0493           0.0749\n",
       "btag_gluonsplitting             -0.2047          -0.1044\n",
       "btag_jes                         0.0500          -0.1222\n",
       "btag_jetaway                     0.0932          -0.2958\n",
       "btag_ksl                         0.0037          -0.0157\n",
       "btag_l2c                        -0.1121           0.0907\n",
       "btag_ltothers                    0.5380          -0.5203\n",
       "btag_mudr                       -0.1939          -0.0292\n",
       "btag_mupt                        0.0637          -0.1697\n",
       "btag_ptrel                       0.0762          -0.1451\n",
       "btag_sampledependence            0.3327          -0.3560\n",
       "btag_pileup                     -0.0920           0.0484\n",
       "btag_statistic                   0.1259          -0.3865\n",
       "ctag                             0.7938           1.4200\n",
       "mistag                           0.3270           0.6521\n",
       "jer                              2.5631           1.5799\n",
       "jes_subtotal_pileup             -1.3950           0.4634\n",
       "jes_subtotal_relative           -0.5051          -0.4771\n",
       "jes_subtotal_pt                  0.0748          -0.7454\n",
       "jes_subtotal_scale               0.2722          -1.0012\n",
       "jes_subtotal_absolute           -0.9081           0.0603\n",
       "jes_flavor_qcd                  -0.1612          -2.7289"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save impacts\n",
    "impacts_np = pd.concat([df_up, df_down], axis=1)\n",
    "impacts_np.to_csv(f'local_data/impacts_{scenario}.csv')\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "pd.set_option('display.max_columns', parameters.shape[0])\n",
    "pd.set_option('display.max_rows', parameters.shape[0])\n",
    "\n",
    "impacts_np[['eff_reco_e_up', 'eff_reco_e_down']]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.064422Z",
     "start_time": "2020-03-04T17:25:09.557Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'errs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-46ef1db38f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert to errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0merr_no_bb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'err_no_bb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0merrs_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0merrs_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_no_bb\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0merrs_np\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#errs_np[errs_np < 0] = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'errs' is not defined"
     ]
    }
   ],
   "source": [
    "# convert to errors\n",
    "err_no_bb = parameters['err_no_bb'].values[mask]\n",
    "errs_np = np.array([np.concatenate([e[:i+4], [0], e[i+4:]]) for i, e in enumerate(errs)])\n",
    "errs_np = err_no_bb**2 - errs_np**2\n",
    "#errs_np[errs_np < 0] = 0\n",
    "#errs_np = np.sqrt(errs_np)\n",
    "##errs_np = np.vstack([errs_np, err_stat, err_mc_stat, err_syst, stderr])\n",
    "#\n",
    "errs_np = pd.DataFrame(errs_np[:,:4], columns=['beta_e', 'beta_mu', 'beta_tau', 'beta_h'], index=list(p_labels))\n",
    "##errs_np = pd.DataFrame(errs_np[:,:4], columns=['beta_e', 'beta_mu', 'beta_tau', 'beta_h'], index=list(p_labels_fancy[4:]) + ['stat.', 'MC stat.', 'syst. total', 'total'])\n",
    "##beta_stderr = stderr.iloc[:,:4].multiply(100)/params_init[:4]\n",
    "100*errs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.065367Z",
     "start_time": "2020-03-04T17:25:09.560Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# print table\n",
    "\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1).to_latex('local_data/errors.tex')\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1).to_csv('local_data/errors.csv')\n",
    "#beta_errs.divide(params_init[:4]/100, axis=1)\n",
    "beta_errs.to_latex('local_data/summary_errors.tex', escape=False)\n",
    "beta_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:25:12.066536Z",
     "start_time": "2020-03-04T17:25:09.563Z"
    }
   },
   "outputs": [],
   "source": [
    "jes_mask = np.array([True if ('jes' in pname and 'btag' not in pname) else False for pname in beta_errs.index])\n",
    "btag_mask = np.array([True if 'btag' in pname else False for pname in beta_errs.index])\n",
    "tau_misid_mask = np.array([True if ('misid_tau' in pname and pname not in ['misid_tau_e', 'misid_tau_h']) else False for pname in beta_errs.index])\n",
    "\n",
    "btag_errs = beta_errs[btag_mask]\n",
    "jes_errs = beta_errs[jes_mask]\n",
    "tau_misid_errs = beta_errs[tau_misid_mask]\n",
    "\n",
    "summary_errs = beta_errs[~btag_mask&~jes_mask&~tau_misid_mask].copy()\n",
    "summary_errs.index = [fit_data._parameters.loc[p].label if p in fit_data._parameters.index else p for p in summary_errs.index]\n",
    "summary_errs.loc['b-tag',:] = np.sqrt(np.sum(btag_errs**2))\n",
    "summary_errs.loc['JES',:]  = np.sqrt(np.sum(jes_errs**2))\n",
    "summary_errs.loc[r'$\\sf jet\\rightarrow\\tau$',:]  = np.sqrt(np.sum(tau_misid_errs**2))\n",
    "\n",
    "summary_errs = summary_errs.divide(params_init[:4]/100, axis=1)\n",
    "summary_errs.to_latex('local_data/summary_errors.tex', escape=False)\n",
    "summary_errs.to_csv('local_data/summary_errors.csv')\n",
    "summary_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "29px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "49px",
    "left": "0px",
    "right": "1493.87px",
    "top": "90.9965px",
    "width": "242px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 744.85,
   "position": {
    "height": "40px",
    "left": "919px",
    "right": "20px",
    "top": "59px",
    "width": "678px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  },
  "widgets": {
   "state": {
    "012f8bbe2fdb410dae6e2cde9d7fe5cb": {
     "views": []
    },
    "080556076f174648bddf64f17a54c523": {
     "views": []
    },
    "0ad83b5f67484ae5b8fd8dd43ccc39bd": {
     "views": []
    },
    "15acd81a9adc493683d9b63813f000bf": {
     "views": []
    },
    "1840cb6fded848b4ae95ec8d3db15ab2": {
     "views": []
    },
    "1dd83f822e074642ae4255b15ee661cf": {
     "views": []
    },
    "1e71a878e6474912a0efc497ecc5d65b": {
     "views": []
    },
    "2022ed83777b4963b630b5c46239e218": {
     "views": []
    },
    "21c4c57bfc48495194663e6a4fbac488": {
     "views": []
    },
    "22c45c75435348c0b9501d493d69fdca": {
     "views": []
    },
    "2635f668a1af4a9db2642e705d7c73ff": {
     "views": []
    },
    "2ad5ddd9347e451b9290e5b4179ab9a2": {
     "views": []
    },
    "2c062b5778024117984822b63b0593d7": {
     "views": []
    },
    "2f5eab2f6fb24192b76a5ffe99195d44": {
     "views": []
    },
    "31632517325046e8b0cb62e4f4ed2480": {
     "views": []
    },
    "3562b97192ed4d42bbab17f77c290f6b": {
     "views": []
    },
    "38a7cc053723492b921cf9f084ed243c": {
     "views": []
    },
    "3b5750b20e1745879ca0f965aad7b614": {
     "views": []
    },
    "3f1cbabbe2694a9dabe3f1c2e09d0ee2": {
     "views": []
    },
    "3fab6a26a70c4238a668a46d4dc88bf6": {
     "views": []
    },
    "3fb3c7a25e954a4888996976fa107737": {
     "views": []
    },
    "415db64fbc574daea8457ab600392f09": {
     "views": []
    },
    "4463de406b4645a4b562fe7917380ff9": {
     "views": []
    },
    "487e5450b5a24507932709f1fa8f59c1": {
     "views": []
    },
    "48aba73013e74e71927f71d42fb44d14": {
     "views": []
    },
    "4a19ad30f77e4fe6a2c84c8b62378a47": {
     "views": []
    },
    "4bc83ff5270d41679d76d26cdded8313": {
     "views": []
    },
    "4bdd9dd5c5c64646a27fa9096851458b": {
     "views": []
    },
    "5014cd42705f45178d5e6eeffd70f119": {
     "views": []
    },
    "5259b340b68e4fdb97fb4eaf9d98d954": {
     "views": []
    },
    "5986ef5b605a42aca10bc5834529ee06": {
     "views": []
    },
    "5c1aa44589a140eb9709734c843abde6": {
     "views": []
    },
    "600cd9ca4f4c46d4ad6fe57df107675a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "61fe369ebdd14eaa89de110f6186e6b7": {
     "views": []
    },
    "62ac836017ae47a38f8fde806c5ec9b7": {
     "views": []
    },
    "632ccbfabe91405aa1c5a77c9ea754db": {
     "views": []
    },
    "66175f618ea5472baac618f998d2c06c": {
     "views": []
    },
    "6647a620af034d26abcd327ae02364d4": {
     "views": []
    },
    "6752222d2cba43e18f344a8db7f99d24": {
     "views": []
    },
    "6b684ba1a7c24a35ba2df77016212904": {
     "views": []
    },
    "6cea898f4aca4f1e84601f843e337238": {
     "views": []
    },
    "6f8d1e87fd60462a89d693b2f3b5f007": {
     "views": []
    },
    "74078646a5eb4047b40370a0ab8b6b30": {
     "views": []
    },
    "745b0c79ff3040788ea952fce9c7d607": {
     "views": []
    },
    "757c9b805eb7445bac9a7f141f87e45f": {
     "views": []
    },
    "76ced68e19a742e8976dbfd4e8594a1a": {
     "views": []
    },
    "783bb5e7538d4d9d8315e2698024b353": {
     "views": []
    },
    "794993d66efe4ab29a8d35aad8cfe079": {
     "views": []
    },
    "8375e24bae7541528d7cdc0f379d1d4c": {
     "views": []
    },
    "8554945ec15041a7bf8004dbc3fc5f11": {
     "views": []
    },
    "878a34e26cce4f18bb8232a682ebe964": {
     "views": []
    },
    "8921a75116a549198eb7b7f4a24ab672": {
     "views": []
    },
    "909f4504f0b049bda8b641defa177062": {
     "views": []
    },
    "910b9d32a3fb45ec99da1f9df1add816": {
     "views": []
    },
    "9d15ce601cd34f0699b7a7a0ce1d17dc": {
     "views": []
    },
    "a26638c9fee247b3891aac027a0918cc": {
     "views": []
    },
    "a9d2bf44a3ad447bb3eecde71363c198": {
     "views": []
    },
    "ad366bf4c95f4cdba62d47ba9501efc9": {
     "views": []
    },
    "ad8e1842ec314a94b6ed4b62c4c0a450": {
     "views": []
    },
    "af525094db304d2a812ae1312b00889b": {
     "views": []
    },
    "b0697c4343da491f9a35bf02681dad8f": {
     "views": []
    },
    "b07ff307919e4268bc8bec8379c47a5d": {
     "views": []
    },
    "b0e85c726ca141079333afb27edc63d4": {
     "views": []
    },
    "bdcc1e5df7a8432b9f40d8249a46f90a": {
     "views": []
    },
    "be1065f37fa24e818d31c3bb075947a3": {
     "views": []
    },
    "c296c8df2f734e268c6c1204536e7142": {
     "views": []
    },
    "c4bfd3e447f0426da144b76abc202129": {
     "views": []
    },
    "cced93184d4445218a2b14567579333d": {
     "views": []
    },
    "d5bd2e4d5f85482e9345f3a7a69380d0": {
     "views": []
    },
    "d798fa64e8be4a7d9ec1cbeece3b1be9": {
     "views": []
    },
    "d7aec0d6d05f442b991ab40af944811d": {
     "views": []
    },
    "db469cea2c8e4180bf6890de80329c1d": {
     "views": []
    },
    "e671857510c54634b6f0fa55bf1fa228": {
     "views": []
    },
    "ebf52deafaf64b0c826533dafdf993c0": {
     "views": []
    },
    "ed1e5439da9c41199a7bbbda21b556f8": {
     "views": []
    },
    "f585cf5db5024280af5b567f0e4fd771": {
     "views": []
    },
    "f6ba8f8800af47adabed847063bda8db": {
     "views": []
    },
    "f9bae72f14e44705b5c38a3ddc69fee8": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
